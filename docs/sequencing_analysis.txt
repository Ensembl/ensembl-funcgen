:::: The Ensembl Regulation (eFG) Sequencing Analysis Environment ::::

This document details the configuration and functionality available using the eFG 'sequencing' 
environment, which utilises both the eFG pipeline environment and the Ensembl eHive technology. 
The eFG environment provides configuration and command line access to various functions 
which can run the whole pipeline or allow a more flexible step wise approach.

:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::


Contents

1. Introduction
2. Overview
3. Pre-requisites
4. The Ensembl eHive 
5. The eFG Sequencing Environment
5.1. Input Data
5.2. Initiating An Instance
6. Running The Pipeline
6.1. Raw Reads Alignment
6.2. Peak Calling
6.3. Motif Annotation
7. Administration Functions & Trouble Shooting
8. Known Issues/Caveats

::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::


1 Introduction

The sequencing analysis pipeline requires a funcgen database, along with an accompanying core database.
To create a funcgen database see the file introduction_to_efg.txt for instructions.  

If you already have a funcgen database, you may still have to add specific entries to run the 
sequencing analysis pipeline. See section 7 for administrative functions to add these entries.

The sequencing analysis pipeline consists of distinct stages:

:: Raw Data Alignment

In this phase raw sequence reads from *-Seq experiments are mapped to the genomic sequence.
We are currently using bwa (Li et al) to perform the mapping.
Only mappings with up to 2 mismatches and/or mapping to up to 30 loci are taken. 
Reads mapping to multiple loci are randomly allocated to one of the possible locus.

Poor quality reads may get incorrect mappings. These should be pre-filtered before
running the short read alignments.

:: Peak Calling

In this phase we want to detect regions in the genome enriched in the *-Seq experiment.
Duplicate reads are removed, as well as reads mapping to mitochondria. 

There are different types of Peak calling supported:

- Sharp Peaks: SWEMBL_R015 and SWEMBL_R0025
These are better used for "peaky" sets like TF binding sites.
This analysis is based on the Swembl peak caller (Wilder et al, to appear)
SWEMBL_R015 is a very strict set of parameters, optimized for CTCF TF binding and applied to TFs
SWEMBL_R0025 is a relaxed set of parameters, and is currently applied for Open Chromatin like Dnase1

Although a control set is desirable, these can be run without one 
(for Open Chromatin by default we actually assume there is no control set)

- Broad Peaks: CCAT_HISTONE
These are better used for enrichment over wide regions, like Histone marks
Currently it is only used for H3K36me3, although it could be applicable to other broad marks
It makes use of CCAT (Han et al, 2008). It *absolutely* requires a control dataset.

Some areas of the genome are known to be problematic. For Human, there is an established
list of problematic genomic regions (blacklist). We also include a filtering process to
eliminate peaks falling inside these regions.

:: Motif Feature mapping
Motifs are mapped in the whole genome using MOODS (Korhonen et al, 2009), a fast motif mapping tool.
A matching score threshold is then selected using called peaks as a reference 
(only 5% matches outside called peaks are allowed)
Finally, only motif matches overlapping called peaks are loaded into the Regulation DB.

As a quality check (QC), there is also a PWM inference module that infers motifs enriched in 
the called peaks, and compares them to known motifs from Jaspar.

:: Read Import
Cumulative read counts are imported as RPKMs (Mortazavi et al. 2008), for displaying purposes 
To enable faster access, these are stored as files. 

This step uses the output of the peaks pipeline, where some filtering of the mappings occur.
This should be run as soon as possible, before the Regulatory Build.

::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::


2 Overview

The pipeline is constituted by several different sub-pipelines and scrips, each
one can be run independently. Nonetheless, we provide a *nix environment with 
different functions to wrap up sub-pipelines/scrips and make it easier to run 
them in combination

If required, edit the following environment files (section 5):
ensembl-funcgen/scripts/efg.config
ensembl-funcgen/scripts/environments/pipeline.config
ensembl-funcgen/scripts/environments/sequencing.config

If required, set up input directory structure and data (section 5.1) e.g.
$DATA_HOME/fasta/${SPECIES}    
$DATA_HOME/bwa_indexes/${SPECIES}    
$DATA_HOME/sam_header/${SPECIES}     
$DATA_HOME/fastq/${SPECIES}     
$DATA_HOME/alignments/${SPECIES}/${ASSEMBLY} 
$DATA_HOME/binding_matrices 
 
You should also create a folder where needed binaries are (see section 3)
e.g. /software/ensembl/funcgen

Create an instance file (see section 5.2), e.g.:
sequencing_human_64.config

Initialise the environment and run the alignments and annotation (section 7):
>bash
>. ensembl-funcgen/scripts/.efg
>. sequencing_human_64.config password
>AddAlignmentDataSets -I /folder
>SubmitAlignments
... wait for alignments to finish
>AddPeakDatasets -a analysis -I /folder
>SubmitPeaks
... wait for peak calls to finish
>FilterBlacklist 
(currently only in case of Human)
>PeaksReport
>RunMotifMatch -f $TF
(only relevant if there are TF datasets, and matrices are in the db)
>ImportMotifMatch
>SetupReadCounts
... wait for file conversions to finish
>SubmitReadCounts

Check logs to ensure pipelines have completed succesfully.
See section 7 for rollback and administration functions.

::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::


3 Pre-requisites/Requirements

You need the Ensembl API installed 

The eHive system also needs to be installed (section 4).
The eHive currently only suppports the LSF job management system
You can also run locally using multiple processors.

You need to have several pieces of softare installed and in you $PATH:
Most of them should also be in the folder $BIN_DIR 

- bwa can be found here:
http://bio-bwa.sourceforge.net/
Add the bwa binary to $BIN_DIR

- samtools can be found here: 
http://samtools.sourceforge.net/
Add the samtools binary to $BIN_DIR

- SWEMBL can be found here:
http://www.ebi.ac.uk/~swilder/SWEMBL/
Add the binary to $BIN_DIR

- CCAT can be found here:
http://cmb.gis.a-star.edu.sg/ChIPSeq/paperCCAT.htm
Add the binary to $BIN_DIR

You also need to set a configuration file. Just copy the config_histone.txt file that comes with CCAT and 
into the folder ${BIN_DIR}/ccat_config/ and modify the window size to 1000

- BedTools can be found here:
http://code.google.com/p/bedtools/
Only bamToBed is required at the moment
Add it to $BIN_DIR

- MOODS can be found here:
http://www.cs.helsinki.fi/group/pssmfind/
Add the binary find_pssm_dna to $BIN_DIR

- Jaspar data files can be found here:
http://jaspar.genereg.net/html/DOWNLOAD/
You need the contents of "all_data", namely matrix_only and the FlatFileDir 
These should be in $DATA_HOME/binding_matrices/Jaspar

- exonerate can found here:
http://www.ebi.ac.uk/~guy/exonerate/
Only the fasta processing utils (eg. fastaexplode) are used
The binaries should be in $BIN_DIR/exonerate

::For the PWM inference pipeline you'll need MEME (Bailey and Elkan, 1994) and STAMP (Mahony and Benos, 2007) 
These are only used for QC at the moment, so you may skip these.

- MEME can be found here: 
http://meme.nbcr.net/
Add the binary to $BIN_DIR

- STAMP can be found here: 
http://www.benoslab.pitt.edu/stamp/ 
http://www.benoslab.pitt.edu/Software/
Add the binary to $BIN_DIR

To generate QC Peak Reports you need R. No special packages are required for this.
To generate fastq reports we recommend using the 'report' and 'qa' functions from the ShortReads R package

Memory:
This is dependant on several factors including the number of reads and genomic size.
This can be easily configured and even changed dynamically while running the jobs

Disk Space:
The pipeline is quite demanding in disk space, as it creates large temporary files.
Usually consider reserving at least the triple amount of space as required for the raw reads.

/tmp space may be an issue in very large sets, but it can be readjusted in job requirements
Use [tmp=xxx] in the job requirements in the eHive conf files to set this.
If you have a pipeline running already, you can set this in the appropriate entry in the pipeline db

::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::


4 The Ensembl eHive

See ensembl-hive/docs/* for generic ehive information and configuration. 

All the configuration files for the Regulation eHive pipelines are in:
Bio::EnsEMBL::Funcgen::HiveConfig::*
You may need to edit some of the parameters, although they should be mostly working as it is

The Runnables for those pipelines are in:
Bio::EnsEMBL::Funcgen::RunnableDB::*
You should not need to change anything in the Runnables

The pipelines should work with the latest eHive version.
Nonetheless, to guarantee that pipelines work, use a specific version: lg4_post_rel63_20110712

eg:
> cvs -d $CVSROOT checkout -r lg4_post_rel63_20110712 -d ensembl-hive_lg4_post_rel63_20110712 ensembl-hive

You need to have an ensembl-hive folder, so use a soft link:
ln -l ensembl-hive_lg4_post_rel63_20110712 ensembl-hive

::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::


5 The eFG Sequencing Environment

See introduction_to_efg.txt for information about general efg.env configuration.

sequencing.env		 Provides specific configuration and functions.
sequencing.config	 Provides deployment configuration for sequencing.env

Edit sequencing.config setting data and binary paths where appropriate. All environmental variables 
should be documented or self explanatory. These should only need setting up once. It should be noted
that any variables set in sequencing.config will override those set in efg.config, likewise any set 
in your instance file will override those set in sequencing.config.

5.1 Input Data

The environment assumes a particular directory structure based on values of $DATA_HOME and $SPECIES
e.g.
${DATA_HOME}/fasta/homo_sapiens
${DATA_HOME}/bwa_indexes/homo_sapiens
${DATA_HOME}/fastq/homo_sapiens
${DATA_HOME}/alignments/homo_sapiens
${DATA_HOME}/sam_header/homo_sapiens
${DATA_HOME}/binding_matrices/Jaspar

There are a set of data needed before running the pipeline:

:: You need the genomic sequence file, which should be under
${DATA_HOME}/fasta/${SPECIES}

Currently we use a naming convention for the fasta file, which must be:
${SPECIES}_${GENDER}_${ASSEMBLY}_unmasked.fasta

For species with gender, we need one version for each gender. 
If gender information is not used, then male is assumed.

Eg. for human, we need to first dump the male genomic sequence and then generate the female by 
stripping the male genomic sequence of the male-specific genomic regions. 

E.g. getting the sequence using ensembl-analysis scripts: takes a bit so better run on a dedicated node
>bsub \
-J get_seq_${SPECIES} \
-e get_seq_${SPECIES}.err \
-o get_seq_${SPECIES}.out \
-R"select[mem>3500] rusage[mem=3500]" \
-M3500000 \
"perl $SRC/ensembl-analysis/scripts/sequence_dump.pl \
-dbhost ${DNADB_HOST} \
-dbuser ${DNADB_USER} \
-dbport ${DNADB_PORT} \
-dbname ${DNADB_NAME} \
-species ${SPECIES} \
-coord_system_name toplevel \
-filename ${DATA_HOME}/fasta/${SPECIES}/${SPECIES}_male_${ASSEMBLY}_${SCHEMA_BUILD}_unmasked.fasta"

This will not output alternative haplotyes. For human, this will only output non PAR-regions
of the Y chromosome, split in distinct sequences. As these will cause problems with coordinates 
when aligning you need to make a single Y chromosome sequence, with N's padding non-PAR regions

Alternatively, you can download directly a sequence dump from the ensembl website:
e.g. ftp://ftp.ensembl.org/pub/release-xx/fasta/homo_sapiens/dna/
Just make sure there are no haplotypes and PAR regions are present but masked with 'N'.

Once this job is finished remove chromosome sequences of alternative gender (eg. Y chromosome)

${DATA_HOME}/fasta/${SPECIES}/${SPECIES}_female_${ASSEMBLY}_${SCHEMA_BUILD}_unmasked.fasta

:: To use bwa for alignments, we need index files for each fasta. 
These are under ${DATA_HOME}/bwa_indexes/${SPECIES}

To generate these, you need to run "bwa index"
This takes some time, and can be memory intensive, so better run on a dedicated node.
Eg. bsub -J bwa_index -e bwa_index.err -o bwa_index.out -M8000000 -R"select[mem>8000] rusage[mem=8000]" \
    "bwa index -a bwtsw ${DATA_HOME}/fasta/homo_sapiens/homo_sapiens_male_GRCh37_unmasked.fasta"

After finishing, there will be several homo_sapiens_male_GRCh37_unmasked.fasta.* files
Transfer these to ${DATA_HOME}/bwa_indexes/homo_sapiens

:: You also need samtools index files to manipulate sam and bam files.
These are under ${DATA_HOME}/sam_header/${SPECIES}

To generate these you need to run "samtools faidx"
This is on the other hand quite fast so you can run it on the commandline
Eg. samtools faidx ${DATA_HOME}/fasta/homo_sapiens/homo_sapiens_male_GRCh37_unmasked.fasta 
Transfer the resulting fai file to ${DATA_HOME}/sam_header/${SPECIES}

Finally, you'll also need a sam header file necessary to merge temporary alignments.
You can just run e.g.:
cat homo_sapiens_*_unmasked.fasta.fai | cut -f 1,2 | sed 's/\t/\tLN\:/' | sed 's/^/@SQ\tSN\:/' \
> homo_sapiens_*_unmasked.header.sam 
The results should also be in ${DATA_HOME}/sam_header/${SPECIES}

5.2 Initiating An Instance 

To initialise an instance of the environment a small instance file is sourced. 
An example of this is available here:
ensembl-funcgen/scripts/environments/sequencing_example.config

This contains a few configuration variables. As the eFG DBAdaptor auto-selects a core DB if one is 
not already specified, it may be necessary to define some DNADB parameters. Do this if a valid 
corresponding core DB in not available on ensembldb.ensembl.org, or if you want to use a particular 
core DB. Due to the multi-assembly nature of the eFG schema it is also necessary to follow the ensembl 
standard naming convention for DBs i.e.

${PREFIX}_${SPECIES}_funcgen_${SCHEMA_BUILD} 
eg. my_homo_sapiens_funcgen_64_37

As detailed above, you may also want to add some more variables to the instance file to override those in 
sequencing.config or efg.env.

Tip: If you're handling several species, it is useful to create a separate 'screen' session for each.
This provides an easy way to manage multiple environments.

Sourcing the environment will print some config setting for you to review and also change the prompt 
and window title, to inform you which instance of the environment you are using.  This is useful when running
numerous instance in parallel.  Source an instance file by sourcing the base eFG environment first (invoking bash 
and passing a dnadbpass if required):

>bash
>. ensembl-funcgen/scripts/efg.env     # or just efg if you have set the alias in your .bashrc
Setting up the Ensembl Function Genomics environment...
Welcome to eFG!
>. path/to/sequencing_my_instance_file.config dbpass

:::: Welcome to the eFG sequencing environment
:::: Setting up the eFG pipeline environment
:: Sourcing SEQUENCING_CONFIG: ${EFG_SRC}/scripts/environments/sequencing.config

DB:               ${DB_USER}@${DB_HOST}:${DB_NAME}:${DB_PORT}
DNADB:            ${DNADB_USER}@${DNADB_HOST}:${DNADB_NAME}:${DNADB_PORT}
VERSION:          ${SCHEMA}
BUILD:            ${BUILD}

sequencing:${DB_NAME}>

All output from the pipeline will be written here:
$DATA_HOME/output/$DB_NAME (will be created if it does not exist)

Use the alias 'workdir' to access this directory.

:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::


6 Running The Pipeline

The sequencing environment specifies some functions (not all documented here) available via the command 
line, these can generally be invoked with a -h option to print a help or usage message.

6.1 Raw Read Alignment

All the input raw reads need to be in fastq, and they can be gzipped (recommended)
Fastq files do not need specific names, but files need to be in a specific folder structure:
$DATA_HOME/fastq/${SPECIES}/experiment/celltype_featuretype/[1-9]+/*.fastq[.gz]

eg. $DATA_HOME/fastq/homo_sapiens/ENCODE/H1ESC_CTCF/1/*.fastq.gz
eg. $DATA_HOME/fastq/homo_sapiens/ENCODE/H1ESC_CTCF/2/*.fastq.gz

Unfortunately, directory structure is case sensitive so use featuretype names exactly as in the efg db
Ideally this step should be performed by a script that interacts with a data tracking db

Numbered folders are for biological replicates, although currently replicates are simply pooled
If there are no replicates, just add all fastq files into a single folder named '1'

The following environment scripts will help you run the alignment pipeline:

AddAlignmentSets  Creates a Read Alignment eHive pipeline DB ("alignments") and alignment jobs. 
   	          It gets experiment name, cell and feature type using folder names
		  It will output eHive output messages which you can use to manually 
		  handle the pipeline. 
			   
e.g.: > AddAlignmentSets -I ${DATA_HOME}/fastq/${SPECIES}/experiment
Where folder experiment contains a set of celltype_featuretype subfolders with fastq files
*All* folders will be used so make sure all appropriate cell types and feature types exist
Otherwise remove folders that you do not wish to align

SubmitAlignments  Submits the read alignment jobs with the eHive.
		  It will enter in a loop until the pipeline finishes.
		  You can interrupt it at any time using Ctrl-C, although you must
		  rerun it to make sure the pipeline finishes.

You can always manually manage the eHive pipeline and sometimes you'll probably have to.

6.2 Peak Calling

This step requires alignments. Usually these will be the result from the previous step.
You can use external alignments, but you'll need to conform to file type and naming (see below).

The input files should be in the sam format, and they can be gzipped (recommended):
Files need specific names, which should have been generated by the previous step:
$DATA_HOME/fastq/${SPECIES}/experiment/celltype_featuretype_samse_sam.gz

eg. $DATA_HOME/alignments/homo_sapiens/ENCODE/H1ESC_CTCF_samse.sam.gz

AddPeakSets       Creates a eHive Peak Calling pipeline DB ("peaks") and peak calling jobs. 
   	          It gets experiment name, cell and feature type using folder and file names
		  It also gets the specific analysis to be used (type of peak calling)
		  It will output eHive output messages which you can use to manually 
		  handle the pipeline. 

e.g.: > AddPeakSets -a analysis -I ${DATA_HOME}/alignments/${SPECIES}/experiment
Where folder experiment contains a set of celltype_featuretype*.samse.sam.gz sam files
      
      Analysis must be one of:

			   SWEMBL_R015		- Peaky histones & transcriptions factors
			   SWEMBL_R0025		- Dnase1
			   CCAT_HISTONE		- Broad histone peaks e.g. H3K27/36me
			   
Each of the AddAlignmentSets, AddPeakSets and SetupReadCounts steps of the sequencing_analysis
pipeline creates a new eHive database that stores the jobs to be run by the eHive pipeline. The output
of these jobs creates processed data which is updated in the current developmental efg database 
(e.g dev_mus_musculus_funcgen_68_38). All the Analysis methods and their parameters are defined in analysis
table of the efg data base and not in the eHive job table. Please take a look at the analysis table of 
efgdb to find all the valid methods of Analysis available within ensembl regulation.

			   
SubmitPeaks       Submits peak calling jobs with the eHive.
		  It will enter in a loop until the pipeline finishes.
		  You can interrupt it at any time using Ctrl-C, although you must
		  rerun it to make sure the pipeline finishes.

You can always manually manage the eHive pipeline and sometimes you'll probably have to.

FilterBlacklist   Filters peaks with a blacklist (currently Human only)
		  Runs ${EFG_SRC}/scripts/miscellaneous/check_overlaps_with_encode_blacklist.pl
		  This needs to be done before motif mapping
		  By default all sets are filtered, as there may be changes in the blacklist

PeaksReport	  Generates reports for peaks as pdf.
		  Runs ${EFG_SRC}/scripts/peaks_report.pl
		  Generates a pdf with barplot of peak numbers and peak length
		  Currently it prints reports for all sets in the DB
		  Use -i to only print the ones that have been generated in this session

6.3 Motif Annotation

This Step can only be run once there are filtered Peaks (Annotated Features) loaded in the eFG DB
All matrices need to be loaded in the eFG and associated to the proper FeatureType (see section 7)

RunMotifMatch	  Creates the Filtered set of Genomic motifs
		  Runs in a single job, currently run on the farm
		  - Memory problems may occur with low information matrices generating many hits
		  - You may need to manually set up the $SCHEMA_BUILD (usually to the previous release)
  
ImportMotifMatch  Imports Motifs into the eFG DB (only those overlapping annotated features)
		  Creates a eHive pipeline "motif_import" to do this. Deletes previous data from eFG
		  Can also be used to partial imports (eg. only one chromosome)

To rollback the data written in test cases or if the alignments fail for some reason and require some clean up,
use the Rollback function. There are also various administrations functions which can be used (see section 7). 

InferPWM	 This runs a PWM inference pipeline that infers PWMs for each Feature Set
		 This is currently only used for checking data quality of the obtained peaks

6.4 Read Counts

This Step imports read counts for web display as wiggle plots. 
It is run after Peak calling because it will use the output of the peak calling process
For efficiency, read counts are now stored as compressed binary files.

SetupReadCounts	Sets a eHive pipeline "read_counts". Needs to be run after the peaks pipeline
		Will generate a set of file conversion jobs to convert sam to bed if needed
		Can only start the next step ONLY after conversion jobs have all finished

SubmitReadCounts This Creates ResultSets and external file to display read count plots
		 Runs a previously created eHive "read_counts" pipeline
		 Need to wait for file conversion jobs to finish before running it		 

::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

7 Administration Functions & Trouble Shooting

Here is a summary of some helpful aliases.

Add this to your .bashrc for convenient initiation of base environment

efg='. ~/src/ensembl-funcgen/scripts/.efg'

After the base environment is initialised 'efg' and other aliases are defined as:

efg	    	- cd's to root efg code dir
efgd            - cd's to root data dir 
workdir		- cd's to working database dir

mysqlefg	- Connects to the output DB
mysqlcore       - Connects to the core/dna DB

mysqlalignpipe	- Connects to the Alignment pipeline DB
mysqlpeakspipe	- Connects to the Peak Calling pipeline DB
mysqlmotifspipe	- Connects to the Motif Feature Import pipeline DB
mysqlreadspipe	- Connects to the Read Counts pipeline DB

:: Adding new Feature Type, Cell Type or Analysis
Use the script ${EFG_SRC}/scripts/import/import_type.pl
E.g.:
perl $EFG_SRC/scripts/import/import_type.pl \
-type FeatureType \
-s mus_musculus \
-dbname $DB_NAME \
-pass $DB_PASS \
-name Rbbp5 \
-class 'Transcription Factor' \
-description 'Rbbp5 Transcription Factor Binding'

New entries should be added to the files:
${EFG_SRC}/scripts/import/types/${SPECIES}.CellTypes.txt
${EFG_SRC}/scripts/import/types/FeatureTypes.txt

This way all will be entered when you start a db from scratch

:: Create complex Feature Types
Some Feature Types are actually complexes composed of other Feature Types (eg. dimers)
To create these links, add new entries to the file:
${EFG_SRC}/scripts/import/types/FeatureType_associations.txt

Run the script:
${EFG_SRC}/scripts/import/import_feature_type_associations.pl
Only non-existing links will be added.

:: Add gene links to feature type
Add new entries to the file:
${EFG_SRC}/scripts/import/types/${SPECIES}.FeatureType_Genes.txt

You may need to create a new database entry in the Xref system.
For the moment this needs to be entered by hand in the external_db table in the eFG DB

Run the script:
${EFG_SRC}/scripts/import/import_feature_type_gene_links.pl
Only non-existing links will be added.

:: Add a matrix
The link of a matrix to a feature type is currently manually established.
Use script  ${EFG_SRC}/scripts/import/import_matrix.pl
It requires a folder with Jaspar data (see requirements)

:: Adding a Peak calling method:

You can easily adapt existing Peak Callers by creating new analysis.
For this, just copy an existing analysis in the db, change the logic name and specific parameters.
e.g. 
Copy the SWEMBL_R0025 entry to a new SWEMBL_R005 analysis, and change the parameters

Add the new analysis logic name in the file import/types/Analyses.txt
Add the new analysis logic name in the $VALID_PEAK_ANALYSIS in the sequencing.config file or your instance

To create a new Peak Caller from scratch you need to create a new Runnable, in addition to a new analysis.
See RunSWEmbl or RunCCAT for examples of Runnables.


::TODO (Nathan?)
:: Rollback

Sometimes we may need to remove previous datasets, either because they become or were reentering data

::TODO ?
:: Multi-Species Support(EnsemblGenomes)

The arrays environment and pipeline code has been developed to support the new multi-species aspect
of the EnsemblGenomes DBs. This has a few impacts on setup and configuration of the arrays 
environment.

To enable multi-species support for funcgen databases add the following to 
either arrays.config or the instance file.

export MULTI_SPECIES=1

To turn on core database multi-species support then export the following:

export DNADB_MULTI_SPECIES=1

If multi-species is specified for a core database and no species is found the
environment will exit. However if you specify it for the funcational genomics
database the environment will insert a multi-species entry for the current 
species name if no species ID can be found in the database.

As species are grouped into 'collection' DBs, this means that they will most 
likely share at least some if not all of the experimental data. To 
reduce redundancy in the input data, files should be stored in a collection 
directory and specific species directories soft linked to these e.g

$DATA_HOME/fastq/STAPH_COLLECTION
$DATA_HOME/fastq/STAPHYLOCOCCUS_AUREUS -> $DATA_HOME/fastq/STAPH_COLLECTION/

If there are differences between the data within the collection, then more 
exhaustive individual file links will need to be set up in a standard input 
directory for that species.

All output for the remainder of the pipelien is individual to each species

Since all species are sharing the same database, the pipelines for the
species should not be run in parallell. 

See known issues for race condition problems which can occur when running
multi-species databases from the Ensembl Bacteria project.

:: Multi Species Name Support (SPECIES_COMMON) 

SPECIES_COMMON is an attribute which when set will be used for file names. This
is not a specific multi-species support attribute but more support for
Ensembl Genomes & the various strain names that can appear which are not
file system friendly. If you have a strain: 
 
e.g. Escherichia coli O1:K1 / APEC 

This can be changed to a more filesystem friendly name: 

e.g. E_coli_O1_K1_APEC 

This conversion must be done by yourself but the usage of it is automatic 

:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::


8 Known Issues/Caveats 

- Heavy tmp usage may make some jobs to fail
You can tweak the analysis resources parameters [tmp>xxxx] in appropriate eHive _conf files
This can particularly affect the ReadCounts pipeline, where jobs can fail that are not caught.
In the case jobs fail you may need to restart everything from scratch for the sets that failed.

- Hardcoded perl and naming conventions
Some of the scripts contain hardcoded perl paths and name structures which may need to be edited.
To use the pipeline, you need to comply to a rather strict file name convention 
File Paths are case sensitive, regarding cell type and feature type names

- Some Metadata may be assumed to preexist in the eFG DB
Some data may need to preentered in eFG by hand, like experimental groups, XRef databases, etc...
There are some scripts to help you do this. See Administrative Functions

- The same SCHEMA_BUILD is assumed between the core and the efg dbs.
Errors may occur if this is not the case. E.g. When running the motif matching job RunMotifMatch
In this case you may need to change the SCHEMA_BUILD (e.g. export SCHEMA_BUILD=${YOUR_CORE_SB})	

- chromosome names should be consistent between core and efg dbs, and preferably GRCh
There is some leeway regarding UCSC naming, but it is not guaranteed to work	  

- Problems may occur with missing coordinate systems

When populating the pipeline for species with coordinate systems other than
chromosome e.g. super-contig or plasmid a race condition can occur in the
pipeline causing processes to fail as each one attempts to insert missing
coordinate system(s). To avoid this run the following script before the 
pipeline:

$EFG_PERL $EFG_SCRIPTS/import/import_coord_systems.pl

This loads config from Bio::EnsEMBL::Funcgen::Config::ProbeAlign & imports
the missing coordinate systems. Eventually this import will be done by
the API in a safe manner. The issue does not affect sequence regions insertions.
