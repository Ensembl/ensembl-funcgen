#!/usr/local/bin/bash

# Copyright [1999-2015] Wellcome Trust Sanger Institute and the EMBL-European Bioinformatics Institute
# 
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
#      http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# CONTACT
# Please post comments/questions to the Ensembl development list
# <http://lists.ensembl.org/mailman/listinfo/dev>



echo ':::: Welcome to the eFG peak calling environment'
. $EFG_SRC/scripts/environments/pipeline.env


export ENV_NAME='peaks'
#env colour is red
export PS1_COLOUR=31
PEAKS_CONFIG=${PEAKS_CONFIG:=$EFG_SRC/scripts/environments/peaks.config}
echo ":: Sourcing PEAKS_CONFIG: $PEAKS_CONFIG"
. $PEAKS_CONFIG


### Some convinient aliases
# Some also set in pipeline.env
#alias indexhome?
#Need to set ensembl_scratch_home? Or just migrate working dirs to 'funcgen' space?
#Potential for pipeline clashes in same DB
#Need pipeline instance name? (In new output dir format i.e. dbname/pipeline e.g. homo_sapiens/array_mapping|peaks etc.
#Need to add default assembly here?
alias aligndir='cd /lustre/scratch103/ensembl/funcgen/alignments/$SPECIES'


#To do
#1 Current set up only allows one run to be processed at a time. 
# Need to be able to parallelise thise for multiple inputs. Old approach was to specify different PDBs and 
# have separate work dir based on input. We probably just want to create input IDs on a data set basis. Need to be careful about
# Adding input_ids to a DB where the pipeline is already running. As the config may be the same.  Also need to allow for adding 
# different config and keep existing/running config i.e. do not delete PDB
#2 Integrate Alignment step which runs bwa? Or wait for 1KG pipeline?
#3 run_bwa.sh or mapping pipeline needs to move sam output to parent dir, then we can remove working bwa dir?
#4 Add 'RUNNABLE'_INPUT_TYPE, so we can make the input_type param requirements optional? 
#  As each Runnable will only ever have one input type(file|slice|array)?
#5 Handle non parameter attrs in for analysis  ie. comparison/storage of version info etc?? 
#6 Enable RunPeaks wrapper method
#7 Write PeaksReport, need to map inputs to feature_set names. This can be done through the same method as defining the input_ids
#  So extract this into a separate method based on the same params e.g. input_dir + regex + exp_name?
#  Also set these as vars such that we don't have to pass them to all subsequent functions?
#  How are we going to handle multiple experiments?
#  FEATURE_SET_NAMES These currently have the logic name preppended to the first part of the INPUT_ID?
#  This is currently done in a perl script, not in the env!!
#8 Change naming of dat files written from Runnable::SWEmbl to reflect feature_set name, and hence prevent duplication


################################################################################
# Func      : _InitEnv(
# Desc      : Sets up all config defined variables. Should only be called from
#             instance config file (species_VERSION.arrays) e.g. mouse_51.arrays 
# Args [n]  : 
# Return    : none
# Exception : 
################################################################################

_InitEnv(){

	#Set all generic pipeline stuff e.g. DB vars, PATH/PERL5LIB etc
	_InitPipelineEnv

	#Do a getopts here?
	#How do we make this pass to _InitPipelineEnv?
	#use \?) to build other params string?
	#Will this handle args properly? May have to write BuildUnkownOptions function?
	#This will grab params and reset $OPTIND accordingly
	#This is only for dump/use seqs with matching schema_builds
	#create db



	echo ":: Setting config for $SPECIES array mapping"

	if [ $warn ]
	then
		echo 'NOTE: NIMBLEGEN_TILING will never go through ImportArrays or ProbeTranscriptAlign steps?'
	fi
	
	#ValidateBooleans can we write a method for this
	#Move this to pipeline.env?

	#if [ $MULTI_SPECIES ]; then
#
#		if [ $MULTI_SPECIES -ne 1 ]; then
#		echo 'MULTI_SPECIES is a boolean variable, please specify 1 or omit for default of 0'
#		else
#		#change to script paramter
#			export MULTI_SPECIES=' -multi_species '
#		fi
#	fi
	
	export WORK_DIR=${DATA_HOME}/${DB_NAME}
	#This is used in pipeline alias workdir
	export BACKUP_DIR=${WORK_DIR}/backup
	export PIPELINE_OUT=${WORK_DIR}/peaks_out

	#Create db output dir if not present
	MakeDirs $WORK_DIR
	#Can we make this the input dir somehow?



	#Set TYPES based on the DB?
	#export VALID_CELL_TYPES=$(QueryVal OUT "select name from cell_type")
	#There are potentially thousands on feature types
	#do let's write a stand along method for this.
	#export VALID_FEATURE_TYPES=$(QueryVal OUT "select name from feature_type")
	#Let's do this for both ValidateVariableFromDB?
	#Or let perl do this?

	workdir 
}


################################################################################
# Func      : RunPeaks
# Desc      : Wrapper method to run the whole peak calling pipeline

# Return    : none 
# Exception : none
################################################################################

RunPeaks(){
	#To run this wrapper method for multiple experiments we would need to take one logic name 
	#and all the several sets of params to be use in CreateInputID e.g. input dirs, regexs etc
	#We also need to make this safe for adding inputs to an already exiting pipeline
	#i.e. we want input_id validation and testing for pipeline DB
	
	echo "This wapper function is not yet complete, please use: SetUpPipeline; CreateInputIDs; SubmitPeaks"
	return 1

	#This makes sure we reset the getopts ind if we have used it previously
	OPTIND=1

	array_formats=
	array_file=
	align_types=
	skip_import=
	usage='usage: RunAlignments [ -f format ]*  [ -t ALIGN_TYPE(e.g. GENOMIC) -l custom_array_list.file  -s(kip import) -h(elp)]'


	#Can we take array_names here too?
	#Is this wise to restrict to arrays within a linked set?


	while getopts ":f:l:t:h" opt; do
		case $opt in 
	        f  ) array_formats="$array_formats $OPTARG" ;; 
	t  ) align_types="$align_types $OPTARG" ;;
	l  ) array_file=$OPTARG ;;
	h  ) echo $usage; return 0;;
	\? ) echo $usage; exit 1;;
	esac 
	done


	#Do this here so we don't have to pass to other methods
	SetArrayFormats $array_formats
	
	file_param=
	
	if [[ $array_file ]]; then
		file_param="-l $array_file"
	fi

	#Validate/Set formats and align types
	SetAlignTypes $align_types
	SetArrayFormats $array_formats
	#Do this here so we don't have to pass to other methods
	
	echo "Array Mapping Pipeline Params are:"
    echo "Funcgen DB:      $DB_MYSQL_ARGS"
	echo "Pipeline DB:     $PDB_MYSQL_ARGS"
	echo "DNA DB:          $DNADB_MYSQL_ARGS"
	echo "GENOMICSEQS:     $GENOMICSEQS"
	echo "TRANSCRIPTSEQS:  $TRANSCRIPTSEQS"

	#Don't need to pass array_formats in the following methods
	#Unless we want to change them
	BuildFastas $file_param
	#Build Fastas need updating as it is not fully tested in custom file mode

    SetUpPipeline $skip_import
	
	if [[ ! $skip_import ]]; then
		BackUpTables -t arrays
		ImportArrays 
		ImportWait
	fi
	
    CreateAlignIDs 
    SubmitAlign
	AlignWait
    monitor
	ProbeAlignReport
    
}

#Step we have in peaks are
#WritePipelineConfig > SetUpPipeline
#ImportPipelineConfig > SetUpPipline
#CreateInputIds
#RunAnalysis(Peaks)


#What variables do we need
#feature type? get from db
#cell type? get from db
#Analysis=SWEmbl
#valid_analyses
#Can we cahnge this to also take Cell/Feature
#Change the naming of this as we don't have -h option set or _set?
#Shoudl never be called directly?


SetCellType(){
	cell_type=$1

	if [[ $cell_type ]]; then
		
		ValidateVariable type VALID_CELL_TYPES

		export CELL_TYPE=$cell_type
		echo -e ": Setting CellType:\t$CELL_TYPE"

	elif [[ ! $ALIGN_TYPES ]]; then
		echo "You must pass a valid cell type to SetCellType e.g. $VALID_CELL_TYPES"
		exit
	fi
	
}

SetFeatureType(){
	feature_type=$1

	if [[ $feature_type ]]; then
		
		ValidateVariable type VALID_CELL_TYPES

		export CELL_TYPE=$cell_type
		echo -e ": Setting CellType:\t$CELL_TYPE"

	elif [[ ! $ALIGN_TYPES ]]; then
		echo "You must pass a valid cell type to SetCellType e.g. $VALID_CELL_TYPES"
		exit
	fi
	
}



################################################################################
# Func      : SetUpPipeline
# Desc      : Imports pipeline tables, analyses and rules into DB
# Return    : none 
# Exception : none
################################################################################

#We need to separate the DB generation from this so that we can set up individual analyses whilst we have other stuff running?
#Can we import rules/goals on top of each other?
#We may get a running pipeline picking up the jobs straight away, so we need to make sure evertyhing is in place first. i.e. BuildFastas
#Split into SetUpPipelineDB and WriteConfig

SetUpPipeline(){
	echo ":: SetUpPipeline $*"

	OPTIND=1 	#This makes sure we reset the getopts ind if we have used it previously
	runnable=$RUNNABLE
	logic_names=
	input_type=
	control=
	usage='usage: SetUpPipeline -r(unnable e.g. SWEmbl) -i(nput type e.g. file|slice|array)  [ -l(ogic_name default=runnable)* -h(elp) -c(has control) ]'
	#slice was for nessie
	#array was for ACME?

	#This should than one runnable\logic_name?
	#Also need the ability to add rules/analysis config with out 
	#Deleting old config, also more validation required?
	#output files should be named after logic names?
	#logic name should always map to one input type
	#Can't we just have SWEMBL_INPUT_TYPE etc. and omit input type here?
	#Also need to capture 'non comparable' options
	#i.e. run options which should not differentiate between analysis entries by the PARAMS
	#validation check e.g. input file type sam/bam/bed. Maybe need to set these as separate var

	while getopts ":r:i:l:ch" opt; do
		case $opt in 
	        r  ) runnable=$OPTARG ;; 
            l  ) logic_names="$OPTARG $logic_names" ;;  #Need to trim trailing space
            i  ) input_type=$OPTARG ;;
			c  ) control='HAS_CONTROL=>1,' ;;
	        h  ) echo $usage; return 0;;
	        \? ) echo $usage; exit 1;;
	    esac 
	done

	



	CheckVariables runnable input_type
	ValidateVariableOrUsage "$usage" runnable  VALID_RUNNABLES
	ValidateVariableOrUsage "$usage" input_type VALID_INPUT_TYPES

	#Set and export RUNNABLE/LOGIC_NAMES/INPUT_TYPE for use by other funcs?
	#Need to split this func first?
	#Check how arrays.env handles SetArraysFormats i.e. overwrite with param of use existing value

	#We need to set the logic_name as runnable if absent and test params in DB match those in config for runnable
	logic_names=${logic_names:=$LOGIC_NAMES}
	logic_names=$(echo $logic_names | sed s'/ $//')

	if [ ! "$logic_names" ]; then
		logic_names=$runnable
	fi
	
	#Should we export RUNNABLE(S), LOGIC_NAME(S) and INPUT_TYPE here/from RunPeaks
	#For use in subsequent functions(SetLogicNames etc?)

	#Need to do the same with version (and other analysis fields?)
	#Genericise all of above into two methods in pipeline.env
	#setVarByName and validateExistingRecord or something?
	#Then we can use these to validate all the analysis config vars for a specific runnable/logic name?
	#Only likely going to want to test params, version and program_file (maybe display_label and description)
	#All this is probably overkill.
	#There are probably input type params which do not need to be compared here.
	#i.e. input file type sam/bed etc
	#Does the SWEmbl runnable take sam?


	#Set up analysis and rule config files
	analysis_conf_file=$WORK_DIR/analysis.conf
	rules_conf_file=$WORK_DIR/rules.conf
	batch_queue_file=$WORK_DIR/batch_queue.pm
	#To get this to work with multiple runnables
	#We just need to nest the config in a runnable key, and call that specific element in the runnable config module
	#Can't yet have multiple runnables due to differing input types
	runnable_conf_file=$WORK_DIR/runnable_config.pm

	BackUpFile $analysis_conf_file
	rm -f $analysis_conf_file
	touch $analysis_conf_file

	BackUpFile $rules_conf_file
	rm -f $rules_conf_file
	touch $rules_conf_file

	BackUpFile $batch_queue_file
	rm -f $batch_queue_file
	touch $batch_queue_file

	BackUpFile $runnable_conf_file
	rm -f $runnable_conf_file
	touch $runnable_conf_file


	echo ": Writing analysis and rules config"

	#Can't add params to analysis here as we don't have access yet, but we should do that somewhere?
	#Could do it in the Runnable DB, but highly redundant, as this will be tested/overwritten for each job i.e. thousands of times.
	#Space separate logic names to allow readability in Analysis::Config modules
    #added null modules to stop warnings on import	
    #These will not overwrite current entries in analysis, so we may have some old data listed
	#Need to validate this?
	
	#Set up start of local batch_queue.pm


	echo  "our @RUNNABLE_CONFIG = (" >> $runnable_conf_file

	echo "our @QUEUE_CONFIG = (" 	>> $batch_queue_file


	for logic_name in $logic_names; do

		PARAMS_VAR=$(echo $logic_name | tr [a-z] [A-Z])
		PARAMS_VAR="${PARAMS_VAR}_PARAMS"
    	#Now get runnable params based on the runnable name
		RUNNABLE_PARAMS=$(eval "echo \$$PARAMS_VAR")
	    #Check PARAMS_VAR so we get a useable error message
		CheckVariables $PARAMS_VAR

	#Now validate this match the params in the DB, other wise we will have to specify a different logic_name
	#ValidateVariableFromDB "$RUNNABLE_PARAMS" "select params from analysis where logic_name='$logic_name'"
	#We may not have an analysis record yet
		db_params=$(QueryVal OUT "select parameters from analysis where logic_name='$logic_name'")

		if [ "$db_params" ] && [[ "$db_params" != "$RUNNABLE_PARAMS" ]]; then
			echo -e "Found analysis params mistmach for $logic_name:\n\t$PARAMS_VAR\t$RUNNABLE_PARAMS\n\tDB\t\t$db_params"
			echo -e "Please correct $PARAMS_VAR or specify a different logic_name"
			exit 1;
		fi	


		echo "'$logic_name' => {
		PARAMETERS => '$RUNNABLE_PARAMS',
        $control
},"  >> $runnable_conf_file


	#How are we going to write runnable specific info here
	#We need to set it in peaks.conf
	#and validate here?
	#Some fancy evaling for program path
	#Can we do that for valid input types for runnable too?
	

		echo "{
             logic_name => '$logic_name',
             queue => '$NORMAL_QUEUE', 
             batch_size => 1, 
             resources => 'select[type==X86_64 && ${DB_HOST_LSFNAME}<80] rusage[${DB_HOST_LSFNAME}=10:duration=10]',
             retries => 3,
             runnabledb_path => 'Bio/EnsEMBL/Analysis/RunnableDB/Funcgen',
             cleanup => 'no',
            }," 	>> $batch_queue_file


	#We really need to do some validation of the parameters
	
	
		echo "[Submit${logic_name}]
input_id_type=$input_type

[$logic_name]
program=$program
module=$runnable
input_id_type=$input_type
" >> $analysis_conf_file
        
		echo "
[$logic_name]
condition=Submit${logic_name}
">> $rules_conf_file
   #condition=Submit${Submit}\n\n    #???

#Could add some a wait here before doing some post run QC
	done

#Finish off config files

	echo ");
" >> $runnable_conf_file

	echo ");
" 	>> $batch_queue_file


	CreatePipelineTables
   
	#Could do with testing for files here
	echo ": Importing analysis config" 

	#EFG
    Execute $EFG_PERL $PIPELINE_SCRIPTS/analysis_setup.pl $PDB_SCRIPT_ARGS -read -file $analysis_conf_file

    echo ": Importing rules config" 
	#EFG
    Execute $EFG_PERL $PIPELINE_SCRIPTS/rule_setup.pl  $PDB_SCRIPT_ARGS -read -file $rules_conf_file

	#Could we test output for 'Not storing' This will not detect whether there are other rules in the DB

 	if [ $warn ]; then
 		echo "NOTE: Need to clean analysis table? Or is this a setup problem"
 		echo "NOTE: check for genomic seq here, are all the chrs present in $GENOMICSEQS"
 		#This would requite getting all toplevel (inc non-ref) and greping file
 		#Is this another case for PipeLineHelper.pm
	 fi
 	
	Execute $EFG_PERL $PIPELINE_SCRIPTS/setup_batchqueue_outputdir.pl 
 	#This will always fail and return 0?!
 	#As we don't have any input_id written for the accumulator yet
 	#This needs to be done after we have created the input ids for ImportArrays

	#echo "NOTE: The following 'analysis Submit_*' and accumulator warnings can largely be ignored" 

 	CheckPipelineSanity

 	echo ""

}


#To do, allow this to take one input file path via -I

CreateInputIDs(){

	OPTIND=1 	#This makes sure we reset the getopts ind if we have used it previously
	logic_names=$LOGIC_NAMES #Overwritten if -l defined
	input_type=$INPUT_TYPE
	input=
	#These need setting in RunPeaks
	exp_regex=
	exp_suffix=
	zip=
	usage="Usage:\tCreateInputIds -l(ogic_name e.g. SWEmbl) -i(input_type e.g. slice|file|array) [ -I(nput e.g. (toplevel|encode)|/input/dir/  -r(egex e.g. 'CD4_H3.*gz' default is '.*') -e(xperiment name e.g. AUTHOR_PMID default is parsed from -input) -z(ip input files)]"

	#The RunnableDB expect overloaded file names to encode the feature/cell typeinfo
	#We really need to remove this and set as env vars to be used in the config?
	#Can't do this if we are running mutiple datasets side by side

	#We also need to be mindful about how to get the cell/feature_type from GEO/SRA etc
	#Do we really need to rename the files, or can we just place them in a CELLTYPE_FEATURETYPE sub directory
	#under the AUTHOR_PMID:NNNNN parent dir
	#We already have these as:
	#ESHyb_H3K36me3.samse.sam or a single bed file
	#Integrate mapping func which calls run_bwa.sh?

	#Can we run this for multiple data sets before starting the pipeline?
	#Or in fact after the pipeline has been started?
	#i.e. should we check for pipeline lock here for safety?


	while getopts ":l:i:I:r:e:zh" opt; do
		case $opt in 
            l  ) logic_names=$OPTARG ;;
            i  ) input_type=$OPTARG ;;
            I  ) input=$OPTARG ;;
            r  ) exp_regex="-exp_regex $OPTARG" ;;
            e  ) exp_suffix="-exp_suffix $OPTARG" ;;
            z  ) zip='-zip';;
	        h  ) echo -e $usage; return 0;;
	        \? ) echo -e $usage; exit 1;;
	    esac 
	done

	CheckVariables logic_names input_type
	ValidateVariableOrUsage "$usage" input_type VALID_INPUT_TYPES
	
	if [[ $input_type != array ]]; then
		CheckVariables input
	fi


	#Set default exp_suffix based on input_dir
	if [ ! "$exp_suffix" ]; then
	
		if [[ $input_type = file ]]; then
	
			exp_suffix=$(echo $input |sed 's/\/$//') # Strip trailing /
			exp_suffix=$(echo $input |sed 's/.*\///')

			if [[ $exp_suffix != *_PMID[0-9]* ]]; then
				echo -e "Could not automatically define experiment name from input:\t$input"
				echo -e "Either specify -e(xperiment name) or rename input dir as follows: EXPNAME_PMID[0-9]"
				exit
			else
				echo  -e "Setting exp_suffix to $exp_suffix"
				exp_suffix="-exp_suffix $exp_suffix"
			fi
			
		else
			echo -e "Can only automatically define experiment name for input_type=file, please define -e(xperiment name)"
			exit
		fi
	fi

	PARAMS=

	
    case $input_type in
        'file'  ) CheckDirs $input; PARAMS="-dir $input" ;;	                
		'slice' ) if [[ $input != 'encode' ]] && [[ $input != 'toplevel' ]]; then 
		             echo -e "Invalid $input_type input:\t$input\n$usage"  
					 exit
				  fi 
				  PARAMS="-${input_type}  -${input}" ;;
		'array' ) PARAMS="-${input_type}" ;;
        *       ) echo "Invalid input_type.\n$usage"; exit ;;
    esac


    # determine analysis_id for SubmitType and write input_ids
	$EFG_SRC/scripts/pipeline/configure_inputs.pl -logic_names $logic_names $DB_SCRIPT_ARGS $DNADB_SCRIPT_ARGS $PARAMS $exp_regex $exp_suffix -species $SPECIES -work_dir $PIPELINE_OUT $zip


    if [ $? == 0 ]; then 
		#mysqlpipe < ${WORK_DIR}/input_ids.sql


        echo "Ready for SubmitPeaks"
    else 
        echo "An error occured while inserting input_ids. You may double check your"
        echo "analysis pipeline isn't configured properly (see WritePipelineConfig)"
        echo "or you are trying to import input ids that have already been imported"
        echo "(use CleanInputIds to drop input_ids and rerun CreateInputIds)."
    fi
    
}

CleanInputIds(){

    if [ $# -ne 1 ]; then
        echo "Usage: CleanInputIds <password>"
        return
    fi
    
    PASS=$1
    shift

	echo "Cleaning input_id_analysis table"
	echo "delete from input_id_analysis" | mysqlw -p$PASS $PDBNAME

	echo "removing links from infiles directory ..."
	for file in ${ANALYSIS_WORK_DIR}/infiles/*; do
		if [ -L $file ]; then 
			rm -f $file
		fi
	done
}



TestRunnable(){
	echo ":: TestRunnable $*"

	write=
	logic_name=
	input_id=
	usage='usage: TestRunnable -l(ogic_name e.g. SWEmbl(case insensitive!) [ -i(input_id e.g. file_name.sam.gz) -w(rite flag) ] [ -h(elp) ]'

	#This makes sure we reset the getopts ind if we have used it previously
	OPTIND=1

	while getopts ":l:w:i:h" opt; do
		case $opt in 
			l  ) logic_name=$OPTARG ;;
			w  ) write=' -write ' ;;
		    i  ) input_id=$OPTARG ;;
			h  ) echo $usage; return 0 ;;
		    \? ) echo $usage; return 1 ;;
		esac 
	done


	#ValidateLogicName?

	if [ ! $input_id ]; then
		echo "SELECT input_id from input_id_analysis where input_id_type='Submit${logic_name}' limit 1"

		input_id=$(QueryVal PIPELINE "SELECT input_id from input_id_analysis where input_id_type='Submit${logic_name}' limit 1")

		if [ ! $input_id ]; then
			echo -e "Could not detect input_id for logic_name:\t$logic_name"
			exit
		fi
		
		echo -e "No input_id specified, using:\t$input_id"
	fi

	
	

	#Need to query for inpud_id 
	
	cmd="$ANALYSIS_SCRIPTS/test_RunnableDB $PDB_SCRIPT_ARGS -logic_name $logic_name -input_id $input_id $write"
	echo $cmd	

	#This currently does not work because the testRunnableDB script does not look in the Funcgen subdir!
	#Could specify the full module path? Or alter RunnableDB to specify optional RunnableDB dir defaulting to standard
	time $EFG_PERL $cmd	

	if [ $write ]; then
		echo -e "WARNING:\t-write flag was set. Re-running this job via the pipeline will create duplicate output"
		echo -e "\t\tUse MarkJobAsComplete before starting the pipeline to avoid this(needs writing in pipeline.env!!!)"
	fi

}



SubmitPeaks(){	
	echo ":: SubmitPeaks $*"

	OPTIND=1
	logic_names=
	usage='usage: SubmitPeaks [ -l(ogic_names e.g. SWEmbl) ]+ [ -h(elp) ]
Simply submits the Peak jobs defined by CreateInputIDs';

	#We could add logic_name/analysis here.

	while getopts ":l:h" opt; do
		case $opt in 
		    l  ) logic_names="$logic_names $OPTARG" ;;
	            h  ) echo -e $usage; return 0;;
                    \? ) echo -e $usage; return 1;;
		esac 
	done


	#Should put this as ValidateLogicName in pipeline.env
	logic_names=${logic_names:=$LOGIC_NAMES}
	analyses=
	lname=

	if [ "$logic_names" ]; then

		for logic_name in $logic_names; do
			lname=$(QueryVal PIPELINE "select input_id_type from input_id_analysis ia, analysis a where ia.analysis_id=a.analysis_id and a.logic_name='Submit${logic_name}' limit 1")

			if [ ! $lname ]; then
				echo -e "Could not find input_ids to submit the analysis with logic_name:\t$logic_name"
				exit
			fi

			analyses="$analyses -analysis $logic_name"
		done
	else
		echo "No logic_names specified"
		exit
		
	fi


	echo "Should check for gzip jobs here"
	cmd="$EFG_PERL $PIPELINE_SCRIPTS/rulemanager.pl $PDB_SCRIPT_ARGS $analyses"
	echo $cmd


	Execute $cmd  2>&1 | tee -a $WORK_DIR/pipeline.out


	echo "Now do PeaksReport when finished"
}

################################################################################
# Func      : PeaksWait
# Desc      : Accumulator step to wait for all Peak jobs
# Arg[1..n] : None
# Return    : None 
# Exception : None
################################################################################


PeaksWait(){
	echo ":: AlignWait $*"
	#add opts here for -force_accumulators and -once?

	cmdline="$EFG_PERL $PIPELINE_SCRIPTS/rulemanager.pl $PDB_SCRIPT_ARGS -force_accumulators -analysis Align_Wait"

	#echo ":: ImportWait $cmdline"

	Execute $cmdline
	
	#echo "Can we cat the AFFY and AFFY_ST arrays_nr files?"
	#As these should use the same rules?  This would mean running with one logic name for both
	#Just keep separate for now

	#Now cat the array names files output by ImportArrays
	#This is to enable any ProbeFeature/IDXref/UnmappedObject deletes

	echo ": Finished waiting...ready for RunTranscriptXrefs?"
}



#To do
#This should really be moved to efg.env, so we can do this without being in a particular env
#and can also do it on the RegBuild?
#Move this to perl script/EFGUtils and wrap in efg.env func?
#Restrict to single fsets or fset names like "%exp_name%";

PeaksReport(){
	echo ":: PeaksReport $*"
	
	
	declare -a logic_names
	#We are not treating this like a true array, rather modifying just first element as a space separated string
	#Which behaves like an array in for loops

	


	usage='usage: PeaksReport [-l(ogic_names e.g. SWEmbl)]+ [-h help]'

	OPTIND=1

	while getopts ":l:h" opt; do
	    case $opt in 
		l  ) logic_names="$logic_names $OPTARG";;
	        \? ) echo $usage; exit 1;;
	        h  ) echo $usage; return 0;;
	     esac 
	done		

	
	report_file=${WORK_DIR}/Peaks.report
	echo ": Generating Peaks Report: $report_file"	
	BackUpFile $report_file

	query="SELECT fset.name as 'Set Name', COUNT(*) as 'Number of Peaks', AVG(af.seq_region_end - af.seq_region_start) as 'Average Peak Length', STDDEV_POP(af.seq_region_end - af.seq_region_start) as 'Standard Deviation of Peak Length' FROM annotated_feature af, feature_set fset WHERE af.feature_set_id=fset.feature_set_id"

	if [ "$logic_names" ]; then
	    query="${query} AND ("
	    add=""


		#This is unsafe as the name are not guaranteed to contain the logic names
		#Change to join to the analysis table

	    for logic_name in $logic_names; do
		if [ "${add}" ]; then
		    add="${add} OR fset.name LIKE '%${logic_name}%'"
		else
		    add="fset.name LIKE '%${logic_name}%'"
		fi
	    done

	    query="${query} ${add} )"

	fi

	query="${query} GROUP BY fset.name;"
	
	#echo "Query: ${query}"

	echo $query | mysql $DB_MYSQL_ARGS > $report_file
	cat $report_file
	
}
	
