#!/usr/local/bin/bash

#LICENSE
#
#  Copyright (c) 1999-2009 The European Bioinformatics Institute and
#  Genome Research Limited.  All rights reserved.
#
#  This software is distributed under a modified Apache license.
#  For license details, please see
#
#    http://www.ensembl.org/info/about/code_licence.html
#
# LICENCE:
# This code is distributed under an Apache style licence. Please see
# http://www.ensembl.org/info/about/code_licence.html for details.
#
# CONTACT
# Please post comments/questions to the Ensembl development list
# <ensembl-dev@ebi.ac.uk>



echo ':::: Welcome to the eFG peak environment'

#export trap_exit=1
#Also change so we don't need absolute path as we can source from the same directory 

. $EFG_SRC/scripts/environments/pipeline.env


export ENV_NAME='peaks'
#env colour is red
export PS1_COLOUR=31
PEAKS_CONFIG=${ARRAYS_CONFIG:=$EFG_SRC/scripts/environments/peaks.config}
echo ":: Sourcing PEAKS_CONFIG: $PEAKS_CONFIG"
. $PEAKS_CONFIG


### Some convinient aliases
# Some also set in pipeline.env
alias arraysdir='cd $ARRAYS_HOME' 


#To do
#1 Current set up only allows one run to be processed at a time. 
# Need to be able to parallelise thise for multiple inputs. Old approach was to specify different PDBs and 
# have separate work dir based on input. We probably just want to create input IDs on a data set basis. Need to be careful about
# Adding input_ids to a DB where the pipeline is already running. As the config may be the same.  Also need to allow for adding 
# different config and keep existing/running config i.e. do not delete PDB
#2 Integrate Alignment step which runs bwa? Or wait for 1KG pipeline?
#3 run_bwa.sh or mapping pipeline needs to move sam output to parent dir, then we can remove working bwa dir?
#4 Add 'RUNNABLE'_INPUT_TYPE, so we can make the input_type param requirements optional? 
#  As each Runnable will only ever have one input type(file|slice|array)?
#5 Handle non parameter attrs in for analysis  ie. comparison/storage of version info etc?? 

################################################################################
# Func      : _InitEnv(
# Desc      : Sets up all config defined variables. Should only be called from
#             instance config file (species_VERSION.arrays) e.g. mouse_51.arrays 
# Args [n]  : 
# Return    : none
# Exception : 
################################################################################

_InitEnv(){

	#Set all generic pipeline stuff e.g. DB vars, PATH/PERL5LIB etc
	_InitPipelineEnv

	#Do a getopts here?
	#How do we make this pass to _InitPipelineEnv?
	#use \?) to build other params string?
	#Will this handle args properly? May have to write BuildUnkownOptions function?
	#This will grab params and reset $OPTIND accordingly
	#This is only for dump/use seqs with matching schema_builds
	#create db



	echo ":: Setting config for $SPECIES array mapping"

	if [ $warn ]
	then
		echo 'NOTE: NIMBLEGEN_TILING will never go through ImportArrays or ProbeTranscriptAlign steps?'
	fi
	
	#ValidateBooleans can we write a method for this
	#Move this to pipeline.env?

	#if [ $MULTI_SPECIES ]; then
#
#		if [ $MULTI_SPECIES -ne 1 ]; then
#		echo 'MULTI_SPECIES is a boolean variable, please specify 1 or omit for default of 0'
#		else
#		#change to script paramter
#			export MULTI_SPECIES=' -multi_species '
#		fi
#	fi
	
	export WORK_DIR=${DATA_HOME}/${DB_NAME}
	#This is used in pipeline alias workdir
	export BACKUP_DIR=${WORK_DIR}/backup
	export PIPELINE_OUT=${WORK_DIR}/peaks_out

	#Create db output dir if not present
	MakeDirs $WORK_DIR
	#Can we make this the input dir somehow?



	#Set TYPES based on the DB
	#export VALID_CELL_TYPES=$(QueryVal OUT "select name from cell_type")
	#There are potentially thousands on feature types
	#do let's write a stand along method for this.
	#export VALID_FEATURE_TYPES=$(QueryVal OUT "select name from feature_type")
	#Let's do this for both ValidateVariableFromDB?
	#Or let perl do this?

	workdir 
}


################################################################################
# Func      : RunPeaks
# Desc      : Wrapper method to run the whole peak calling pipeline

# Return    : none 
# Exception : none
################################################################################

RunPeaks(){
	#Make this take multiple formats?
	#Is this possible to combine with the file?
	#Are we getting confused with logic names here? ProbeAlign ProbeTranscriptAlign?
	
	echo "this has not be written yet"

	#This makes sure we reset the getopts ind if we have used it previously
	OPTIND=1

	array_formats=
	array_file=
	align_types=
	skip_import=
	usage='usage: RunAlignments [ -f format ]*  [ -t ALIGN_TYPE(e.g. GENOMIC) -l custom_array_list.file  -s(kip import) -h(elp)]'


	#Can we take array_names here too?
	#Is this wise to restrict to arrays within a linked set?


	while getopts ":f:l:t:hs" opt; do
		case $opt in 
	        f  ) array_formats="$array_formats $OPTARG" ;; 
	t  ) align_types="$align_types $OPTARG" ;;
	s  ) skip_import=' -s ' ;;
	l  ) array_file=$OPTARG ;;
	h  ) echo $usage; return 0;;
	\? ) echo $usage; exit 1;;
	esac 
	done


	#Do this here so we don't have to pass to other methods
	SetArrayFormats $array_formats
	
	file_param=
	
	if [[ $array_file ]]; then
		file_param="-l $array_file"
	fi

	#Validate/Set formats and align types
	SetAlignTypes $align_types
	SetArrayFormats $array_formats
	#Do this here so we don't have to pass to other methods
	
	echo "Array Mapping Pipeline Params are:"
    echo "Funcgen DB:      $DB_MYSQL_ARGS"
	echo "Pipeline DB:     $PDB_MYSQL_ARGS"
	echo "DNA DB:          $DNADB_MYSQL_ARGS"
	echo "GENOMICSEQS:     $GENOMICSEQS"
	echo "TRANSCRIPTSEQS:  $TRANSCRIPTSEQS"

	#Don't need to pass array_formats in the following methods
	#Unless we want to change them
	BuildFastas $file_param
	#Build Fastas need updating as it is not fully tested in custom file mode

    SetUpPipeline $skip_import
	
	if [[ ! $skip_import ]]; then
		BackUpTables -t arrays
		ImportArrays 
		ImportWait
	fi
	
    CreateAlignIDs 
    SubmitAlign
	AlignWait
    monitor
	ProbeAlignReport
    
}

#Step we have in peaks are
#WritePipelineConfig > SetUpPipeline
#ImportPipelineConfig > SetUpPipline
#CreateInputIds
#RunAnalysis(Peaks)


#What variables do we need
#feature type? get from db
#cell type? get from db
#Analysis=SWEmbl
#valid_analyses
#Can we cahnge this to also take Cell/Feature
#Change the naming of this as we don't have -h option set or _set?
#Shoudl never be called directly?


SetCellType(){
	cell_type=$1

	if [[ $cell_type ]]; then
		
		ValidateVariable type VALID_CELL_TYPES

		export CELL_TYPE=$cell_type
		echo -e ": Setting CellType:\t$CELL_TYPE"

	elif [[ ! $ALIGN_TYPES ]]; then
		echo "You must pass a valid cell type to SetCellType e.g. $VALID_CELL_TYPES"
		exit
	fi
	
}

SetFeatureType(){
	feature_type=$1

	if [[ $feature_type ]]; then
		
		ValidateVariable type VALID_CELL_TYPES

		export CELL_TYPE=$cell_type
		echo -e ": Setting CellType:\t$CELL_TYPE"

	elif [[ ! $ALIGN_TYPES ]]; then
		echo "You must pass a valid cell type to SetCellType e.g. $VALID_CELL_TYPES"
		exit
	fi
	
}



################################################################################
# Func      : SetUpPipeline
# Desc      : Imports pipeline tables, analyses and rules into DB
# Return    : none 
# Exception : none
################################################################################

#We need to separate the DB generation from this so that we can set up individual analyses whilst we have other stuff running?
#Can we import rules/goals on top of each other?
#We may get a running pipeline picking up the jobs straight away, so we need to make sure evertyhing is in place first. i.e. BuildFastas
#


SetUpPipeline(){
	echo ":: SetUpPipeline $*"

	OPTIND=1 	#This makes sure we reset the getopts ind if we have used it previously
	runnable=
	logic_name=
	input_type=
	usage='usage: SetUpPipeline -r(unnable e.g. SWEmbl  -i(nput type e.g. file|slice|array)  [-l(ogic_name default=runnable) -h(elp) ]'
	#slice was for nessie
	#array was for ACME?

	#This should than one runnable\logic_name?
	#Also need the ability to add rules/analysis config with out 
	#Deleting old config, also more validation required?
	#output files should be named after logic names?
	#logic name should always map to one input type
	#Can't we just have SWEMBL_INPUT_TYPE etc. and omit input type here?
	#Also need to capture 'non comparable' options
	#i.e. run options which should not differentiate between analysis entries by the PARAMS
	#validation check e.g. input file type sam/bam/bed. Maybe need to set these as separate var

	while getopts ":r:i:l:h" opt; do
		case $opt in 
	        r  ) runnable=$OPTARG ;; 
            l  ) logic_name=$OPTARG ;;
            i  ) input_type=$OPTARG ;;
	        h  ) echo $usage; return 0;;
	        \? ) echo $usage; exit 1;;
	    esac 
	done

	CheckVariables runnable input_type
	ValidateVariableOrUsage "$usage" runnable  VALID_RUNNABLES
	ValidateVariableOrUsage "$usage" input_type VALID_INPUT_TYPES

	#We need to set the logic_name as runnable if absent and test params in DB match those in config for runnable
	if [ ! $logic_name ]; then
		logic_name=$runnable
	fi
	
	#Should we export RUNNABLE(S), LOGIC_NAME(S) and INPUT_TYPE here/rom RunPeaks
	#For use in subsequent functions(SetLogicNames etc?)


	PARAMS_VAR=$(echo $runnable | tr [a-z] [A-Z])
	PARAMS_VAR="${PARAMS_VAR}_PARAMS"
	#Now get runnable params based on the runnable name
	RUNNABLE_PARAMS=$(eval "echo \$$PARAMS_VAR")
	#Check PARAMS_VAR so we get a useable error message
	CheckVariables $PARAMS_VAR

	#Now validate this match the params in the DB, other wise we will have to specify a different logic_name
	#ValidateVariableFromDB "$RUNNABLE_PARAMS" "select params from analysis where logic_name='$logic_name'"
	#We may not have an analysis record yet
	db_params=$(QueryVal OUT "select parameters from analysis where logic_name='$logic_name'")

	if [ "$db_params" ] && [[ "$db_params" != "$RUNNABLE_PARAMS" ]]; then
		echo -e "Found analysis params mistmach for $logic_name:\n\t$PARAMS_VAR\t$RUNNABLE_PARAMS\n\tDB\t\t$db_params"
		echo -e "Please correct $PARAMS_VAR or specify a different logic_name"
		exit 1;
	fi

	#Need to do the same with version (and other analysis fields?)
	#Genericise all of above into two methods in pipeline.env
	#setVarByName and validateExistingRecord or something?
	#Then we can use these to validate all the analysis config vars for a specific runnable/logic name?
	#Only likely going to want to test params, version and program_file (maybe display_label and description)
	#All this is probably overkill.
	#There are probably input type params which do not need to be compared here.
	#i.e. input file type sam/bed etc
	#Does the SWEmbl runnable take sam?


	#Set up analysis and rule config files
	analysis_conf_file=$WORK_DIR/analysis.conf
	rules_conf_file=$WORK_DIR/rules.conf
	batch_queue_file=$WORK_DIR/batch_queue.pm
	#To get this to work with multiple runnables
	#We just need to nest the config in a runnable key, and call that specific element in the runnable config module
	#Can't yet have multiple runnables due to differing input types
	runnable_conf_file=$WORK_DIR/runnable_config.pm

	BackUpFile $analysis_conf_file
	rm -f $analysis_conf_file
	touch $analysis_conf_file

	BackUpFile $rules_conf_file
	rm -f $rules_conf_file
	touch $rules_conf_file

	BackUpFile $batch_queue_file
	rm -f $batch_queue_file
	touch $batch_queue_file

	BackUpFile $runnable_conf_file
	rm -f $runnable_conf_file
	touch $runnable_conf_file


	echo ": Writing analysis and rules config"

	#Can't add params to analysis here as we don't have access yet, but we should do that somewhere?
	#Could do it in the Runnable DB, but highly redundant, as this will be tested/overwritten for each job i.e. thousands of times.
	#Space separate logic names to allow readability in Analysis::Config modules
    #added null modules to stop warnings on import	
    #These will not overwrite current entries in analysis, so we may have some old data listed
	#Need to validate this?
	
	#Set up start of local batch_queue.pm

	echo  "
our @RUNNABLE_CONFIG = ('$logic_name' => {
		PARAMETERS => '$RUNNABLE_PARAMS',
});
" >> $runnable_conf_file

	#How are we going to write runnable specific info here
	#We need to set it in peaks.conf
	#and validate here?
	#Some fancy evaling for program path
	#Can we do that for valid input types for runnable too?
	

	echo "our @QUEUE_CONFIG = (
{
             logic_name => '$logic_name',
             queue => '$NORMAL_QUEUE', 
             batch_size => 1, 
             resources => 'select[type==X86_64 && ${DB_HOST_LSFNAME}<80] rusage[${DB_HOST_LSFNAME}=10:duration=10]',
             retries => 3,
             runnabledb_path => 'Bio/EnsEMBL/Analysis/RunnableDB/Funcgen',
             cleanup => 'no',
            });
" 	>> $batch_queue_file



	#We really need to do some validation of the parameters
	
	
	echo "[Submit${logic_name}]
input_id_type=$input_type

[$logic_name]
program=$program
module=$runnable
input_id_type=$input_type
" >> $analysis_conf_file
        
echo "
[$logic_name]
condition=Submit${logic_name}
">> $rules_conf_file
   #condition=Submit${Submit}\n\n    #???

#Could add some a wait here before doing some post run QC


	CreatePipelineTables
   
	#Could do with testing for files here
	echo ": Importing analysis config" 

	#EFG
    Execute $EFG_PERL $PIPELINE_SCRIPTS/analysis_setup.pl $PDB_SCRIPT_ARGS -read -file $analysis_conf_file

    echo ": Importing rules config" 
	#EFG
    Execute $EFG_PERL $PIPELINE_SCRIPTS/rule_setup.pl  $PDB_SCRIPT_ARGS -read -file $rules_conf_file

	#Could we test output for 'Not storing' This will not detect whether there are other rules in the DB

 	if [ $warn ]; then
 		echo "NOTE: Need to clean analysis table? Or is this a setup problem"
 		echo "NOTE: check for genomic seq here, are all the chrs present in $GENOMICSEQS"
 		#This would requite getting all toplevel (inc non-ref) and greping file
 		#Is this another case for PipeLineHelper.pm
	 fi
 	
	Execute $EFG_PERL $PIPELINE_SCRIPTS/setup_batchqueue_outputdir.pl 
 	#This will always fail and return 0?!
 	#As we don't have any input_id written for the accumulator yet
 	#This needs to be done after we have created the input ids for ImportArrays

	#echo "NOTE: The following 'analysis Submit_*' and accumulator warnings can largely be ignored" 

 	CheckPipelineSanity

 	echo ""

}


CreateInputIDs(){

	OPTIND=1 	#This makes sure we reset the getopts ind if we have used it previously
	logic_name=
	input_type=
	input=
	#These need setting in RunPeaks
	exp_regex=
	exp_suffix=
	zip=
	usage="Usage:\tCreateInputIds -l(ogic_name e.g. SWEmbl) -i(input_type e.g. slice|file|array) [ -I(nput e.g. (toplevel|encode)|/input/dir/  -r(egex e.g. 'CD4_H3.*gz' default is '.*') -e(xperiment name e.g. AUTHOR_PMID default is parsed from -input) -z(ip input files)]"

	#The RunnableDB expect overloaded file names to encode the feature/cell typeinfo
	#We really need to remove this and set as env vars to be used in the config?
	#Can't do this if we are running mutiple datasets side by side

	#We also need to be mindful about how to get the cell/feature_type from GEO/SRA etc
	#Do we really need to rename the files, or can we just place them in a CELLTYPE_FEATURETYPE sub directory
	#under the AUTHOR_PMID:NNNNN parent dir
	#We already have these as:
	#ESHyb_H3K36me3.samse.sam or a single bed file
	#Integrate mapping func which calls run_bwa.sh?

	#Can we run this for multiple data sets before starting the pipeline?
	#Or in fact after the pipeline has been started?
	#i.e. should we check for pipeline lock here for safety?


	while getopts ":l:i:I:r:e:zh" opt; do
		case $opt in 
            l  ) logic_name=$OPTARG ;;
            i  ) input_type=$OPTARG ;;
            I  ) input=$OPTARG ;;
            r  ) exp_regex="-exp_regex $OPTARG" ;;
            s  ) exp_suffix="-exp_suffix $OPTARG" ;;
            z  ) zip='-zip';;
	        h  ) echo -e $usage; return 0;;
	        \? ) echo -e $usage; exit 1;;
	    esac 
	done

	CheckVariables logic_name input_type
	ValidateVariableOrUsage "$usage" input_type VALID_INPUT_TYPES
	
	if [[ $input_type != array ]]; then
		CheckVariables input
	fi


	#Set default exp_suffix based on input_dir
	if [ ! $exp_suffix ]; then
	
		if [[ $input_type = file ]]; then
	
			exp_suffix=$(echo $input |sed 's/\/$//') # Strip trailing /
			exp_suffix=$(echo $input |sed 's/.*\///')

			if [[ $exp_suffix != *_PMID[0-9]* ]]; then
				echo -e "Could not automatically define experiment name from input:\t$input"
				echo -e "Either specify -e(xperiment name) or rename input dir as follows: EXPNAME_PMID[0-9]"
				exit
			else
				echo  -e "Setting exp_suffix to $exp_suffix"
				exp_suffix="-exp_suffix $exp_suffix"
			fi
			
		else
			echo -e "Can only automatically define experiment name for input_type=file, please define -e(xperiment name)"
			exit
		fi
	fi

	PARAMS=

    case $input_type in
        'file'  ) CheckDirs $input; PARAMS="-dir $input" ;;	                
		'slice' ) if [[ $input != 'encode' ]] && [[ $input != 'toplevel' ]]; then 
		             echo -e "Invalid $input_type input:\t$input\n$usage"  
					 exit
				  fi 
				  PARAMS="-${input_type}  -${input}" ;;
		'array' ) PARAMS="-${input_type}" ;;
        *       ) echo "Invalid input_type.\n$usage"; exit ;;
    esac

    # determine analysis_id for SubmitType and write input_ids
	$EFG_SRC/scripts/pipeline/configure_inputs.pl -logic_name $logic_name $DB_SCRIPT_ARGS $DNADB_SCRIPT_ARGS $PARAMS $exp_regex $exp_suffix -species $SPECIES -work_dir $PIPELINE_OUT $zip


    if [ $? == 0 ]; then 
		#mysqlpipe < ${WORK_DIR}/input_ids.sql


        echo "Ready for SubmitPeaks"
    else 
        echo "An error occured while inserting input_ids. You may double check your"
        echo "analysis pipeline isn't configured properly (see WritePipelineConfig)"
        echo "or you are trying to import input ids that have already been imported"
        echo "(use CleanInputIds to drop input_ids and rerun CreateInputIds)."
    fi
    
}

CleanInputIds(){

    if [ $# -ne 1 ]; then
        echo "Usage: CleanInputIds <password>"
        return
    fi
    
    PASS=$1
    shift

	echo "Cleaning input_id_analysis table"
	echo "delete from input_id_analysis" | mysqlw -p$PASS $PDBNAME

	echo "removing links from infiles directory ..."
	for file in ${ANALYSIS_WORK_DIR}/infiles/*; do
		if [ -L $file ]; then 
			rm -f $file
		fi
	done
}



TestRunnable(){
	echo ":: TestRunnable $*"

	write=
	logic_name=
	input_id=
	usage='usage: TestRunnable -l(ogic_name e.g. SWEmbl(case insensitive!) [ -i(input_id e.g. file_name.sam.gz) -w(rite flag) ] [ -h(elp) ]'

	#This makes sure we reset the getopts ind if we have used it previously
	OPTIND=1

	while getopts ":l:w:i:h" opt; do
		case $opt in 
			l  ) logic_name=$OPTARG ;;
			w  ) write=' -write ' ;;
		    i  ) input_id=$OPTARG ;;
			h  ) echo $usage; return 0 ;;
		    \? ) echo $usage; return 1 ;;
		esac 
	done


	#ValidateLogicName?

	if [ ! $input_id ]; then
		echo "SELECT input_id from input_id_analysis where input_id_type='Submit${logic_name}' limit 1"

		input_id=$(QueryVal PIPELINE "SELECT input_id from input_id_analysis where input_id_type='Submit${logic_name}' limit 1")

		if [ ! $input_id ]; then
			echo -e "Could not detect input_id for logic_name:\t$logic_name"
			exit
		fi
		
		echo -e "No input_id specified, using:\t$input_id"
	fi

	
	

	#Need to query for inpud_id 
	
	cmd="$ANALYSIS_SCRIPTS/test_RunnableDB $PDB_SCRIPT_ARGS -logic_name $logic_name -input_id $input_id $write"
	echo $cmd	

	#This currently does not work because the testRunnableDB script does not look in the Funcgen subdir!
	#Could specify the full module path? Or alter RunnableDB to specify optional RunnableDB dir defaulting to standard
	time $EFG_PERL $cmd	

	if [ $write ]; then
		echo -e "WARNING:\t-write flag was set. Re-running this job via the pipeline will create duplicate output"
		echo -e "\t\tUse MarkJobAsComplete before starting the pipeline to avoid this(needs writing in pipeline.env!!!)"
	fi

}



SubmitPeaks(){	
	echo ":: SubmitPeaks $*"

	OPTIND=1
	logic_names=
	usage='usage: SubmitAlign [ -l(ogic_names e.g. SWEmbl) ]+ [ -h(elp) ]
Simply submits the Peak jobs defined by CreateInputIDs';

	#We could add logic_name/analysis here.

	while getopts ":l:h" opt; do
		case $opt in 
            l  ) logic_names="$logic_names $OPTARG" ;;
			h  ) echo -e $usage; return 0;;
            \? ) echo -e $usage; return 1;;
		esac 
	done


	#Should put this as ValidateLogicName in pipeline.env
	analyses=
	lname=

	if [ $logic_names ]; then

		for logic_name in $logic_names; do
			lname=$(QueryVal PIPELINE "select input_id_type from input_id_analysis where input_id_type='Submit${logic_name}' limit 1")

			if [ ! $lname ]; then
				echo -e "Could not find input_ids to submit the analysis with logic_name:\t$logic_name"
				exit
			fi

			analyses="$analyses -analysis $logic_name"
		done
	fi

	echo "Should check for gzip jobs here"
	cmd="$EFG_PERL $PIPELINE_SCRIPTS/rulemanager.pl $PDB_SCRIPT_ARGS $analyses"
	echo $cmd
	Execute $cmd  2>&1 | tee -a $WORK_DIR/pipeline.out


	echo "Now do PeaksReport when finished"
}

################################################################################
# Func      : PeaksWait
# Desc      : Accumulator step to wait for all Peak jobs
# Arg[1..n] : None
# Return    : None 
# Exception : None
################################################################################


PeaksWait(){
	echo ":: AlignWait $*"
	#add opts here for -force_accumulators and -once?

	cmdline="$EFG_PERL $PIPELINE_SCRIPTS/rulemanager.pl $PDB_SCRIPT_ARGS -force_accumulators -analysis Align_Wait"

	#echo ":: ImportWait $cmdline"

	Execute $cmdline
	
	#echo "Can we cat the AFFY and AFFY_ST arrays_nr files?"
	#As these should use the same rules?  This would mean running with one logic name for both
	#Just keep separate for now

	#Now cat the array names files output by ImportArrays
	#This is to enable any ProbeFeature/IDXref/UnmappedObject deletes

	echo ": Finished waiting...ready for RunTranscriptXrefs?"
}


PeaksReport(){
	echo ":: PeaksReport $*"
			
	logic_names=

	usage='usage: PeaksReport [-l(ogic_names e.g. SWEmbl)]+ [-h help]'

	OPTIND=1

	while getopts ":l:h" opt; do
		case $opt in 
	        l  ) formats="$OPTARG $logic_names";;
		    \? ) echo $usage; exit 1;;
			h  ) echo $usage; return 0;;
		esac 
	done		


	#How are we going to do this, input_id will not always be the same for non-file input_id_types
	#Do we need to curate a file of feature_set names somehow?
	#Output from RunnableDB::Funcgen::check_Sets?



 	for format in $ARRAY_FORMATS; do
		SetArrayNames $format
		
		#This sets custom_names to $ARRAY_NAMES also!
		#names=${custom_names:=$ARRAY_NAMES}
		if [ $custom_names ]; then
			names=$custom_names
		else
			names=$ARRAY_NAMES
		fi

	
		#Validate custom names(sub this)?
		valid_names=
		
		for name in $names; do
			
			valid=$(echo $ARRAY_NAMES | grep -E "^$name | $name | $name$|^$name$")
			
			if [[ $valid ]]; then
				valid_names="'$name' $valid_names"
			fi
		done
		
		valid_names=$(echo $valid_names | sed 's/ /, /g')
				
		echo ""

		if [[ ! $valid_names ]]; then
			echo ": Skipping $format ProbeAlignReport, no valid custom names found: $names($ARRAY_NAMES)"
		else
			report_file=${WORK_DIR}/ProbeAlign.$format.report
			echo ": Generating $format($valid_names) ProbeAlignReport: $report_file"
			BackUpFile $report_file

	
			
			#Total Probes Mapped and Features
			#Is this correct?
			#Why are we getting >300 freqs here?
			#These should only be more than 100 if we get transcript mappings
			#Up to a maximum of 200
			query="SELECT 'Mapped Probes' as ''; SELECT rpad(a.name, 20, ' ') as 'Array Name           ', rpad(an.logic_name, 40, ' ') as 'Align Analysis                          ', rpad(count(distinct pf.probe_id), 19, ' ')  as 'Total Probes Mapped', rpad(count(pf.probe_feature_id), 20, ' ') as 'Total Probe Features' from analysis an , array a, array_chip ac, probe p, probe_feature pf where a.array_id=ac.array_id and ac.array_chip_id=p.array_chip_id and p.probe_id=pf.probe_id and pf.analysis_id=an.analysis_id and a.name in($valid_names) group by a.array_id, pf.analysis_id;"
			#WITH ROLLUP does not behave well here

			
			#Probe frequencies
			#This is still not quite right?
			#We are getting count > 100
			#Or is this a data duplication?
			

			#We can't rpad the counts here as this will make the sort lexical!
			query="${query} SELECT 'Probe Frequencies' as ''; select rpad(t.cnt, 13, ' ') as 'Feature Count', rpad(count(t.cnt), 11, ' ') as 'Probe Count' from (select count(distinct pf.probe_feature_id) as cnt from probe_feature pf, probe p, array_chip ac, array a WHERE ac.array_id=a.array_id AND ac.array_chip_id=p.array_chip_id AND pf.probe_id=p.probe_id AND a.name in ($valid_names) GROUP by pf.probe_id) as t GROUP by t.cnt order by t.cnt;"
	
		    #Top 20 most abundant probes
			query="${query} select ''; select 'Top $number Most Mapped Probes' as '';"

			#We can't rpad the counts here as this will make the sort lexical!
			#Would need to count in a sub select to enable the rpad
			#This is assuming all probes are deifferent within a probeset
			#For those which are NOT tsk tsk
			#We need to group
			
			query="${query} select count(distinct pf.probe_feature_id) as FeatureCount, rpad(pf.probe_id, 10, ' ') as 'ProbeID   ' from probe_feature pf, probe p, array_chip ac, array a WHERE ac.array_id=a.array_id AND ac.array_chip_id=p.array_chip_id AND pf.probe_id=p.probe_id AND a.name in ($valid_names) GROUP by pf.probe_id order by FeatureCount desc limit $number;"
			

			#Do we really need to group by array_id here?
			#Surely pf.probe_id would be fine?
			#No! This does something weird get 99 for the wrong probe which actually have only 33
			#Now we are getting duplicate rows with correct counts?
			#Is this due to the array group?


			#But are we missing some higher counts or is previous query incorrect?
			#2983483
			

	
				#Total unmapped probes
			query="${query} select ''; select rpad(count(distinct uo.ensembl_id), 21,' ') as 'Total Unmapped Probes', ur.summary_description from  probe p, array_chip ac, array a, unmapped_object uo, unmapped_reason ur WHERE ac.array_id=a.array_id AND ac.array_chip_id=p.array_chip_id AND uo.ensembl_id=p.probe_id AND a.name in ($valid_names) and uo.unmapped_reason_id=ur.unmapped_reason_id and (ur.summary_description='Unmapped probe' or ur.summary_description='Promiscuous probe') GROUP by ur.summary_description;"


			#Top 20 most unmapped probe
			query="${query} select ''; select 'Top $number Most Unmapped Probes' as '';"
			#Should group here but this seems to hide some records
			#This sorts lexically! on full description, no mapping count, would need order by replace(ur.full_description, '/%', '')?
			query="${query} select distinct rpad(p.probe_id, 10, ' ') as 'Probe ID  ', ur.full_description as 'Reason' from probe p, array_chip ac, array a, unmapped_object uo, unmapped_reason ur WHERE ac.array_id=a.array_id AND ac.array_chip_id=p.array_chip_id AND uo.ensembl_object_type='Probe' AND uo.ensembl_id=p.probe_id AND a.name in ($valid_names) and uo.unmapped_reason_id=ur.unmapped_reason_id and ur.summary_description='Promiscuous probe' order by ur.full_description desc limit $number;"


	        #Mapped Probes by seq_region name and array
			#This is too big, need to split into array/seq_region queries
			#Needs to be done in perl script
			#query="${query} select ''; select 'Mapped Probes by array.name seq_region.name' as '';"
			#query="${query} select a.name, count(distinct pf.probe_feature_id), sr.name from probe_feature pf, probe p, seq_region sr, array_chip ac, array a where a.array_id=ac.array_id and ac.array_chip_id=p.array_chip_id and p.probe_id=pf.probe_id and pf.seq_region_id=sr.seq_region_id and a.name in ($valid_names) group by a.name, sr.name';



			echo $query | mysql $DB_MYSQL_ARGS > $report_file
			cat $report_file

		
			

		fi
	done
}
	
