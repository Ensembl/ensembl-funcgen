#!/usr/local/bin/bash

#LICENSE
#
#  Copyright (c) 1999-2009 The European Bioinformatics Institute and
#  Genome Research Limited.  All rights reserved.
#
#  This software is distributed under a modified Apache license.
#  For license details, please see
#
#    http://www.ensembl.org/info/about/code_licence.html
#
#CONTACT
#
#  Please email comments or questions to the public Ensembl
#  developers list at <ensembl-dev@ebi.ac.uk>.
#
#  Questions may also be sent to the Ensembl help desk at
#  <helpdesk@ensembl.org>.

echo ":::: Setting up the eFG pipeline environment" 

#Source in some handy functions
#This will be changed so this sources .efg or rename efg.env
#And efg.env sources funcs.sh
#Also change so we don't need absolute path as we can source from the same directory
. $EFG_SRC/scripts/environments/funcs.sh





#env colour is red
export PS1_COLOUR=31


#This should really source from .efg first

PIPELINE_CONFIG=${PIPLINE_CONFIG:=$EFG_SRC/scripts/environments/pipeline.config}
echo ":: Sourcing PIPELINE_CONFIG: $PIPELINE_CONFIG"
. $PIPELINE_CONFIG

# DONE
# CleanJobs function

# TO DO
# Change AskQuestion to also take a list of valid answers and validate or ask again
# Extract vars to separate config file and source them in?
# Remove SCHEMA_BUILD as a mandatory param and parse from dbname


#This is now a base function env for all eFG pipeline activites
#Nothing analysis specific should be put in here!

#### Some convinient aliases
#These need to be single quoted to interpolate at run time
#These cannot be 'Executed' as they are not available in funcs.sh or arrays.env??
alias datahome='cd $DATA_HOME'
#this was dbhome, but clashed with oracle home
alias monitor='monitor $PDB_SCRIPT_ARGS -current -finished'
alias mysqlefg='mysql $DB_MYSQL_ARGS'
alias mysqlpipe='mysql $PDB_MYSQL_ARGS'
#This may not have been set!
#alias mysqlcore='mysql $DNADB_MYSQL_ARGS' #Now in efg.env
#This is not tested and only set in the caller??
#So should only be set in the caller? But is generic
alias workdir='cd $WORK_DIR' 
alias configdir='cd $SRC/ensembl-analysis/modules/Bio/EnsEMBL/Analysis/Config'
alias configmacs='xemacs $SRC/ensembl-analysis/modules/Bio/EnsEMBL/Analysis/Config $SRC/ensembl-pipeline/modules/Bio/EnsEMBL/Pipeline/Config &'

#alias scriptsdir='cd $SCRIPTSDIR'



# TO DO
# Check module versions in the DB match specified, auto update?

### Functions ###


################################################################################
# Func      : _InitPipelineEnv(
# Desc      : Sets up all config defined variables. Should only be called from
#             _InitEnv in child environment file e.g. arrays.env or analysis.env
# Args [n]  : 
# Return    : none
# Exception : 
################################################################################

_InitPipelineEnv(){

	CheckVariables DB_PORT DB_NAME DB_HOST SPECIES DB_PASS SPECIES ANALYSIS_SCRIPTS PIPELINE_SCRIPTS EFG_SCRIPTS DATA_HOME
	CheckDirs $DATA_HOME $ANALYSIS_SCRIPTS $PIPELINE_SCRIPTS $EFG_SCRIPTS

	#SCHEMA_BUILD now optional
	#Can grab from DBNAME
	#Maybe useful if using non-standard efg DBNAME
	
	if [ ! $SCHEMA_BUILD ]; then
		schema_build=$(GetSchemaBuild $DB_NAME)

		if [ $? -ne 0 ]; then
			echo -e $schema_build
			echo  'Failed to set $SCHEMA_BUILD. Please define this in your config file or change your $DB_NAME'
			exit 1
		fi

		SCHEMA_BUILD=$(echo $schema_build | sed 's/ /_/')

	fi
		

	VERSION=$(echo $SCHEMA_BUILD | sed 's/_.*//')
	BUILD=$(echo $SCHEMA_BUILD | sed 's/.*_//')


	export QUEUE_MANAGER=${QUEUE_MANAGER:=LSF}
	ValidateVariable QUEUE_MANAGER VALID_QUEUE_MANAGERS
 

	#This should take a different colour for every env?
	#blue is base efg
	#green for arrays
	#red for peaks/pipeline
	export PS1="\
\[\033[${PS1_COLOUR}m\]\
${ENV_NAME}:${DB_NAME}>\
\[\033[0m\]"

	if [ ! $PIPELINE_ENV  ]; then
		#Things here only ever need doing once, even if we source more than once instance
		#This avoid creating huge PATH and PERL5LIB vars when changing between species
		export PATH=${ANALYSIS_SCRIPTS}:${PIPELINE_SCRIPTS}:$PATH
		export PERL5LIB=$SRC/ensembl-functgenomics/modules:$SRC/bioperl-live:$SRC/ensembl/modules:$SRC/ensembl-pipeline/modules:$SRC/ensembl-analysis/modules:$SRC/bioperl-run:$SRC/ensembl-pipeline/modules:$PIPELINE_SCRIPTS:$SRC/Bio-Das-ProServer/lib
		#pipeline scripts dir in PERL5LIB as this is where some of the modules are!

		#This is instance specific and needs to go in analysis.env?
		# add to PERL5LIB for DAS server set up
        #${SRC}/Bio-Das-ProServer/lib:\
	
		export PIPELINE_ENV=1

		#chmod analysis/pipeline script and check for presence
		#Only ever need to do this once also
		
		for f in $ANALYSIS_SCRIPTS/test_RunnableDB  ${PIPELINE_SCRIPTS}/analysis_setup.pl ${PIPELINE_SCRIPTS}/rule_setup.pl ${PIPELINE_SCRIPTS}/setup_batchqueue_outputdir.pl ${PIPELINE_SCRIPTS}/rulemanager.pl; do

			if [ ! -f $f ]; then
				echo "Could not find file:\t$f"
				echo "Maybe you need to check out the relevant CVS package?"
				exit
			elif [ ! -x $f ]; then
				chmod +x $f
			fi
		done
	fi



	#Need to split schema_build here

	#Set vars/defaults
	#Pipeline DB
	export PDB_USER=${PDB_USER:=$DB_USER}
	export PDB_PASS=${PDB_PASS:=$DB_PASS}
	export PDB_HOST=${PDB_HOST:=$DB_HOST}
	export PDB_NAME=${PDB_NAME:="${ENV_NAME}_pipeline_${DB_NAME}"}
	export PDB_PORT=${PDB_PORT:=$DB_PORT}

    #DNADB/Core DB
	export DNADB_USER=${DNADB_USER:=$DB_USER}
	export DNADB_HOST=${DNADB_HOST:=$DB_HOST}
	export DNADB_PORT=${DNADB_PORT:=$DB_PORT}

	export DB_MYSQL_ARGS="-h${DB_HOST} -P${DB_PORT} -u${DB_USER} -p${DB_PASS} $DB_NAME"

	#DNADB_MYSQL_ARGS
	#When is it necessary to use these?


	if [ ! -z  "$DNADB_NAME" ];	then
		pass_arg=

		#We aslo need to set this to DBPASS if DNADB_USER is not set?

		if [ ! -z  "$DNADB_PASS" ]
		then
			args_pass="-dnadb_pass $DNADB_PASS"
			#dnadb_args_pass="-dnadb_pass $DNADB_PASS"
			mysqlargs_pass="-p $DNADB_PASS"
		fi
		
		export DNADB_SCRIPT_ARGS="-dnadb_host $DNADB_HOST -dnadb_user $DNADB_USER $args_pass -dnadb_name $DNADB_NAME -dnadb_port $DNADB_PORT"
		export DNADB_MYSQL_ARGS="-h${DNADB_HOST} -P${DNADB_PORT} -u${DNADB_USER} $mysqlargs_pass $DNADB_NAME"
	fi

	export PDB_MYSQL_ARGS="-h${PDB_HOST} -P${PDB_PORT} -u${PDB_USER} -p${PDB_PASS} $PDB_NAME"
	#These need changing to pdb_host?
	export PDB_SCRIPT_ARGS="-dbhost $PDB_HOST -dbuser $PDB_USER -dbpass $PDB_PASS -dbname $PDB_NAME -dbport $PDB_PORT"

	#These are script args, not mysql args
	#export READ_DB_ARGS="-dbhost $DB_HOST -dbuser $DB_ROUSER -dbname $DB_NAME -dbport $DB_PORT"
	#We don't use this DB_ARGS anymore as we use PDB_ARGS instead.
	export DB_SCRIPT_ARGS="-dbhost $DB_HOST -dbuser $DB_USER -dbpass $DB_PASS -dbname $DB_NAME -dbport $DB_PORT"
	
	SetMYSQL_ARGS OUT

	echo ""

	if [ ! $DB_HOST_LSFNAME ]; then
		DB_HOST_LSFNAME=$DB_HOST
		
		if [[ $DB_HOST_LSFNAME != 'localhost' ]] && [[ $DB_HOST_LSFNAME_host != '127.0.0.1' ]]; then
			DB_HOST_LSFNAME=$(echo $DB_HOST_LSFNAME | sed 's/-/_/')
			export DB_HOST_LSFNAME="my${DB_HOST_LSFNAME}"
			echo -e "\$DB_HOST_LSFNAME not set, defaulting to:\t$DB_HOST_LSFNAME"
		fi
	fi

	if [ ! $DNADB_HOST_LSFNAME ]; then
		DNADB_HOST_LSFNAME=$DB_HOST
		
		if [[ $DNADB_HOST_LSFNAME != 'localhost' ]] && [[ $DNADB_HOST_LSFNAME_host != '127.0.0.1' ]]; then
			DNADB_HOST_LSFNAME=$(echo $DNADB_HOST_LSFNAME | sed 's/-/_/')
			export DNADB_HOST_LSFNAME="my${DNADB_HOST_LSFNAME}"
			echo -e "\$DNADB_HOST_LSFNAME not set, defaulting to:\t$DNADB_HOST_LSFNAME"
		fi
	fi



	#echo 'NOTE: Also test for DB by connecting using both sets of MySQLARGS? Are we using this anymore?'

	echo "DB:               ${DB_USER}@${DB_HOST}:${DB_NAME}:${DB_PORT}
DNADB:            ${DNADB_USER}@${DNADB_HOST}:${DNADB_NAME}:${DNADB_PORT}
PIPELINEDB:       ${PDB_USER}@${PDB_HOST}:${PDB_NAME}:${PDB_PORT}
VERSION:          $VERSION
BUILD:            $BUILD
"
}




################################################################################
# Func      : SetMYSQL_ARGS(
# Desc      : Sets the MYSQL_ARGS variable dependent on the DB type argument passed
#             This should really only be used for QueryVal now, with explicit params
#             being used for other calls
# Args [n]  : Mandatory - DB type e.g. OUT, TARGET
# Return    : none
# Exception : Exits if DB type not recognised
################################################################################


SetMYSQL_ARGS(){
	DB_TYPE=$1
	CheckVariables DB_TYPE
  


	#Move string definition to Init?
	#Make dynamic? This would prevent updating of vars without resourcing env.
	#Clear this up, do we really want TARGET and OUT now we are using eFG
	
	
	if [[ $DB_TYPE = OUT ]]
	then
		export MYSQL_ARGS=$DB_MYSQL_ARGS

	elif [[ $DB_TYPE = DNADB ]]
	then
		export MYSQL_ARGS=$DNADB_MYSQL_ARGS

	elif [[ $DB_TYPE = PIPELINE ]]
	then
		export MYSQL_ARGS=$PDB_MYSQL_ARGS

	else
		echo "Cannot set MYSQL_ARGS for DB = $1"
		exit 1
		#This does not exit when using $(QueryVal)
		#I guess $() must be in a subshell
	fi

}



#This actually drops the whole DB
#So need to be careful if we set the PDB to the out DB
#Need to change this to actually just drop the pipeline tables

DropPipelineDB(){
	echo ":: DropPipelineDB $*"

	#This makes sure we reset the getopts ind if we have used it previously
	OPTIND=1

	force=
	usage='usage: DropPipelineDB [ -f(orce) -h(elp) ]'


	#Can we take array_names here too?
	#Is this wise to restrict to arrays within a linked set?


	while getopts ":fh" opt; do
		case $opt in 
	        f  ) force=1 ;; 
            h  ) echo $usage; return 0;;
			\? ) echo $usage; exit 1;;
		esac 
	done


	exists=$(echo "show databases like '$PDB_NAME'" | mysql -h$PDB_HOST -u$PDB_USER -P$PDB_PORT -p$PDB_PASS)

	if [[ ! $exists ]]; then
		echo "Pipeline DB $PDB_NAME does not exist"
		return 0
	fi


	if [[ $force -ne 1 ]]; then

		AskQuestion "Drop all the pipeline tables from $DBNAME[y|n]"

		if [[ $REPLY != [yY]* ]]; then
			echo "Aborting"
			return
		fi
	fi
		
	BackUpTables -t pipeline


	#This will also drop the DB if it is different to the output DB
    if [[ "$PDB_MYSQL_ARGS" != "$DB_MYSQL_ARGS" ]]
	then
		echo ":: Dropping pipeline database: $PDB_MYSQL_ARGS"
		echo "drop database $PDB_NAME" | mysql -h$PDB_HOST -u$PDB_USER -P$PDB_PORT -p$PDB_PASS

	else

		query="drop table job; drop table job_status; drop table input_id_type_analysis; 
	    drop table input_id_analysis; drop table rule_goal; drop table rule_conditions;"

		echo $query | mysqlpipe
		RemoveLock	
	fi
}


BackUpTables(){
	echo ":: BackUpTables $*"

	usage='usage: BackUpTables -t(able group) arrays|xrefs|pipeline  [ -s(uffix default=txt) -h(elp) ]'

	table_group=
	suffix=txt

	#This makes sure we reset the getopts ind if we have used it previously
	OPTIND=1
	while getopts ":t:s:h" opt; do
		case $opt in 
			t  ) table_group=$OPTARG ;;
		    s  ) suffix=$OPTARG ;;
			h  ) echo $usage; return 0 ;;
		    \? ) echo $usage; return 1 ;;
		esac 
	done

	#OrUsage?
	CheckVariables table_group


	
	backup_dir=${BACKUP_DIR:=$WORK_DIR}
	MakeDirs $backup_dir

	mysql_args=


	if [[ $table_group = xrefs ]]; then
		mysql_args=$DB_MYSQL_ARGS
		tables='xref object_xref unmapped_object unmapped_reason'

	elif [[ $table_group = arrays ]]; then
		mysql_args=$DB_MYSQL_ARGS
		tables='array array_chip probe probe_set probe_feature'

	elif [[ $table_group = pipeline ]]; then
		mysql_args=$PDB_MYSQL_ARGS
		tables='input_id_analysis input_id_type_analysis job job_status rule_goal rule_conditions analysis'
	else
		echo "The BackUpTables type $table_group is not supported or not defined"
		echo "BackUpTables Failed"
		exit 1;
	fi


	#Use one date for all to allow easy deletes
	#Obviously not all tables will ahve bgeen dumped at this exact time
	#backup_date=$(date '+%T')
	#Just use $$ as we can get the date from ls

	for table in $tables; do
		mysqldump $mysql_args $table > ${BACKUP_DIR}/${table}.${suffix}.$$
	done
}


RemoveLock(){

    echo ":: Removing pipeline lock from: $PDB_MYSQL_ARGS"
    echo "delete from meta where meta_key = 'pipeline.lock';" | mysqlpipe

}



CreatePipelineTables(){
	echo ":: CreatePipelineTables $*"
#??? This was not creating pipeline DB but is creating eFG DB


	#We need to use ContinueOverride here to drop previous tables

	#Compare args to enable DBs of same name on different instances

	if [[ "$PDB_MYSQL_ARGS" != "$DB_MYSQL_ARGS" ]]; then
		#Need to create PDB is it doesn't exist
		#This will throw an error as we haven't created it yet
		

		exists=$(echo "show databases like '$PDB_NAME'" | mysql -h$PDB_HOST -u$PDB_USER -P$PDB_PORT -p$PDB_PASS)

		if [[ $exists ]]; then
			echo -e "DB $PDB_NAME already exists. Maybe you want to DropPipelineDB?"
			exit 1;
		fi


		echo ":: Creating pipeline database: $PDB_MYSQL_ARGS"
		echo "create database $PDB_NAME" | mysql -h$PDB_HOST -u$PDB_USER -P$PDB_PORT -p$PDB_PASS

		#This just contains meta table which is missed in table.sql(and others?)
		mysqlpipe < $SRC/ensembl-functgenomics/sql/efg.sql
		echo "INSERT INTO meta (meta_key, species_id, meta_value) VALUES ('schema_version', NULL, '$VERSION')" | mysqlpipe		

	fi



	tables=$(QueryVal PIPELINE "show tables like 'input_id_type_analysis'")

	if [[ $tables ]]; then
		echo -e "DB $PDB_NAME already contains pipeline line tables"
		exit 1;
		#We may not want to DropPipelineDB tho as this may also be the output DB
		#Need to write DropPipelineTables 
		#And change test above to just warn if $PDB_NAME already exists
	fi


	CheckFile $SRC/ensembl-pipeline/sql/table.sql
	mysqlpipe < $SRC/ensembl-pipeline/sql/table.sql

	#Now tweak to allow longer logic_names
	#rule_condition should really be an anlaysis_id, not a string
	#now matches logic_name definition
	mysqlpipe -e 'Alter table rule_conditions modify `rule_condition` varchar(100) DEFAULT NULL'


	if [ $? -ne 0 ]
	then
		echo "WARNING: Failed to import pipeline tables to $PDB_NAME"
	fi
	
}

#AddSchemaVersion(){

	#take MYSQLRGS here?
	#Will only ever call this once?
	#Do in line

  
#    echo "Adding schema version $VERSION to database $EFG_DBNAME"
#    echo "INSERT INTO meta (meta_key, meta_value) VALUES (\"schema_version\", \"$VERSION\");" \
#        | mysqlw -p$PASS $EFG_DBNAME

#}


################################################################################
# Func      : CleanJobs
# Desc      : Deletes all input_id_analysis entries for a given RunnableDB logic_name
# Return    : none 
# Exception : none
################################################################################

#Extend this to also remove the job and job_status
#entries for given input_ids
#Simply having input_ids in the table will always return Finished for Submit type jobs?

CleanJobs(){
	echo ": CleanJobs $*"

	logic_names=

	usage='usage: CleanJobs [ -l(ogic_name) e.g. AFFY_UTR_ProbeAlign]+ [ -h(elp) ]'

	#This makes sure we reset the getopts ind if we have used it previously
	OPTIND=1
	while getopts ":l:h" opt; do
		case $opt in 
			l  ) logic_names="$OPTARG $logic_names" ;;
			h  ) echo $usage; return 0 ;;
		    \? ) echo $usage; return 1 ;;
		esac 
	done



	#Should we also tidy up job/job_status here too?
	
	#echo "We need to BackUpTables the input_id_analysis table here?";
	#Not so important now as we're only deleting IDs for a given analysis
	#echo "We need to $* ResetJobs here too or merge methods?"


	if [ ! -z "$logic_names" ]; then

		for lname in $logic_names; do
			#Do the jobs first
			cmd="select analysis_id from analysis where logic_name='$lname'"
			#echo "cmd is $cmd"
			anal_id=$(QueryVal PIPELINE $cmd)

			#Need to capture no return value here
			#maybe in QueryVal?

			if [ $anal_id ]; then
				cmd="delete j, js from job j, job_status js where j.analysis_id=$anal_id and j.job_id=js.job_id"
			#echo "cms id $cmd"
				Execute echo $cmd | mysql $PDB_MYSQL_ARGS

		    #Then do the input ids
				cmd="select analysis_id from analysis where logic_name='Submit_${lname}'"
			#echo "cmd is $cmd"
				anal_id=$(QueryVal PIPELINE "select analysis_id from analysis where logic_name='Submit_${lname}'")
				cmd="delete i from input_id_analysis i where i.analysis_id=$anal_id"
				Execute echo $cmd | mysql $PDB_MYSQL_ARGS
			else
				echo "No analysis found for $lname"
			fi
  
		done
	else
		ContinueOverride "Do you want to delete input IDs for all analyses"
		#cmd="echo 'delete from input_id_analysis' | mysql $PDB_MYSQL_ARGS"
		Execute echo 'delete from input_id_analysis' | mysql $PDB_MYSQL_ARGS
		Execute echo 'delete from job' | mysql $PDB_MYSQL_ARGS
		Execute echo 'delete from job_status' | mysql $PDB_MYSQL_ARGS		
	fi

	#Should this also remove err and out files from $PIPELINE_OUT?	

	#echo "removing links from infiles directory ..."
	#for file in ${ANALYSIS_WORK_DIR}/infiles/*; do
	#	if [ -L $file ]; then 
	#		rm -f $file
	#	fi
	#done

}

################################################################################
# Func      : CleanPipelineSanity
# Desc      : Runs script to check pipeline set up is sane
# Return    : none 
# Exception : none
################################################################################


CheckPipelineSanity(){
    echo ":: Check pipeline sanity $*"

	#This returns 0 even if it 'fails'

    Execute perl $PIPELINE_SCRIPTS/pipeline_sanity.pl $PDB_SCRIPT_ARGS

}                        

################################################################################
# Func      : ValidateVariableFromDB
# Arg[0]    : variable e.g. H3K4me3
# Arg[1]    : query e.g. "select name from feature_type where name='H3K4me3'"
# Desc      : Checks variable is present in DB via specified query
# Return    : Boolean
# Exception : Exits if variable not found
################################################################################

#ValidateVariableFromDB





#This is too specific for pipeline.env
#Move to efg_analysis.env

test_eFG_Runnable () {


	#Do we need module and logic_name?
	#Use case to parse opts?

    if [ $# -lt 3 ]; then
        echo "Usage: test_eFG_Runnable <password> <module> <logic_name> <input_id>"
        return
    fi


	
    MODULE=$1; shift
    LOGIC_NAME=$1; shift
    INPUT_ID=$1; shift
	#Can pass other args e.g. -write


    echo "MODULE: $MODULE"
    echo "LOGIC_NAME: $LOGIC_NAME"
    echo "INPUT_ID: ${INPUT_ID}"

    time \
    $PIPELINE_SCRIPTS/test_RunnableDB $PDB_SCRIPT_ARGS \
        -runnabledb_path Bio/EnsEMBL/Analysis/RunnableDB/Funcgen \
        -module $MODULE \
        -logic_name $LOGIC_NAME \
        -input_id ${INPUT_ID} \
        $*
    
}

#Was ResetAnalysis
#Do we also need to remove Succesful current status here?
#Can use return here! We should only use return for functions that we are not autmating
#i.e. those ones we use on the command line but not when running the pipeline


#This is not that straight forward as we may have input_id_analysis records written for completed jobs
#Do we have to delete the input_id_analysis records for it to run again?
#Yes as job and job_status records are removed after success

#This should not take the Submit logic name

#This is for all jobs after cleaning all output

RemoveJobs() {

	OPTIND=1
	logic_name=

	usage="RemoveJobs\n
Description:\tRemoves job, job status and input IDs for a given analysis logic_name\n
Usage:\t\tRemoveJobs -l(ogic_name) [ -h(elp) ]"

	#Should we include rules here to?

	while getopts ":l:h" opt; do
		case $opt in 
            l  ) logic_name=$OPTARG ;;
			h  ) echo -e $usage; return 0;;
            \? ) echo -e $usage; return 1;;
        esac 
    done


	CheckVariablesOrUsage $usage logic_name

    #if [ $# -ne 1 ]; then
	#		echo "Usage: ResetJobs logic_name e.g. AFFY_ProbeTranscriptAlign"
	#		return
    #fi

	#echo "This will remove all job, job_status and input_id_analysis records for $logic_name"
	#ContinureOverride here?
	echo "Have you cleaned the output for $logic_name?"


	#exit here if logic_name begins with Submit?
	
	anal_id=$(QueryVal PIPELINE "select analysis_id from analysis where logic_name='$logic_name'")

	
	#We need to get the analysis_id first to validate that the analysis exists

	if [ ! $anal_id ]; then
		echo "$logic_name is not a valid analysis logic_name"
		#return 1?
	else

		echo "Deleting job and job_status records for $logic_name"
		sql="DELETE js, j from job_status js, job j where j.job_id=js.job_id and j.analysis_id=$anal_id";
		echo $sql | mysqlpipe;

		echo "Deleting input_id_analysis records for $logic_name"
		sql="DELETE from input_id_analysis where analysis_id=$anal_id";
		echo $sql | mysqlpipe;
	fi

}

Monitor(){
	#This uses alias at top of this script
	#Here for compliance with efg_analysis env???
#	remove this?
    monitor $*

}

GetFailedJobs(){
	query="select js.status, j.job_id, j.retry_count, j.stderr_file from job j, job_status js where j.job_id = js.job_id and js.is_current = 'y' and (js.status ='FAILED' or js.status='AWOL' or js.status='FAIL_NO_RETRY');"
	echo $query | mysqlpipe
	
}


#This should be called from RollbackArrays

ResetJobs(){
	echo ":: ResetJobs $*"
	logic_name=
	awol_jobs=
	all_jobs=
	failed_jobs=
	successful_jobs=
	states=
    
	#?? There are none for success?
	#Yes there is 'SUCCESSFUL', but this eventually gets removed
	#along with all previous states
	#How do we reset these?

#CREATED    | 
#READING    | 
#RUNNING    | 
#SUBMITTED  | 
#SUCCESSFUL | 
#WAITING    | 
#WRITING    | 

	#Job and job_status entries are deleted by the pipeline
	#input_id_analysis entries have runhost and created data

	#Should warn to Rollback results if we are doing successful jobs

	#Completed/successful jobs are eventually specified by the presence of two 
	#input ids, one for the submit and one for the jobs itself
	#When no states exist for the actual job, then this is complete

	#Submit input_ids always show as complete via monitor?


	#successful_states="'
	usage="ResetJobs\n
Description:\tResets job status and retry count a given analysis logic_name\n
Usage:\t\tResetJobs -l(ogic_name e.g. AFFY_UTR_ProbeAlign)  -a(ll jobs) -f(ailed_jobs) -i(nclude awol jobs) [ -h(elp) ]"

	#We need to add format and align_types here to make this useable!
	#Would require wrapper script in arrays.env
	#This is why it is easier to reset all by dropping the pipeline DB and CreateAlignIDs from fresh
	

	#This makes sure we reset the getopts ind if we have used it previously
	OPTIND=1
	while getopts ":l:iafsh" opt; do
		case $opt in 
			l  ) logic_name=$OPTARG ;;
            a  ) all_jobs=1 ;;
            i  ) awol_jobs=1 ;;
            f  ) failed_jobs=1 ;;
            s  ) successful_jobs=1 ;;
			h  ) echo -e $usage; return 0 ;;
		    \? ) echo -e $usage; return 1 ;;
		esac 
	done

	CheckVariables logic_name
	#Should check this is in the DB?

	if [ ! $failed_jobs ] && [ ! $awol_jobs ] && [ ! $successful_jobs ] && [ ! $all_jobs ]; then
		echo "You must select at least one jobs status to reset"
		echo $usage
		return 1
	fi


	if [ $all_jobs ]; then
		successful_jobs=1
		failed_jobs=1
		#awol_jobs=1
		states="'FAILED', 'FAIL_NO_RETRY', 'AWOL'"
	fi

	if [ $failed_jobs ]; then
		states="'FAILED', 'FAIL_NO_RETRY'"
	fi

	if [ $awol_jobs ]; then
		
		if [ "$states" ]; then
			states="${states},"
		fi

		states="$states 'AWOL'"
	fi

	if [ $successful_jobs ] || [ $all_jobs ]; then
		echo "-s(uccessful jobs) and -a(ll jobs) modes not yet implemented"
		return 1;
	fi


	#Now add commas to states
	#for state in  


	AskQuestion "Have you stopped the pipeline first? [y|n]:"

	if [ "$REPLY" = y ]; then
		#Could do with adding some counts here

		query="update job j, job_status js, analysis a set j.retry_count=0 where j.job_id=js.job_id and js.status in($states) and js.is_current='y' and a.logic_name='$logic_name' and a.analysis_id=j.analysis_id";
		echo $query | mysqlpipe
	
	    #Now need to set FAIL_NO_RETRY jobs is_current status back to FAIL
		if [ $failed_jobs ]; then
			query="SELECT js.job_id from job_status js, job j, analysis a where js.status='FAIL_NO_RETRY' and js.is_current='y' and js.job_id=j.job_id and j.analysis_id=a.analysis_id and a.logic_name='$logic_name'"
			job_ids=$(QueryVal PIPELINE $query)

	
			if [ "$job_ids" ]; then
				job_ids=$(echo $job_ids | sed 's/ /,/g')
	  			#FAIL jobs may have been removed or never existed in the first place?
				query="update job_status js set js.is_current='y' where js.status='FAIL' and js.job_id in($job_ids)"
				echo $query | mysqlpipe
			
	            #Now delete FAIL_NO_RETRY jobs
				query="delete js from job_status js where js.status='FAIL_NO_RETRY' and js.is_current='y'";
				echo $query | mysqlpipe
			fi
		fi

		if [ $successful_jobs ]; then

			#Now just need to reset created and runhost in input_id_analysis table?
			
			#This actually creates warnings, but sets to default
			#update input_id_analysis set created=NULL, runhost=NULL;
			#Now need to restrict to those which are not present in job? Or can we just do with all?
			#These are still showing as successful!
			#Is this because both the submit and actual job exist and have no status?


			echo "Need to remove input_id_analysis records here"

		fi

	else
		echo "You must stop the pipeline first before resetting FAILED/FAIL_NO_RETRY jobs"
	fi

}


GetRunningJobs(){
	query="select job.job_id, stderr_file, exec_host from job, job_status where job.job_id = job_status.job_id and is_current = 'y' and job_status.status like \"%RUN%\";"
	echo $query | mysqlpipe
	
}




QueryVal(){
	dbtype=$1
	shift
	query=$*

	#Always have to escape quotes in query otherwise we lose them!?

	#This is not exiting if called from another env!!
	CheckVariables dbtype query

	SetMYSQL_ARGS $dbtype


	#Need to Execute here and make it echo the result, will this get caught by the caller of QueryVal?
	val=$(echo $query | mysql $MYSQL_ARGS)


	#should capture error here
	#this works differently if passing a var or passing a quoted string, var get's split
	#do not quote query!
	
	#will this return more than one row or do we have to write QueryVals?
	#Clip the field name returned
	#this is maximal matching so will always return the last word!
	echo $val | sed "s/[^ ]* //"
}





ConvertTableEngine(){
	echo ": ConvertTableEngine $*"

	echo "Need to make backup and do post conversion checks here before dropping original table"
	return 1

	#If doing this to Copy InnoDB over a server, it may be better to dump with primary key sort
	#and import back as MyISAM in the new server

	
	table=
	engine=
	usage="usage:\tConvertTableEngine  -t table_name(e.g. probe_feature) -e engine_name(e.g. MyISAM|InnoDB) [ -h(elp) ]"
	
	OPTIND=1
	
	while getopts ":t:e:h" opt; do
		case $opt in 
	        t  ) table=$OPTARG ;;
	        e  ) engine=$OPTARG ;;  
		    \? ) echo $usage; return 1;;
			h  ) echo $usage; return 0;;
		esac 
	done		
	
	error=$(CheckVariablesOrUsage "$usage" table engine)


	if [ $? != 0 ]; then
		echo $error
		return 1
	fi


	if [[ $engine != InnoDB ]] && [[ $engine != MyISAM ]]; then
		echo -e $usage
		echo -e "You supplied an invalid engine type:\t$engine"
		return 1
	fi

	
	#check table exists
	#QueryVal doesn't capture STDERR output
	table_exists=$(QueryVal OUT "show tables like '$table'")
	

	if [[ $table_exists != "($table) $table" ]]; then
		echo -e $table_exists
		return 1
	fi


	create_statement=$(QueryVal OUT "show create table $table")
	#Need to check current engine to avoid doing unnecessary insert
	same_engine=$(echo $create_statement | grep -i "engine=$engine")

	if [[ $same_engine ]]; then
		echo -e "Table $table is already a $engine table"
		return 0
	fi

	
	echo "ALTER table $table ENGINE=$engine" | mysqlefg
	#This won't do anything if InnoDB is not available as an engine
	#show engines;
	
	if [ $? -ne 0 ]; then
		return 1
	fi
	
	#Validate here and return 3 if not converted?	
}

