#!/usr/local/bin/bash

echo ':::: Welcome to the eFG array mapping environment'

#export trap_exit=1
#Also change so we don't need absolute path as we can source from the same directory 
. $EFG_SRC/scripts/environments/pipeline.env




### DONE
# DONE Error catch properly > Execute
# DONE implement 'Execute' for critical commands, see func.sh and affy.env Execute
# DONE Add full setup for each rule in config. Each array in ImportArrays, Each format in Probe(Transcript)Align
# DONE DumpSoftMaskedSeq should handle striping on GENOMICSEQS dir.
# DONE Collapses each array format separately merging to one NR_FASTA. Different ARRAY_DESIGN should be run separately.
# DONE enable multiple arrays.list files for each array format
# DONE implement ARRAY_ENV to reduce the amount of vars set e.g. do not extend PERL5LIB ?
# DONE Enable parallelised ImportArrays using dynamic bsub config
# DONE Base ImportArrays bsub -R config on size of files in each format group
# DONE Move All set up to init sub, clean all vars when sourcing, then call init after species_VERSION.arrays has finished set up.  This will stop vars being inherited from previous envs.
# DONE Change ARRAY_DESIGN check to use VALID_DESIGNS array, can we grep this?
# DONE Ask if we want to dump GENOMICSEQS/TRANSCRIPTSEQS if not set. Also test files
# DONE Change BackUpFile to use back up dir if set.
# DONE Change OUTPUT_DIR to WORK_DIR and genera
# DONE Source .pipeline
# DONE Use awk to validate FASTA
# DONE Sort BuildFasta so we don't mis any sub string arrays when greping?
# DONE No way of doing this ? As list always contains full file name which could zip name or fasta name if it wasn't zipped
# DONE To get around this, we need to unzip all fastas and add unzipped name to list, then match on exact name
# DONE This depends on unzipped file being the same as the zipped file s/zip|gz//
# DONE RollbackArrays now handles all previous 'Clean'/rollback functionality via rollback_array.pl/Helper.pm
# DONE Remove DEFAULT_ARRAY_FORMATS, now uses ARRAYS_HOME or ARRAY_FORMATS in dbname.arrays
# DONE MakeHugememWorkdir

### TO DO LIST ###
# Add exit code handling, 101 for miscleaneous.
# Update ArrayAlignReport to use oligo_array entries
# add ARRAY_FORMAT to arrays.list file and BuildFasta method, so custom list works with multiple formats
# distribute files across work and scratch, soft link from work1
# Set status IMPORTED status for arrays
# Catch ImportArrays $? error and also archive old output files, so whe can test for the rpesenc of new output files when cat'ing
# Add ISKIPLIST/REGEX/FIELD to config to enable skipping of certain non-experimental probes
# Move all interactive stuff to Init to enable full automation of RunAlignments
# Set vars for all interactive questions to allow full non-interactive automation, need to Execute here to error catch
# Make _InitEnv take opts passed from instance.env?
# Make sure .efg stuff does not overlap, make .efg source from funcs.sh mysql aliases fail due to MYSQL_ARGS
# Move all vars to separate file and just have defaults in here.
# Make batch size dynamic
# Tweak Input ID numbers for ProbeTranscriptAlign currently running 10-15 minutes
# Autogenerate BatchQueue config and insert using sed?
# Or generate Config::Arrays.pm which we can write to locally hence avoiding editing ensembl code and can be used in BatchQueue and other Analysis/Config modules
# e.g. Just add this to a Config hash %{$Config::Array::batch_queue}
# Will this not create problems when running other analyses? Can we use a generic var for this and set the path dynamically depending on the analysis env. e.g. EFG_CONFIG_MODULE='Config::Arrays'
# Then in BatchQueue: use $ENV{'EFG_CONFIG_MODULE'};
# %{$EFG_CONFIG_MODULE::batch_queue}
#
# Create CreateAlignIDs RunnableDB? And runnable DBs for all array wide clean steps?
# Remove species common and put all arrays in subdir (ARRAYS) and use latin name?
# Check if we are handling haplotype regions in dump, did I fix the memory problem?
# Carry out appropriate steps given ARRAY_FORMAT? Or dump this and just use ALIGN_TYPES?
# Use DNADB_NAME schema_build to guess transcript dump file 
# Use the db fasta cache?
# Test DB connections in Init
# Grep for mysql reserved characters in DBNAME in _Init
# Use default seq file based one validated build?
# Need to reset jobs align jobs when we CleanArrays?
# Remove SPECIES_COMMON?
# Validate number or probes in db vs. fasta?
# Roll back by chunk(This would require using/recreating exonerates chunk function). 
# Currently no way of rolling back output for individual chunk
# So when we re-run we may have duplicate Unmapped objects and features depending on where the failure happened
# This can result in anomalous Transcript annotations due to duplicate features which are not caught by probe2transcript
# Create CleanUpProbeAligns: We are currently storing unmapped objects for *_ProbeTranscriptAlign which may be spurious
# due to valid alignments in *_ProbeAlign. And vice versa, we may have exceeded hits in ProbeAlign, but ProbeTranscriptAlign
# will throw these away, so will not be aware that it is a promiscuous probe. Will have to update Helper::rollback_ArrayChip. We could integrate the Genomic and Transcript alignment into the same job, running one after the other?
# This would enable us to use the same caches and therefore would remove clean up step.
# GenerateCDF
# Handle ARRAY_FORMATS not being set better
# Man pages for functions?
# Need to account for align_types in SetUpPipeline. Implement ALIGN_TYPES similar to ARRAY_FORMATS
# Make sure we have all the functionality friom these scripts $SRC/ensembl-personal/npj/oligo_mapping/scripts
# array_FORMAT.names can empty if ImprtArrays fell over, we should warn about this in Rollback. Can also become empty after ImportArrays somehow?
# GetOpts in funcs.sh which handles  -f 1 2 3 type args and also fails if any additional prefix args are found
# would also handle OPTIND
# Add LSF DB throttling in SubmitAlign?
# Remove SPECIES_COMMON and just uc SPECIES to get the arrays_dir
# Set up individual AlignWaits for each analysis, then we can kick off RunTranscriptXrefs straight away?
# This would require threading or integration of probe2transcript into a RunnableDB.
# Same here for CreateAlignIDs step and Import to Align transition
# Make probe_per_chunk and batch size dependant on size of TRANSCRIPT/GENOMICSEQS file and array_nr for each format
# This is also how we can set the -R separately for each of the CONFIG by setting \$${FORMAT}_CONFIG_VAR before submitting
# and having $"FORMAT"_CONFIG_VAR in config for each FORMAT 
# THIS IS WHY WE HAVE TO DO IT IN STEPS AS EVEN IF WE COULD INCLUDE SETTING THE ENV VARS IN IMPORTARRAYS
# THESE WOULD NOT BE PROPOGATED TO THE ENV OF THE PIPELINE(just the import job env)
# Same for probe2transcript submission? Altho these could be calculated in CreateAlignIDs step.


# ISSUE: we cannot re-do transcript mapping without reconstituting the nr fasta file with dbIDs
# Cannot assume we will always have the original file, so we need to alter ImportArrays to run with the original sources files, but simply use the DB to retrieve dbIDs, rather than storing the probes.  We could extend this to allow incremental updates of new arrays rather than having to clear the DB and re import everything.



#DOCS
#Need to define format and array specific config in Analysis/Config/ImportArrays
#ARRAY_PARAMS needs to match the content of the file, not the name of the file as this can differ.
#Need to set up format dirs in species home reflecting these array formats
# add doc to the top of this env about dir setup, and required code bases e.g. analysis, pipeline, core, efg
#Add docs to the VALID vars, about what you would need to do to add an an extra valid var, and a summary description of how they are used

### Initialise/Reset Variables ###
#which may be inherited from a previous environment
#All these need to be exported to persist for access by the funcs

export ALIGN_TYPES=
#This is just to restrict the type of arrays we map
#Default is set in _InitEnv, which is all.
#Are we going to have problems with probes exists across DESIGNS?
#i.e. remapping one design will affect another?
#e.g. AFFY_EXPRESSION, NIMBLEGEN_TILING, ILLUMINA_?
#Need to think more about exactly what we want to do with this var
#export ARRAY_DESIGNS=


#This is to restrict the particular arrays we map
#We will definitely encounter problems here,as we will be silently remapping probes from 
#other arrays, which may invalidate previous transcript mappings
#We will also have the problem of the array name not matching the file name
#Isn't this what arrays.list is for?

#Or we could just query the DB?
export ARRAYS=


## Vars which are exported from the methods
# These can be set in species_VERSION.arrays or passed to method
#Need to change all these to pick up defaults


export MULTI_SPECIES=
#do not change this one as this prevent inheritance from previous env
#this is required as we may want to test to see if we have set the ARRAY_FORMATS
#explicitly rather than just using the defaults
export ARRAY_FORMATS=
export ALIGN_TYPES=


### Clean only args!
# Never set here!
export GENOMICSEQS=
export TRANSCRIPTSEQS= 

### Define constants ###

export ENV_NAME='array_mapping'

#Formats must always be "VENDOR_FORMATTYPE"
#Or can simply be "VENDOR"
#This is used in RunTranscriptXrefs
export VALID_ARRAY_FORMATS='AFFY_UTR AFFY_ST ILLUMINA_WG CODELINK'
export VALID_SEQ_TYPES='GENOMICSEQS TRANSCRIPTSEQS'
export VALID_ALIGN_TYPES='GENOMIC TRANSCRIPT'

#hugemem actually has 192GB, but we've set it to 185 for safety
export MAX_HUGE_MEM=185000
#Set this lower than 15800 as will pend forever on normal if farm is busy
export MAX_NORMAL_MEM=15000


#These FACTORS are the multipliers by which the actual physicial size of the fasta file is multipled 
#to get the the memory usage figure for the import and xref steps
#9.3 comes to just under ~14.5GB for the the human exon and gene ST arrays
#Actual mem used is ~10.7GB for human STs so can drop this a little
#>9.3 tends to never get submitted
#Which is handy as this is below the max we can get on a normal node
#These are now used to multiply sum of the seq and array file sizes
#To get the amount of KB required
#Do not change these!
export IMPORT_MEM_FACTOR=7
export IMPORT_MEM_BASE=2500

#This comes to about 18GB for ST
#But only 280MB for ILLUMINA_WG
#ALIGN_MEM_FACTOR will be used in conjuction with CHUNKS
#Can we just use one mem factor to generate all?

#Actual figures HUMAN Import
#ST 11GB??? So why are we hugememming this?
#ILLUMINA WG 141MB
#UTR 4.2GB?
#Let's test this.



#Do not change!! XREF_MEM_FACTOR=11
export XREF_MEM_FACTOR=11
#export XREF_MEM_BASE=1500
export XREF_MEM_BASE=0
#export WEIGHT=1000000000


#XREF actual figures
#HUMAN ILLUMINA 1.4GB
#HUMAN ST 4GB
#HUMAN UTR 1.9GB

#Can't get this to work with exponent weighting, see SetMusage

#GENOMIC_CHUNKS was 3000
#Reduced to 2500 as we we getting the odd job >2GB mem
#Need to up the batch number accodingly and throttle?
#Either that or set SetMusage for particular format and set as env var in ProbeAlign config
export GENOMIC_CHUNKS=2500
#do we set a hard chunk number or just multiply by 10?
export TRANSCRIPT_CHUNKS=25000

export DATA_HOME=/lustre/work1/ensembl/nj1/array_mapping
export HUGEMEM_HOME=/nfs/acari/nj1/huge_array_mapping

#This is for location of compiled binaries
#Cannot interpolate directly in EXONERATE_PATH as
#pipeline tries to locate the exectuable and fails somehow
#This is not ideal as we are now using the arch of the submitter node to run all the farm jobs
export ARCH=`arch`
export EXONERATE_PATH="/lustre/work1/ensembl/gs2/local/${ARCH}/bin/exonerate"
export EXONERATE_VERSION='2.2.0'
export PROBE2TRANSCRIPT_PARAMS='--calculate_utrs --utr_multiplier 1'
#export PROBE2TRANSCRIPT_BSUB_CMD='bsub -q hugemem -R "select[mem>30000] rusage[mem=30000]" -M 30000000'
#remove this to run locally
export PROBE2TRANSCRIPT_BSUB_CMD='bsub'
#Now calculate requirements dynamically
# -R \\"select[mem>$musage] rusage[mem=$musage]\\" -M $musage_k'

#### Some convinient aliases
#Some also set in pipeline.env
alias arraysdir='cd $ARRAYS_HOME' 
alias configdir='cd $SRC/ensembl-analysis/modules/Bio/EnsEMBL/Analysis/Config'




################################################################################
# Func      : _InitEnv(
# Desc      : Sets up all config defined variables. Should only be called from
#             instance config file (species_VERSION.arrays) e.g. mouse_51.arrays 
# Args [n]  : 
# Return    : none
# Exception : 
################################################################################




_InitEnv(){

	#Set all generic pipeline stuff e.g. DB vars, PATH/PERL5LIB etc
	_InitPipelineEnv

	echo ":: Setting config for $SPECIES array mapping"

	if [ $warn ]
	then
		echo 'NOTE: NIMBLEGEN_TILING will never go through ImportArrays or ProbeTranscriptAlign steps?'
	fi
	
	#ValidateBooleans can we write a method for this
	if [ $MULTI_SPECIES ] && [ $MULTI_SPECIES -ne 1 ]; then
		echo 'MULTI_SPECIES is a boolean variable, please specify 1 or omit for default of 0'
	else
		#change to script paramter
		export MULTI_SPECIES=' -multi_species '
	fi
	
   


	#Let's trim this and move the host and working dir to the window title if we can

		export PS1="\
\[\033[31m\]\
arrays@\h ${DB_NAME}>\
\[\033[0m\]"

	#Can we set these so they are dynamic dependent on the value of $DATA_HOME? 
	#i.e. to facilitate use of $HOME for hugemem I/O
	#Contains only array format data dirs or the GENOMIC or TRANSCRIPTSEQS dirs
	#These should also match config in Analysis/Config/ImportArrays
	#uc SPECIES_COMMON here?
	export ARRAYS_HOME=${DATA_HOME}/${SPECIES_COMMON}
	export WORK_DIR=${DATA_HOME}/${DB_NAME}
	#This is used in pipeline alias workdir
	export BACKUP_DIR=${WORK_DIR}/backup
	export PIPELINE_OUT=${WORK_DIR}/align_out

	#Check some dirs are present
	CheckDirs $ARRAYS_HOME

	#Create db output dir if not present
	MakeDirs $WORK_DIR

	## Test and dump seq files ##
	## Also for TRANSCRIPT ALIGN_TYPE do extra DNADB checks
	## Import transcripts as coord_system if not present

	#Actually version is not right test for transcript set
	#Validate version
	#version_db=$(echo $DB_NAME | grep /[a-z]+_funcgen_${VERSION
	#check standard location and match version

	#Can we add /data/blastdb paths to here, no standard naming :( !!!

	if [ $warn ]
	then
		echo "NOTE:: Separate these into a separate sub to avoid having lengthy seq_dumping happening in the init func?"
		echo "NOTE:: Or do we want this interactive stuff forced at the start, so we don't nohup this and then have it hanging?"
	fi


	#Set/validate formats/types
	SetArrayFormats $ARRAY_FORMATS
	SetAlignTypes $ALIGN_TYPES

	for align_type in $ALIGN_TYPES
	do
	
		#Check/SetUp TRANSCRIPT dependant vars


		#Can remove this as this is generic for both seq dumps

		if [ $align_type = TRANSCRIPT ]
		then

			if [ $warn ]
			then
				echo 'NOTE: Also need to check DNADB vars if ALIGN_TYPE is TRANSCRIPT'
			fi

			CheckVariables DNADB_NAME DNADB_HOST BUILD
			
			#This could be the admin user, which we don't want for DNADB
			export DNADB_USER=${DNADB_USER:='ensro'}
			#We're not defaulting host, so let's not default port either
			export DNADB_PORT=${DNADB_PORT:='3306'}
			export DNADB_USER=${DNADB_USER:='ensro'}
   			#export DNADB_OPTIONS=' -
			#export DNADB_MYSQLARGS
		fi


		seq_type="${align_type}SEQS"
		found_file=
		seq_path=$(eval "echo \$$seq_type")

		#Test seq path		

		if [ $seq_path ]
		then

			#echo 'seq path defined'
		
			if [ ! -f "$seq_path" ]
			then
				echo "Could not find $seq_type: $seq_path"
			else
				found_file=1
			fi
		fi
	

		#Use default seq file based on validated build?

		if [ ! $found_file ]
		then
			seqs_dir="${ARRAYS_HOME}/${seq_type}"
				
			if [ -d $seqs_dir ]
			then

				if [ ! $found_file ]
				then

					if [ $seq_type = GENOMICSEQS ]
					then
						file_regex=toplevel_*

					elif [ $seq_type = TRANSCRIPTSEQS ]
					then
						file_regex=transcripts.*
					fi


					# Now ask if we want to use any of the files currently present					
					#This will only capture the first line
					#Do we have to read here or just do in loop below
					#files=$(ls $ARRAYS_HOME/$seq_type/$file_regex)
					#This puts all the separate lines into the first array element
					#Would need to cahnge separator
				    #files does not have scope outside of this loop
					#Is this because file is only defined in loop?
					# no file=
					#This is because the piped command runs as a separate process in a subshell and can't modify the 
					#files var in the parent environment, so is always locally scoped
					#ls $ARRAYS_HOME/$seq_type/$file_regex | while read file
					#do
					#	echo "file is $file"
					#	files="$file $file"
					#done
			 
					files=

					for file in $(ls $ARRAYS_HOME/$seq_type/${SPECIES}_${file_regex}.fasta)
					do
						files=($files $file)
					done		
								
					num_files=${#files[@]}
					#This can be an empty string so num_file can be 1	
		  			i=0
					
		
					if [ $files ] && [ $num_files -gt 0 ]
					then
						echo "Would you like to use the following $seq_type file?"
				
                    #Clean REPLY before while just incase we have inherited it from a previous question
						REPLY=

					#Need to evaluate vars here with ""'s as might be undef, which would cause error
	
						while [ "$REPLY" != y  ] && [ $i -lt "$num_files" ]; do
				   		#This causes a hang
						#REPLY=$(AskQuestion "Would you like to use the following $seq_type file? [y|n] $file")
							AskQuestion "${files[$i]} [y|n]"
						
							if [ $REPLY = y ]; then
								eval $seq_type=${files[$i]}
								echo ":: Using $seq_type file: ${files[$i]}"
								found_file=1
							fi
						
							i=`expr $i + 1`
						done	
					fi
				fi
			fi

			if [ ! $found_file ]
			then
				AskQuestion "No $seq_type file found, would you like to DumpSeq? [y|n]"

				if [ $REPLY = y ]
				then
					DumpSeq $align_type
					found_file=1
				fi
			fi	
		fi

		if [ ! $found_file ]
	   	then
	   		echo "WARNING: Could not set valid $seq_type file, please dump manually or try again"
			exit 1;
     	fi
	done

	echo "GENOMICSEQS:      $GENOMICSEQS
TRANSCRIPTSEQS:   $TRANSCRIPTSEQS

"
workdir

}


#Could we move this to pipeline.env and make generic?

DumpSeq(){
	seq_type=$1
	#should take output_file var here?

	CheckVariables seq_type

	#Validate
	ValidateVariable seq_type VALID_ALIGN_TYPES

	#seqs_var="${seq_type}SEQS"
   	dir="${ARRAYS_HOME}/${seq_type}SEQS"
   	
	#Create SEQS dir with large stripe size

	if [ ! -d $dir ]
	then
		MakeDirs $dir
		lfs setstripe $dir 0 -1 -1
	fi
	

	if [ $seq_type = "GENOMIC" ]
	then
		export GENOMICSEQS=${dir}/${SPECIES}_toplevel_${VERSION}.fasta 
		echo ":: Dumping softmasked genomic sequence: $GENOMICSEQS"

		#This sometimes runs out of memory on head nodes??
		#Always after chr 6 and before chr 4 for human
		#Even tho mem usage has remained ~ 20%???

		#Do this as CVS co will not be executable
		chmod +x $ANALYSIS_SCRIPTS/sequence_dump.pl

		#Can't do this if we have note made a full copy of the DB!!!
		bsub -e ${dir}/genomic_seq.err -o ${dir}/genomic_seq.out -R"select[mem>3500] rusage[mem=3500]" -M3500000 perl $ANALYSIS_SCRIPTS/sequence_dump.pl -dbhost $DNADB_HOST -dbuser $DNADB_USER -dbname $DNADB_NAME -species $SPECIES -coord_system_name toplevel -mask_repeat Dust  -mask_repeat RepeatMask  -softmask -nonref $MULTI_SPECIES -filename $GENOMICSEQS 
		echo "Now need to wait for dump to finish"

		#Can we write a func to do this?

	else
		#CheckVariables now done in _InitEnv

		export TRANSCRIPTSEQS=${dir}/${SPECIES}_transcripts.${VERSION}_${BUILD}.fasta
		echo ":: Dumping transcript sequence: $TRANSCRIPTSEQS"

		#If we are doing a TRANSCRIPT mapping, do we have to define the DNADB if we already have the correct transcript dump?
		#The default DNADB may not be the correct one
		#Need to define DNADB options string if we are doing TRANSCRIPT

		Execute perl $EFG_SCRIPTS/export/dump_genes.pl \
			-dbport  $DNADB_PORT\
			-dbuser  $DNADB_USER\
			-dbhost  $DNADB_HOST\
			-dbname  $DNADB_NAME\
			-species $SPECIES\
			$MULTI_SPECIES \
			-cdna \
			-stable_id \
			-file $TRANSCRIPTSEQS
	fi
}



################################################################################
# Func      : RunAlignments
# Desc      : Wrapper method to run the whole genomic and transcript array 
#             mapping and xref pipeline 
# Args [1]  : Optional - Custom arrays.list file
# Args [2]  : Optional - ARRAY_FORMATS for custom arrays.list file
# Return    : none 
# Exception : none
################################################################################

RunAlignments(){
	#Make this take multiple formats?
	#Is this possible to combine with the file?
	#Are we getting confused with logic names here? ProbeAlign ProbeTranscriptAlign?
	
	#This makes sure we reset the getopts ind if we have used it previously
	OPTIND=1

	array_formats=
	array_file=
	align_types=
	skip_import=
	usage='usage: ImportArrays [ -f format ]*  [ -l custom_array_list.file ] [ -s(kip import) ]'


	#Can we take array_names here too?
	#Is this wise to restrict to arrays within a linked set?


	while getopts ":f:l:hs:" opt; do
		case $opt in 
	        f  ) array_formats="$array_formats $OPTARG" ;; 
	t  ) align_types="$align_types $OPTARG" ;;
	s  ) skip_import=' -s ' ;;
	l  ) array_file=$OPTARG ;;
	h  ) echo $usage; return 0;;
	\? ) echo $usage; exit 1;;
	esac 
	done


	#Do this here so we don't have to pass to other methods
	SetArrayFormats $array_formats
	
	file_param=
	
	if [[ $array_file ]]; then
		file_param="-l $array_file"
	fi

	#Validate/Set formats and align types
	SetAlignTypes $align_types
	SetArrayFormats $array_formats
	#Do this here so we don't have to pass to other methods
	
	echo "Array Mapping Pipeline Params are:"
    echo "Funcgen DB:      $DB_MYSQL_ARGS"
	echo "Pipeline DB:     $PDB_MYSQL_ARGS"
	echo "DNA DB:          $DNADB_MYSQL_ARGS"
	echo "GENOMICSEQS:     $GENOMICSEQS"
	echo "TRANSCRIPTSEQS:  $TRANSCRIPTSEQS"

	#Don't need to pass array_formats in the following methods
	#Unless we want to change them
	BuildFastas $file_param
	#Build Fastas need updating as it is not fully tested in custom file mode

    SetUpPipeline $skip_import
	
	if [[ ! $skip_import ]]; then
		BackUpTables arrays
		ImportArrays 
		ImportWait
	fi
	
    CreateAlignIDs 
    SubmitAlign
	AlignWait
    monitor
	ProbeAlignReport
    
}



################################################################################
# Func      : BuildFastas
# Desc      : Merges array fasta files in to array format specific files for 
#             ImportArrays.  
# Args [1]  : Optional - Custom arrays.list file
# Args [2]  : Optional - array_format for custom arrays.list file
# Return    : none 
# Exception : Exits if custom file specified but not array format specific
################################################################################

BuildFastas(){
    CheckVariables WORK_DIR
	
	echo ":::: BuildFastas $* ::::"
   		
	#This makes sure we reset the getopts ind if we have used it previously
	OPTIND=1

	array_list=
	array_formats=

	usage='usage: BuildFormats [ -f format ]*  [ -l custom arrays.list ]'

	while getopts ":f:l:h" opt; do
		case $opt in 
	        f  ) array_formats="$array_formats $OPTARG" ;; 
            l  ) array_list=1 ;;
			h  ) echo $usage; return 0;;
            \? ) echo $usage; exit 1;;
		esac 
	done


	cnt=0
	
	

	#This currently doesn't allow for multiple formats in one array.list
	#We need to either implement format in list file and change this sub to handle
	#Or we need to run spearately for each format, and add format to ARRAY_FORMATS
	#Do we relaly want to have the default used?
	
	#We also need to be able to FlushFasta or DeleteFastas to enable regeneration from scratch
	
	
	#echo "array_list is $array_list"
	#echo "array_formats $array_formats"
	
	
	ArchiveFile $WORK_DIR/arrays.list
	
	if [ $array_list ]; then
		echo ":: Building $array_format fasta from custom array list: $array_list"

		echo "Not supported in efg yet"
		exit;
		
		if [ ! -f $array_list ]; then
			echo ":: Array list file does not exist: $array_list"
			exit 1;
		fi

		CheckVariables array_formats
   		cat $array_list
		
		format_fasta=$WORK_DIR/arrays.${array_formats}.fasta
		BackUpFile $format_fasta
		ChangeDir $WORK_DIR
		
		#We could do with some healtchecks here
		#AskQuestion if files already present for array_format
		#prompt to clean array and probe tables?
		
		while read path; do

			#Need to split path and format here

			if [[ -f $path ]]; then
				cp $path $WORK_DIR
				file=$(GetFilename $path)
				
				UnzipCatFasta $file $format_fasta
				
				if [ $? -ne 0 ]; then
					cnt=`expr $cnt + 1`
					echo $file >> $WORK_DIR/arrays.list
				fi
			else
				echo ":: Exiting: Could not find $path"
				return 1
			fi
			
		done < $array_list
		
		#Should copy $array_list to WORK_DIR if it isn't the same
	    	    		
    else # ! $array_list
		echo "setting array format with $array_formats"

		SetArrayFormats $array_formats
		arraysdir
				
		#We now need to build individual lists for each ARRAY_FORMAT
		#ignoring the TRANSCRIPTSEQS/GENOMICSEQS and maybe README?
		#Restrict ls to only list directories?
		#ls -d? only lists current dir as .?
		#Also account for ARRAY_FORMATS here, which may have been defined in the env.

		if [ ! -z "$ARRAY_FORMATS" ]; then
			echo ":: Building fasta from specified formats: ${ARRAY_FORMATS}"
			list=$ARRAY_FORMATS
		else
			echo ":: Building fasta from default array formats in ${ARRAYS_HOME}"
			list=$(ls)
		fi


		
		for format in $ARRAY_FORMATS; do
			echo ":: Building fasta file for $format"

		#We could do with some healthchecks here
		#AskQuestion if files already present
		#prompt to clean array and probe tables?
		#Or will pipeline fail when we see a duplicate array?
		#We have should have an array_format subdir
			
			format_home=${ARRAYS_HOME}/${format}
			format_fasta=$format_home/arrays.${format}.fasta
			
		#BackUp when we are dealing with the WORK_DIR version
			
		#this is necessary to allow the grep below
			if [[ ! -f ${format_home}/arrays.list ]]; then
				touch ${format_home}/arrays.list
			fi
			
			ChangeDir $format_home
			
			for file in $(ls); do
			
				if [ ! -d $file ]; then
	
					if [[ $file != arrays.${format}.fasta ]] && [ $file != arrays.list ]; then 	#Skip previous arrays.fasta
					
						if [ ! $(grep "^$file" ${format_home}/arrays.list) ]; then
					#This will catch any files which are a sub string of the zipped file
					#So should skip both the zip and the unzipped file
							
					#However this may also mean some arrays maybe missed if they are 
					#sub strings of pre-existing arrays, but only likely if they are added unzipped
					#e.g. first_array.with_suffix.gz
					#first_array.gz
					#This will not cause a problem because of the gz
					#but add it unzipped
					#first_array 
					#Will grep first_array.with_suffix.gz
					#And therefore will be skipped
							
							UnzipCatFasta $file $format_fasta
							
							if [ $? -ne 0 ]; then
								cnt=`expr $cnt + 1`
								echo $file >> $format_home/arrays.list
							fi
						fi
					fi
				fi
			done


			#Regenerate softlink in case data/workdir has moved
			if [ -L $WORK_DIR/arrays.${format}.fasta ]; then
				
				rm -f $WORK_DIR/arrays.${format}.fasta
			fi
			
			Execute ln -s $format_fasta $WORK_DIR/arrays.${format}.fasta
			cat $format_home/arrays.list >> $WORK_DIR/arrays.list
			
		    #Change back to species/array home here
			arraysdir
		done
	fi


	#Now just print out some details
	
	if [[ $cnt -eq 0 ]]; then
	    echo ":: No new fasta files found"
	else
		echo ":: Found $cnt new array fasta files"
	fi
	
    cnt=($(wc -l $WORK_DIR/arrays.list))
	cnt=${cnt[0]}
    echo ":: $cnt array fasta files cat'd to $WORK_DIR/arrays.list:"
	cat $WORK_DIR/arrays.list
	echo ""
	
	workdir
}



################################################################################
# Func      : UnzipCatFasta
# Desc      : Unzips fasta file if required and tests for ^> fasta headers
# Args [1]  : Fasta file path
# Return    : Boolean - true is succesful, false if not fasta file
# Exception : None
################################################################################


#We need to remove this or make it aware of files that are already unzipped
#so we don't get duplication

UnzipCatFasta(){
	
	

    file=$1
	shift 
	format_fasta=$1
	shift

	CheckVariables file format_fasta


	#Need to implement IsCompressed here?
	
    if [[ $file = *zip ]] || [[ $file = *gz ]]; then
		
		if [[ $file = *zip ]]; then
			unzip $file
			nfile=$(echo $file | sed 's/\.zip//')
		else	
			gunzip $ofile
			nfile=$(echo $file | sed 's/\.gz//')
		fi
	else
		nfile=$file
	fi
			

    #This awk will not work on large files!!
	#count=$(awk  'BEGIN { RS = "/^>/" }; /^>.*\n^[AGCTUagctu]+\n/' $nfile | wc -l) 
	#count=$(( ($count - 1) /2 ))

	#Cannot do wrapped grep
	#header and non seq lines must match!
	dos2unix $file 
	count=$(grep -c -E "^>" $nfile)
	non_seq_count=$(grep -c -v -E "^[AGCTUacgtu]+$" $nfile)
	
	if [ $count -ne $non_seq_count ]; then
		echo "$count $non_seq_count"
		echo "Found non valid fasta record in $nfile"
		exit 1
	fi
	
    if [[ $count -gt 0 ]]; then
		echo ":: Found array fasta file: $file"
		echo ":: Contains $count probes"
		fasta=1
		cat $nfile >> $format_fasta
    else
		echo ":: Skipping non-fasta file: $file"
		fasta=0
	fi
	
    return $fasta
}


SetAlignTypes(){
	align_types=$*

	if [[ $align_types ]]; then
		
		for type in $align_types; do
			ValidateVariable type VALID_ALIGN_TYPES
		done

		ALIGN_TYPES=$align_types
		echo ": Setting align types: $ALIGN_TYPES"

	elif [[ ! $ALIGN_TYPES ]]; then
		ALIGN_TYPES=$VALID_ALIGN_TYPES
		echo ": Setting default align types: $ALIGN_TYPES"
	fi
	
	#else we have ALIGN_TYPES which has been previously Set & therefore validated
}



################################################################################
# Func      : SetArrayFormats
# Desc      : Set the ARRAY_FORMATS varible with passed args, or defaults to
#             previously set ARRAY_FORMATS  or the array formats present in 
#             $ARRAYS_HOME   
# Args [*]  : optional - list of valid array formats
# Return    : none 
# Exception : none
################################################################################

SetArrayFormats(){
	formats=$*
	CheckDirs $ARRAYS_HOME
	cwd=$PWD
	arraysdir
	skip=

	
	if [ -z "$formats" ]; then
		#$formats not set

		if [ ! -z "$ARRAY_FORMATS" ]; then
			#$ARRAY_FORMATS previously set
			skip=1
		else
			echo ": Setting default ARRAY_FORMATS in: ${ARRAYS_HOME}"
			tmp=$(ls $ARRAYS_HOME)
			formats=

			for entry in $tmp; do
				
				if [[ ! $entry = GENOMICSEQS ]] && [[ ! $entry = TRANSCRIPTSEQS ]]; then

					if [ -d $entry ]; then
						formats="$formats $entry"
					fi
				fi
			done

			echo ": ARRAY_FORMATS are $formats"
		fi
	fi

	#Validate new formats
	if [ ! $skip ]
	then
	
		for format in $formats; do

		  if [ ! -d $format ]; then
			  echo "You have specified an array format which does not exist in your input dir: $PWD/$format"
			  exit 1
		  fi

		  #we need to capture error here and warn about only having valid format dirs in arrayshoms
		  ValidateVariable format VALID_ARRAY_FORMATS
		done		 

		export ARRAY_FORMATS=$formats
		echo ": Setting ARRAY_FORMATS: $ARRAY_FORMATS"
	fi

	cd $cwd
}


################################################################################
# Func      : SetUpPipeline
# Desc      : Imports pipeline tables, analyses and rules into DB
# Args [1]  : 
# Args [2]  : Optional - ARRAY_FORMAT for custom arrays.list file
# Return    : none 
# Exception : none
################################################################################

#We need to separate the DB generation from this so that we can set up individual analyses whilst we have other stuff running?
#Can we import rules/goals on top of each other?
#We may get a running pipeline picking up the jobs straight away, so we need to make sure evertyhing is in place first. i.e. BuildFastas

SetUpPipeline(){
	echo ":: SetUpPipeline $*"

	#This makes sure we reset the getopts ind if we have used it previously
	OPTIND=1

	array_formats=
	skip_import=
	usage='usage: SetUpPipeline [ -f format ]* [ -s(kip import) ] [ -h(elp) ]'


	#Need to take align type here too

	while getopts ":f:hs:" opt; do
		case $opt in 
	        f  ) array_formats="$array_formats $OPTARG" ;; 
	s  ) skip_import=' -s ' ;;
	h  ) echo $usage; return 0;;
	\? ) echo $usage; exit 1;;
	esac 
	done



	SetArrayFormats $array_formats

	#Set up analysis and rule config files
	analysis_conf_file=$WORK_DIR/probe_transcript_analysis.conf
	rules_conf_file=$WORK_DIR/probe_transcript_rules.conf
	
	BackUpFile $analysis_conf_file
	rm -f $analysis_conf_file
	touch $analysis_conf_file

	BackUpFile $rules_conf_file
	rm -f $rules_conf_file
	touch $rules_conf_file


	#Can't add params to analysis here as we don't have access yet, but we should do that somewhere?
	#Could do it in the Runnable DB, but highly redundant, as this will be tested/overwritten for each job i.e. thousands of times.
	

	#Space separate logic names to allow readability in Analysis::Config modules


	#This is not right?
	#We can map AFFY and AFFY_ST together, as they have the same probe design, i.e. 25 mers
	#Just keep separate for now for simplicity
	#We also don't want ProbeTranscriptAlign for Tiling arrays?
	#We need to this dependent on ARRAY_DESIGN?  This is current AFFY or AFFY_ST
	#or CLASS. TILING/EXPRESSION.
	#This concept is a little weird as it's the application of the array, not the array design itlsef, 
	#as you can use TILING designs for WG expression analysis.  Then the idea of probe sets becomes a little
	#awry.

	if [ $warn ]; then
		echo 'TO DO: Need to fix analysis and rules set up dependant on array type, i.e. tiling should not be transcript mapped'
		echo 'NOTE: also need to test for transcript coord_system if we are mapping to transcript?'
	fi

	echo ": Writing analysis and rules config"

    #added null modules to stop warnings on import
	
    #These will not overwrite current entries in analysis, so we may have some old data listed
	#Need to validate this?
	
	#Create empty conf files
	rm -f $rules_conf_file
	touch $rules_conf_file
	rm -f $analysis_conf_file
	touch $analysis_conf_file
	
	

	align_condition=

	if [ ! $skip_import ]; then
		align_condition='condition=Import_Wait'

	#Build mutli-condition waits first
		echo "[Import_Wait]" >> $rules_conf_file

		for format in $ARRAY_FORMATS; do
			echo "condition=Import_${format}_Arrays" >> $rules_conf_file
		done


	#Do we not need to add another rule here for the Submit jobs to wait for Import_Wait?
	#Or is this what the Import_Wait accumulator is doing?
	
	
		echo "[Import_Wait]
module=Accumulator
input_id_type=ACCUMULATOR
">> $analysis_conf_file
	
	else
		echo ": Skipping Import setup"
	fi

	echo "[Align_Wait]" >> $rules_conf_file

	for format in $ARRAY_FORMATS; do
		echo "condition=${format}_ProbeAlign" >> $rules_conf_file
		echo "condition=${format}_ProbeTranscriptAlign" >> $rules_conf_file
	done


	#Do we not need to add another rule here for the Submit jobs to wait for Import_Wait?
	#Or is this what the Import_Wait accumulator is doing?
	
	
	echo "[Align_Wait]
module=Accumulator
input_id_type=ACCUMULATOR
">> $analysis_conf_file


	#Need to account for align type here!
	#We are currently writing analyses for both
	
	for format in $ARRAY_FORMATS; do

		if [ ! $skip_import ]; then
		
			echo "[Import_${format}_Arrays]
module=ImportArrays
input_id_type=PROBE_SET

[Submit_Import_${format}_Arrays]
input_id_type=PROBE_SET
" >> $analysis_conf_file
			
			echo "
[Import_${format}_Arrays]
condition=Submit_Import_${format}_Arrays
" >> $rules_conf_file
		fi
		
		echo "[Submit_${format}_ProbeAlign]
input_id_type=PROBE_CHUNK

[${format}_ProbeAlign]
program=exonerate
program_version=$EXONERATE_VERSION
program_file=$EXONERATE_PATH
module=ProbeAlign
input_id_type=PROBE_CHUNK

[Submit_${format}_ProbeTranscriptAlign]
input_id_type=PROBE_TRANSCRIPT_CHUNK

[${format}_ProbeTranscriptAlign]
program=exonerate
program_version=$EXONERATE_VERSION
program_file=$EXONERATE_PATH
module=ProbeAlign
input_id_type=PROBE_TRANSCRIPT_CHUNK
" >> $analysis_conf_file
		
		echo "
[${format}_ProbeAlign]
$align_condition
condition=Submit_${format}_ProbeAlign

[${format}_ProbeTranscriptAlign]
$align_condition
condition=Submit_${format}_ProbeTranscriptAlign
" >> $rules_conf_file
		
	done



	CreatePipelineTables
   


	#Could do with testing for files here
	echo ": Importing analysis config" 

	#EFG
    Execute perl $PIPELINE_SCRIPTS/analysis_setup.pl $PDB_SCRIPT_ARGS -read -file $WORK_DIR/probe_transcript_analysis.conf 

    echo ": Importing rules config" 
	#EFG
    Execute perl $PIPELINE_SCRIPTS/rule_setup.pl  $PDB_SCRIPT_ARGS -read -file $WORK_DIR/probe_transcript_rules.conf

	#Could we test output for 'Not storing' This will not detect whether there are other rules in the DB

	if [ $warn ]; then
		echo "NOTE: Need to clean analysis table? Or is this a setup problem"
		echo "NOTE: check for genomic seq here, are all the chrs present in $GENOMICSEQS"
		#This would requite getting all toplevel (inc non-ref) and greping file
		#Is this another case for PipeLineHelper.pm
	fi
	
    Execute perl $PIPELINE_SCRIPTS/setup_batchqueue_outputdir.pl 
	
	#This will always fail and return 0?!
	#As we don't have any input_id written for the accumulator yet
	#This needs to be done after we have created the input ids for ImportArrays
	CheckPipelineSanity

	echo ""

	}






################################################################################
# Func      : ImportArrays
# Desc      : Collapses arrays of related formats in to unique probe records 
#             based on probeset and sequence identity
# Params    : -f optional - Array format e.g. AFFY_UTR, can be specified more than once
#             -r rollback - Rollback flag remove IMPORTED status from each array to enable
#             rollback in ImportArrays.
# Return    : none 
# Exception : Exits if ARRAY_FORMATS not defined
################################################################################

#Need to add no collapse/straight import functionality for nr_arrays i.e. ST arrays

ImportArrays(){
	echo ":::: ImportArrays $* ::::"

	formats=
	rollback=

	#This makes sure we reset the getopts ind if we have used it previously
	OPTIND=1
	usage='usage: ImportArrays [ -f format ]*  [ -r(ollback) ] [ -h(elp) ]'

	while getopts ":f:rh" opt; do
		case $opt in 
	        f  ) formats="$formats $OPTARG" ;; 
            r  ) rollback=1 ;;
			h  ) echo $usage; return 0;;
            \? ) echo $usage; exit 1;;
		esac 
	done

			
	SetArrayFormats $formats
	CheckVariables ARRAY_FORMATS ARRAYS_HOME
			
	echo "Need to check pipeline sanity here?"
	echo "Need to set up separate pipeline DB"
	echo "Should clean up job_status here too?"

	#pipeline_sanity.pl -dbhost $DB_HOST -dbport $DB_PORT -dbuser $DB_USER -dbpass $DB_PASS -dbname $DB_NAME
	#This is currently barfing due to presence of other analyses which do not have modules.
	
	workdir
	
	#Now done in wrapper
	#BackUpTables arrays
	
	#We need to write the config file for each format here.
	cnt=0

	#Create input_ids and config file for ImportArrays
	#Let's have 1:genome etc to maintain some continuity
	#export ARRAY_FORMAT_FILE=$WORK_DIR/ImportArrays.config
	#BackUpFile $ARRAY_FORMAT_FILE
	#rm -f $ARRAY_FORMAT_FILE
	#touch $ARRAY_FORMAT_FILE


	#Need to sub relevant parts of this loop to enable TestImportArray?
	#Would not write or bsub, but then were not testing storage code
	#This loop only works as the rule manager finishes after every submit?
	#Or does it...is this running in series?
	#If so we need to kick off with no -analysis flag 
	#And get it to stop after Import_Wait somehow, so we can create the Align IDs etc
	#Or should we just put this in a RunnableDB and then pipeline the whole thing?

	for format in $ARRAY_FORMATS; do
		logic_name="Import_${format}_Arrays"

		#How does this not result in duplicate imports?
		CleanJobs $logic_name
		
		if [[ $rollback ]]; then
			
			SetArrayNames $format
			
			if [[ $ARRAY_NAMES ]]; then
				
				names=$(echo $ARRAY_NAMES | sed "s/ /', '/")	
				
				sql="DELETE s from status s, status_name sn, array_chip ac where ac.name in('${names}') and ac.array_chip_id=s.table_id and s.table_name='array_chip' and s.status_name_id=sn.status_name_id and sn.name='IMPORTED'";
				echo ": Deleting IMPORTED status for: $ARRAY_NAMES"
				Execute echo $sql | mysqlefg
			fi
		fi
		
		
	    #Do we want to do this for every run, what if we've had a failure and we're just re-running
	    cmdline="make_input_ids $PDB_SCRIPT_ARGS -single -logic_name Submit_Import_${format}_Arrays -single_name ${format}:genome"
		echo ": Creating Import_${format}_Arrays input IDs" 
		Execute $cmdline
		export RAW_FASTA=$WORK_DIR/arrays.${format}.fasta
	
		#Set musage
		SetMusage -a "$ARRAYS_HOME/$format/arrays.${format}.fasta" -f $IMPORT_MEM_FACTOR -b $IMPORT_MEM_BASE
		musage_k=$(($musage * 1000))
	   
		if [ $musage -gt $MAX_HUGE_MEM ]; then
			echo "Expected memory usage exceeds that available on hugemem queue: $musage. Modify rsuage generation?"
			exit 1;
		elif [ $musage -gt $MAX_NORMAL_MEM ]; then
			queue=hugemem
			echo "Need to move things to home dir for hugemem jobs. This will go away when we import ST arrays directly?"
			exit 1;
		else
			queue=normal
		fi

		
		#This has to be double quotes or it get's ignored!!???
		#Weird behaviour with -R options, can sometimes also truncate see RunTranscriptXrefs
		resource="-R \"select[mem>${musage}] rusage[mem=${musage}]\" -M $musage_k"
		
		#We need to copy input files from lustre to somewhere where hugemem can see it
		#Also change respective vars and handle moving data back to lustre after ImportArrays
		#Or shall we move everything for Import and then move everything back later?
		
		
		export IMPORT_ARRAYS_QUEUE=$queue
		export IMPORT_ARRAYS_RESOURCE=$resource
		echo ":: Import_${format}_Arrays bsub params: -q $IMPORT_ARRAYS_QUEUE $IMPORT_ARRAYS_RESOURCE"
		
        #We are just running the pipeline through one cycle to submit here
		#This is because we need to create the input_ids for each ProbeAlign step before we carry on
		#We should create another RunnableDB to do this automatically so we can run the pipeline all in one go!
		cmdline="perl $PIPELINE_SCRIPTS/rulemanager.pl $PDB_SCRIPT_ARGS -once -analysis $logic_name -input_id_type PROBE_SET"
		#echo ":: Import_${format}_Arrays: $cmdline"


		#Archive the names files before Executing
		ArchiveFile "${WORK_DIR}/arrays.$Pformat}.names"
		Execute $cmdline
		
		
		#THis is running with jobname like nj_arrays_mus_musculus_funcgen_51_37d:default. How do we replace the default with the analysis? Look in rulemanager. This should have used the analysis!!		
	done
		
	
	echo "Completed incremental submission of ImportArrays for formats: $ARRAY_FORMATS"
		
}



################################################################################
# Func      : ImportWait
# Desc      : Accumulator step to wait for all ImportArrays jobs
# Arg[1..n] : None
# Return    : None 
# Exception : None
################################################################################


ImportWait(){
	echo ":: ImportWait $*"
	#add opts here for -force_accumulators and -once?

	cmdline="perl $PIPELINE_SCRIPTS/rulemanager.pl $PDB_SCRIPT_ARGS -force_accumulators -analysis Import_Wait"

	#echo ":: ImportWait $cmdline"

	Execute $cmdline
	
	#echo "Can we cat the AFFY and AFFY_ST arrays_nr files?"
	#As these should use the same rules?  This would mean running with one logic name for both
	#Just keep separate for now

	#Now cat the array names files output by ImportArrays
	#This is to enable any ProbeFeature/IDXref/UnmappedObject deletes

	#export NAMES_FILE="${WORK_DIR}/arrays.names"

	#ArchiveFile NAMES_FILE

	#CheckVariables ARRAY_FORMATS

	#for format in $ARRAY_FORMATS; do
	#	format_names="${WORK_DIR}/arrays.${format}.names"
	#	cat $format_names > $NAMES_FILE
	#done
		



	#We could validate array names or at least number of arrays here?
	echo "Finished waiting...ready for CreateAlignIds [ formats ]"
}



SetArrayNames(){
	formats=$*

	format_files=

	if [ ! -z "$formats" ]; then

		for format in $formats; do
			format_files="${WORK_DIR}/arrays.${format}.names $format_files"
		done
	else
		format_files="${WORK_DIR}/arrays.names"
	fi


	#need to change this so it uses the format files instead of this cat'd file?
		#when do we ever use the cat'd file?

	ARRAY_NAMES=

	for file in $format_files; do

		if [ ! -f $file ]; then
			echo "Could not find $file"
			echo "You need to ImportArrays for this file"
		else
			
			while read name; do
				ARRAY_NAMES="$name $ARRAY_NAMES"			
			done < $file
		fi
	done


	#Can't do this as we may want to call this format by format
	#and some may not have names files yet?
	#i.e. When ImportArrays has failed
	#if [ -z $ARRAY_NAMES ]; then
	#	echo "Found no valid array names"
	#	exit 1;
	#fi

	#So must test return value in caller		

}

#getopts doesn't parse arrays of opts
#Multiple switches didn'twork

GetOptArgArray(){

	args=$*
	echo "args are $args"
	echo "optind is $OPTIND"
	
	inarg=

	#for (( i=$OPTIND; $inarg; i++ )); do			
	#	echo "arg is $arg"
				
	#	if [ $inarg ]; then
	
	#		echo "inarg"
			
			#if [[ "${args[$i]}" = -* ]]; then
			#	inarg=0
			#	OPTIND=$i
			#	echo "next arg is at $OPTIND"
			#fi
			
	#	fi
	#done	
}

################################################################################
# Func      : CreateAlignIDs
# Desc      : Creates ProbeAlign or ProbeTranscriptAlign input IDs
# Opt -t    : Mandatory - Align types e.g. GENOMIC and/or TRANSCRIPT
# Opt -f    : Optional - Array formats e.g. 'AFFY AFFY_ST', defaults will be used if absent
# Opt -c    : Optional - Chunks, chunks will be calculated if omitted
# Arg[3]    : would need to parse opts
#			  ARRAY_DESIGN????? Would need to create different sets of IDs for different designs
#             This would probably screw up  the chunking based on the IDs, would also need different source file
#             Hence another config file would need to be written. 
# Return    : none 
# Exception : None
################################################################################

CreateAlignIDs(){
	
	echo ":::: CreateAlignIDs $*"

	align_types=
	formats=
	chunk_arg=
	usage='usage: CreateAlignIDs [ -t align_type ]* [ -f format ]* [-c chunks] [ -h(elp) ]'

	#This makes sure we reset the getopts ind if we have used it previously
	OPTIND=1

	while getopts ":t:f:c:h" opt; do
		case $opt in 
			t  ) align_types="$align_types $OPTARG" ;;
		f  ) formats="$formats $OPTARG" ;; 
		c  ) chunk_arg=$OPTARG ;;
		h  ) echo $usage; return 0;;
		\? ) echo $usage; exit 1;;
		esac 
	done

	#No shift required as we don't expect any other args
    #shift $(($OPTIND - 1))

	SetAlignTypes $align_types
	SetArrayFormats $formats
	#We need to back up the input ids before deleting and recreating
	



	for format in $ARRAY_FORMATS; do
	
		#If we are re-initialising the env
		#and have only built the fastas using a subset of
		#of formats, this will currently try and create IDs for all available formats
		#and will give errors as the fasta will not be found
				#Just do the fasta count once if required
		

		NR_FASTA="$WORK_DIR/arrays_nr.${format}.fasta"
		
		if [[ ! -f $NR_FASTA ]]; then
			echo "Skipping $format, NR_FASTA not found: $NR_FASTA"
		else
			
			if [ ! $chunks_arg ]; then
				num_probes=`expr $(wc -l $NR_FASTA | sed "s| $NR_FASTA||") / 2`
				echo ": Found $num_probes $format NR probe sequences"
			fi
			
			for align_type in $ALIGN_TYPES; do	 
				
  				if [ $align_type = GENOMIC ]; then
					id_type='PROBE_CHUNK' 
					logic_name="${format}_ProbeAlign"
					
				elif [ $align_type = TRANSCRIPT ]; then
					id_type='PROBE_TRANSCRIPT_CHUNK'
					logic_name="${format}_ProbeTranscriptAlign"
				fi
				
            #Delete the old input IDs here before importing!?
				CleanJobs $logic_name

            #Calculate the number of chunks
				if [ $chunk_arg ]; then
					chunks=$chunk_arg
				else
                #Tune the chunk numbers wrt array type and align type
					
                #This is roughly based on nimblegen probes being 50-60mers
	    	    #And affy probes being roughly 25mers				
					if [[ $ARRAY_DESIGN = NIMBLEGEN_TILING ]]; then
						probes_per_chunk=8000
					    #May need to change this
					else
					#probes_per_chunk=8000
					#Takes ~ 7 mins for Genomic ProbeAlign
					#On head node
					#probes_per_chunk=30000
					#With batch size 5, this should take between 2.5-5 hours!!
					#This is taking >3 hours?
					#On a fully loaded node
					#probes_per_chunk=10000
					#Takes >1.5 hours on fully loaded node
					#probes_per_chunk=5000

						probes_per_chunk=3000
					fi

					if [[ $logic_name = *ProbeTranscriptAlign ]]; then	
					#Multiply for transcripts, 10 for now
						probes_per_chunk=`expr $probes_per_chunk \* 10`
					fi
	
					#Add 1 to avoid running with 0
					chunks=$((($num_probes / $probes_per_chunk) + 1))	
					
				fi

		
				echo ": Creating $chunks $logic_name IDs with $probes_per_chunk probes/chunk (e.g. 1:$chunks:$format)"	
				anal_id=$(QueryVal PIPELINE "select analysis_id from analysis where logic_name='Submit_${logic_name}'")
				cmd="for \$i(1..$chunks){ print \"insert into input_id_analysis(input_id,analysis_id,input_id_type) values (\\\"\$i:$chunks:$format\\\",$anal_id,\\\"$id_type\\\"); \n\" }"	
			
			    #Execute "perl -e '$cmd' > ${WORK_DIR}/probe_input_ids.sql"
			    #Cannot get Execute to work for this perl -e
				perl -e "$cmd" > ${WORK_DIR}/probe_input_ids.sql
			    #Execute "mysql $PDB_MYSQL_ARGS < ${WORK_DIR}/probe_input_ids.sql"
				mysqlpipe < ${WORK_DIR}/probe_input_ids.sql

			#Catch error here
			#This is currently causing problems as you can't have the same input_id for two different analyses!!!
            #We now need to parse the input_id differently in ProbeAlign, or is this done in ExonerateProbe?
	done
	fi
	done
	
	echo ": Ready for SubmitAlign"
	
	}


#Do all resources in one go 
#So this can be done by CreateAlignIDS
#And then we can have separate align waits
#and can therefore kick of probetranscriptalign RunnableDB asynchronously
#if we restart the env after failure?
#Therefore should be done in SubmitAlign(which well eventually just run straight through to probe2transcript)


SetResources(){
	echo ": SetResources $*"
	#We may not have Transcript seqs as we may only be doing genomic mapping?
	#Do for all now and change this if we want to include genomic only mapping
	#We have other thing we need to fix for this?

	types=
	formats=
	chunk_arg=
	usage='usage: SetResources [ -t resource_type  e.g. TRANSCRIPT, GENOMIC or XREF ]* [ -f format ]* [ -h(elp) ]'

	

	#This makes sure we reset the getopts ind if we have used it previously
	OPTIND=1

	while getopts ":t:f:c:h" opt; do
		case $opt in 
			t  ) types="$types $OPTARG" ;;
		f  ) formats="$formats $OPTARG" ;; 
		h  ) echo $usage; return 0;;
		\? ) echo $usage; exit 1;;
		esac 
	done

	SetArrayFormats $formats
	types=${type:="XREF $VALID_ALIGN_TYPES"}

	#Now we need multipliers $XREF_RESOURCE_MULTIPLIER $TRANSCRIPT_RESOURCE_MULTIPLIER $GENOMIC_RESOURCE_MULTIPLIER
	

	for type in $types; do
		

		#We want to set eg 
		#GENOMIC_AFFY_UTR_MEM_MB
		#GENOMIC_AFFY_UTR_MEM_K
		seq_type=

		if [ $type = XREF ]; then
			seq_type=TRANSCRIPT
		fi

		seq_type=${seq_type:=$type}
		seq_file=$(eval "echo \$${seq_type}SEQS")

		#We need to get number of chunks here


		#We need to set a var name using a var here using eval
		
		eval ${type}_${format}_MEM_K=$mem

		eval ${type}_${format}_MEM_MB=$(($mem / 1000 | bc))


	done
}


#Add verbose option here?

TestProbeAlign(){
	echo ":: TestProbeAlign $*"

	align_type=
	formats=
	chunk=
	write=
	logic_name=
	usage='usage: TestProbeAlign -t GENOMIC|TRANSCRIPT -f "array_format" -c "chunk" [ -w(rite flag) ] [ -h(elp) ]'

	#This makes sure we reset the getopts ind if we have used it previously
	OPTIND=1

	while getopts ":t:f:c:wh" opt; do
		case $opt in 
			t  ) align_type=$OPTARG ;;
	        f  ) format=$OPTARG ;;
            c  ) chunk=$OPTARG ;;
			w  ) write=' -write ' ;;
			h  ) echo $usage; return 0 ;;
		    \? ) echo $usage; return 1 ;;
		esac 
	done



	CheckVariables format align_type
	
	ValidateVariable format VALID_ARRAY_FORMATS
	ValidateVariable align_type VALID_ALIGN_TYPES
	
	
	#Check nr fasta file
	CheckFile "${WORK_DIR}/arrays_nr.${format}.fasta"
	
	
	#Set the logic name
	if [[ $align_type = "GENOMIC" ]]; then
		CheckVariables GENOMICSEQS
		logic_name="${format}_ProbeAlign";
	else
		CheckVariables TRANSCRIPTSEQS
		logic_name="${format}_ProbeTranscriptAlign";	
	fi
	
	#Get the chunk count for this analysis
	chunks=$(QueryVal PIPELINE "select count(*) from input_id_analysis i, analysis a where a.logic_name='Submit_${logic_name}' and a.analysis_id=i.analysis_id")
	
	if [ $chunks = 0 ]; then
		echo "Could not find any input_ids for analysis $logic_name";
		exit 1;
	fi
	
	#Take the first chunk if it is not defined
	chunk=${chunk:=1}
	input_id="${chunk}:${chunks}:$format"
	
	#Need to query for inpud_id 
	
	cmd="$ANALYSIS_SCRIPTS/test_RunnableDB $PDB_SCRIPT_ARGS -logic_name $logic_name -input_id $input_id $write"
	echo $cmd
	
	time perl $cmd	
}

#This should only be used to test the parsing of the fasta file
#-write should not be specific 
#Altho this will still write Array and ArrayChip info to the DB
#This will automatically be rolled back if not fully 'IMPORTED'


TestImportArrays(){
	echo "TestImportArrays $*"

	format=
	write=

	#Need to test for jsut one format
	#This makes sure we reset the getopts ind if we have used it previously
	OPTIND=1

	usage='usage: TestImportArrays -f format [ -w(rite) ] [ -h(elp) ]';

	while getopts ":f:wh" opt; do
		case $opt in 
	        f  ) format=$OPTARG ;; 
w  ) write=' -write ';;
h  ) echo $usage; return 0;;
\? ) echo $usage; return 1;;
		esac 
	done

	
	#ValidateVariable $format $ARRAY_FORMATS?
	

	if [[ ! $format ]]; then
		echo 'You must supply an array format(-f) argument e.g. AFFY_UTR, AFFY_ST, ILLUMINA_WG etc'
		return;
	fi


	
	#No write here!
	#We should turn on debugging here to test the output
	#-verbose only works in the test script not in the RunnableDB!
	#Probably best not to over load the Runnable with iterative tests for verbose

	cmd="$ANALYSIS_SCRIPTS/test_RunnableDB $PDB_SCRIPT_ARGS -logic_name Import_${format}_Arrays -input_id ${format}:genome $write"
	echo $cmd

	time perl $cmd

}


CleanImportRules(){
	echo ": CleanImportRules $*"

	sql='delete rc from rule_conditions rc where rc.rule_condition like "Import_%"'
	echo $sql | mysqlpipe
	sql='delete rg from rule_goal rg, analysis a where rg.goal=a.analysis_id and a.logic_name like "Import_%"'
	echo $sql | mysqlpipe
	#Do we need to add input_id_analysis and input_is_type_analysis here?
}

#Do we need ReSubmitAlign?
#This would set batch size to user defined amount
#



#Should this also CleanJobs?
#Can we not do this from the RunnableDB and use the roll_back_ArrayChip method?


RollbackArrays(){
	echo ":: RollbackArrays $*"

	format=
	mode=
	align_types=


	#Need to test for jsut one format
	#This makes sure we reset the getopts ind if we have used it previously
	OPTIND=1

	#We could use the Pipeline stages here Instead? e.g. Import, Align, TranscriptXrefs? 

	usage='usage: RollbackArrays -f format [ -m mode e.g. probe2transcript | ProbeAlign | ProbeTranscriptAlign | probe_feature | probe(default) ] [ -d(force delete) ] [ -m(apping type. default is all) ] -h(elp) ]';

	while getopts ":f:m:dh" opt; do
		case $opt in 
	        f  ) format=$OPTARG ;; 
m  ) mode=" -m $OPTARG " ;;
t  ) align_types="$OPTARG $align_types";;
d  ) force=' -force ' ;;
h  ) echo $usage; return 0;;
\? ) echo $usage; return 1;;
		esac 
	done

	ValidateVariable format VALID_ARRAY_FORMATS
	SetArrayNames $format

	vendor=$(echo $format | sed 's/_.*//')
				
	if [[ ! $vendor ]]; then
		echo "Could not defined vendor for format $format"
		echo "Format must 'VENDOR_FORMAT'"
		exit 1;
	fi


	for name in $ARRAY_NAMES; do
		Execute perl $EFG_SRC/scripts/rollback/rollback_array.pl $DB_SCRIPT_ARGS -s $SPECIES -a $name -v $vendor -c $name $mode $force
	done


}



SubmitAlign(){
	
	echo ":: SubmitAlign $*"

	#This will kick off the rest of the pipeline
	#So if we have cleaned the import job details
	#Then it will try and do the collapse step again.
	#Use CleanImportRules here just to be safe?
	CleanImportRules

	#sql to remove import rules
	#delete rc from rule_conditions rc where rc.rule_condition like "Import_%";
	#delete rg from rule_goal rg, analysis a where rg.goal=a.analysis_id and a.logic_name like "Import_%";

	#CleanArrays here? No need, will fail if they have already been imported 
	#or will be rolled back if not(and fail if associated xrefs found)

		#Need to add DB throttling here too
		#rusage[mem=${musage}:$DBHOST=10:duration=10:decay=0]
	#rusage="mem=${musage}"

		#if [ $RUSAGE_DB_HOST ]; then
		#	rusage="${rusage}:"
		#fi



	Execute perl $PIPELINE_SCRIPTS/rulemanager.pl $PDB_SCRIPT_ARGS

	#This is giving weird batching behaviour where the stdout files
	#for some jobs list the analysis of another
	#This is due to the BatchQueue config not having logic name config for each format.


	return;

	#We can only run this once, so we need to remove this loop and check conflicting options
	


	align_types=
	formats=
	chunk_arg=

	echo "before getopts $*"


	#If we reource the env whilst already in the env, then this fails to set vars and align_types check fails!?

	#getopts does not run if we resource the environemnt whilst we are still in the environment!!!

	while getopts ":t:f:c:" opt; do
		case $opt in 
			t  ) echo "found $opt with $OPTARG"; align_types="$align_types $OPTARG" ;; #align_types=$(GetOptArgArray $*) ;;
	        f  ) formats=$OPTARG ;;
            c  ) chunk_arg=$OPTARG ;;
			? ) echo 'usage: CreateAlignIDs -t "Align types" [-f "array formats"] [-c chunks]'; exit 1;;
		esac 
	done


	echo "vars are types: $align_types formats: $formats chunks: $chunk_arg"

	CheckVariables align_types
	SetArrayFormats $formats

	echo ":: Align types: $align_types"



    #Could do with knowing probe size for these calculations
    #Can we pass this as an arg?

	#We should delete the input IDs here before importing!
	

	echo "Submitting ProbeAlign jobs"

	for format in $ARRAY_FORMATS; do
	
		#Need to set this in the runnable as we have to submit in one go.
		#Same with TARGET_SEQS?
		NR_FASTA="$WORK_DIR/arrays_nr.${format}.fasta"

		for align_type in $align_types; do

			#This is slightly redundant as we're doing it for each loop.
			ValidateVariable align_type VALID_ALIGN_TYPES
			seqs_var="${align_type}SEQS" 
			CheckVariables $seqs_var
			
			if [ $align_type = GENOMIC ]; then
				logic_name="Submit_${format}_ProbeAlign"
				
			elif [ $align_type = TRANSCRIPT ]; then
				logic_name="Submit_${format}_ProbeTranscriptAlign"
			fi
		
					
			#We could tune the batch size here
			#But we're already tuning the chunks to ~30mins
			#So we know that this should complete within about 3 hours
			#as we have a batch size of 5.
			#This doesn't always seem to work
			#As I have seen 126 running jobs from 500 chunks when batch
			#size is 5.  This should have just been 100, so batch size has been reduced
			#somehow

			#Execute perl $PIPELINE_SCRIPTS/rulemanager.pl $PDB_SCRIPT_ARGS -analysis $logic_name
			#Do we need input_id_type...will this not be picked up?
			#-input_id_type PROBE_CHUNK

         	#append to array list for species???
        	#echo $ARRAY >> ${WORK_DIR}/array.list
		done
	done

	echo "Now do ArrayAlignReport when finished"
}



################################################################################
# Func      : AlignWait
# Desc      : Accumulator step to wait for all ProbeAlign jobs
# Arg[1..n] : None
# Return    : None 
# Exception : None
################################################################################


AlignWait(){
	echo ":: AlignWait $*"
	#add opts here for -force_accumulators and -once?

	cmdline="perl $PIPELINE_SCRIPTS/rulemanager.pl $PDB_SCRIPT_ARGS -force_accumulators -analysis Align_Wait"

	#echo ":: ImportWait $cmdline"

	Execute $cmdline
	
	#echo "Can we cat the AFFY and AFFY_ST arrays_nr files?"
	#As these should use the same rules?  This would mean running with one logic name for both
	#Just keep separate for now

	#Now cat the array names files output by ImportArrays
	#This is to enable any ProbeFeature/IDXref/UnmappedObject deletes

	echo ": Finished waiting...ready for RunTranscriptXrefs?"
}


ProbeAlignReport(){
	echo ":: ProbeAlignReport $*"
			
	formats=
	custom_names=
	number=

	usage='usage: ProbeAlignReport [ -f "array_format"]* [ -a "array_name" ]* [ -n number of most mapped trancripts to list defualt 20 ][-h help]'

	OPTIND=1

	while getopts ":t:f:a:h" opt; do
		case $opt in 
	        f  ) formats="$OPTARG $formats";;
		    a  ) custom_names="$OPTARG $custom_names";;
		    n  ) number=$OPTARG;;
		    \? ) echo $usage; exit 1;;
			h  ) echo $usage; return 0;;
		esac 
	done		

	SetArrayFormats $formats
	number=${number:=20}


 	for format in $ARRAY_FORMATS; do
		SetArrayNames $format
		
		#This sets custom_names to $ARRAY_NAMES also!
		#names=${custom_names:=$ARRAY_NAMES}
		if [ $custom_names ]; then
			names=$custom_names
		else
			names=$ARRAY_NAMES
		fi

	
		#Validate custom names(sub this)?
		valid_names=
		
		for name in $names; do
			
			valid=$(echo $ARRAY_NAMES | grep -E "^$name | $name | $name$|^$name$")
			
			if [[ $valid ]]; then
				valid_names="'$name' $valid_names"
			fi
		done
		
		valid_names=$(echo $valid_names | sed 's/ /, /g')
				
		if [[ ! $valid_names ]]; then
			echo ": Skipping $format ProbeAlignReport, no valid custom names found: $names($ARRAY_NAMES)"
		else
			report_file=${WORK_DIR}/ProbeAlign.$format.report
			echo ": Generating $format($valid_names) ProbeAlignReport: $report_file"
			BackUpFile $report_file

	
			
			#Total Probes Mapped and Features
			#Is this correct?
			#Why are we getting >300 freqs here?
			#These should only be more than 100 if we get transcript mappings
			#Up to a maximum of 200
			query="SELECT 'Mapped Probes' as ''; SELECT a.name as 'Array Name', an.logic_name as 'Align Analysis', count(distinct pf.probe_id) as 'Total Probes Mapped', count(pf.probe_feature_id) as 'Total Probe Features' from analysis an , array a, array_chip ac, probe p, probe_feature pf where a.array_id=ac.array_id and ac.array_chip_id=p.array_chip_id and p.probe_id=pf.probe_id and pf.analysis_id=an.analysis_id and a.name in($valid_names) group by a.array_id, pf.analysis_id;"
	
			
			#Probe frequencies
			query="${query} SELECT 'Probe Frequencies' as ''; select t.cnt as 'Feature Count', count(t.cnt) as 'Probe Count' from (select count(pf.probe_id) as cnt from probe_feature pf, probe p, array_chip ac, array a WHERE ac.array_id=a.array_id AND ac.array_chip_id=p.array_chip_id AND pf.probe_id=p.probe_id AND a.name in ($valid_names) GROUP by pf.probe_id) as t GROUP by t.cnt order by t.cnt;"
	
		    #Top 20 most abundant probes
			query="${query} select ''; select 'Top $number Most Mapped Probes' as '';"
			query="${query} select count(pf.probe_id) as FeatureCount, pf.probe_id as ProbeID from probe_feature pf, probe p, array_chip ac, array a WHERE ac.array_id=a.array_id AND ac.array_chip_id=p.array_chip_id AND pf.probe_id=p.probe_id AND a.name in ($valid_names) GROUP by pf.probe_id, a.array_id order by FeatureCount desc limit $number;"
	
				#Total unmapped probes
			query="${query} select ''; select count(uo.ensembl_id) as 'Total Unmapped Probes' from  probe p, array_chip ac, array a, unmapped_object uo, unmapped_reason ur WHERE ac.array_id=a.array_id AND ac.array_chip_id=p.array_chip_id AND uo.ensembl_id=p.probe_id AND a.name in ($valid_names) and uo.unmapped_reason_id=ur.unmapped_reason_id and ur.summary_description='Unmapped probe GROUP by p.probe_id';"


			#Top 20 most unmapped probe
			query="${query} select ''; select 'Top $number Most Unmapped Probes' as '';"
			#Should group here but this seems to hide some records
			query="${query} select p.probe_id as 'Probe ID', ur.full_description as 'Reason' from probe p, array_chip ac, array a, unmapped_object uo, unmapped_reason ur WHERE ac.array_id=a.array_id AND ac.array_chip_id=p.array_chip_id AND uo.ensembl_object_type='Probe' AND uo.ensembl_id=p.probe_id AND a.name in ($valid_names) and uo.unmapped_reason_id=ur.unmapped_reason_id and ur.summary_description='Promiscuous probe' order by ur.full_description desc limit $number;"
			echo $query | mysql $DB_MYSQL_ARGS > $report_file
		fi
	done
}
	




##Recover AWOL jobs?
#Lists disinct states of AWOL jobs which have had there AWOL status deleted?
#select distinct status from job_status where job_id not in(select j.job_id from job_status js, job j where j.job_id=js.job_id and is_current='y');

#Not sure about this as there is no SUCCESFUL state?
#Are jobs simply removed from table when successful?


#Change this to RunProbe2Transcript
#Then have RunTranscriptXrefs as a wrapper which would also call TranscriptXrefsWait, tail -n 25 each log and GenerateCDF etc...

#Simply sum the files used and multiply by factor


SetMusage(){
	array_file=
	factor=
	type=
	base_mem=

	OPTIND=1
	
	usage='write usage!'

	while getopts ":a:f:t:b:h" opt; do
		case $opt in 
	        a  ) array_file=$OPTARG;;
		    f  ) factor=$OPTARG;;
		    t  ) type=$OPTARG;;
			b  ) base_mem=$OPTARG;;
		    \? ) echo $usage; exit 1;;
			h  ) echo $usage; return 0;;
		esac 
	done		

	#Do we need to check factor is num?
	CheckVariables array_file factor

	seq_file_type=
	total_size=0

	if [ $type ]; then
		seq_file_name="${type}SEQS"
		CheckVariables $seq_file_name
		seq_file=$(eval "echo \$$seq_file_name")
		seq_file_type=seq
		seq_size=
	fi


	for file_type in $seq_file_type array; do
		file=$(eval "echo \$${file_type}_file")
		size=$(GetFileSize $file)
		total_size=$(($size + $total_size))
	done


	#This should work but doesn't seem to!!

	#if [ $type ]; then
	#	seq_file_name="${type}SEQS"
	#	CheckVariables $seq_file_name
	#	file=$(eval "echo \$$seq_file_name")
	#	echo "seq_file is $seq_file"
	#	total_size=$(GetFileSize $file)
		#upweight the array_file by cubing?
		#Is this valid for all the memusage calcs?
		#No this brings values closer!??
		#How is this possible if AFFY_ST is larger, growth should be exponential???

		#total_size=$(echo "($total_size * $total_size) / $WEIGHT" | bc) 
		#total_size=$(echo "($total_size * $total_size * $total_size) / $WEIGHT" | bc) 
		#total_size=$(echo "($total_size * $total_size * $total_size * $total_size * $total_size) / $WEIGHT" | bc) 
	#fi

	#eval ${file_type}_size=$(GetFileSize $file)
	#size=$(GetFileSize $array_file)
	#total_size=$(($size + $total_size))
	
	
		
	musage=$(echo "($total_size * $factor)/1000" | bc)
    musage=$(echo $musage | sed 's/\..*//')

	#Add base usage
	if [ $base_mem ]; then
		musage=$(($musage + $base_mem))
	fi


}



#HUMAN_AFFY_ST_probe2transcript select[mem>23256] rusage[mem=23256]



#Simply make a working dir

MakeHugememWorkdir(){
	echo ": MakeHugememWorkdir $*"

	format=$1
	shift

 	hugemem_workdir="${HUGEMEM_HOME}/$DB_NAME"
	echo "making $hugemem_workdir"

	if [ ! -d $hugemem_workdir ]; then
		Execute mkdir -p $hugemem_workdir
	fi

	#Create links here to err log out files?
	#Can we do this before they have been created?
	#This is step specific so leave to callers

}


RunTranscriptXrefs(){
    #$1 is update flag, i.e. this will delete all xrefs and object_xrefs
    #Speed up update process
	echo ":: RunTranscriptXrefs $*"
	

	formats=
	delete=
	array_names=
	skip_backup=
	usage='usage: RunTranscriptXrefs [ -f "array_format" default=$ARRAY_FORMATS ]+ [ -d(elete xrefs) -s(kip backup) -h(elp) ]'
	
	OPTIND=1
	
	while getopts ":df:a:sh" opt; do
		case $opt in 
	        f  ) formats="$OPTARG $formats";;
		    a  ) array_names="$OPTARG $array_names";;
		    d  ) delete="--delete";;
	        s  ) skip_backup=1;;
		    \? ) echo $usage; exit 1;;
			h  ) echo $usage; return 0;;
		esac 
	done		

	
	echo ": PROBE2TRANSCRIPT_BSUB_CMD is $PROBE2TRANSCRIPT_BSUB_CMD"
	echo ": PROBE2TRANSCRIPT_PARAMS are $PROBE2TRANSCRIPT_PARAMS"
	



	SetArrayFormats $formats
    ChangeMakeDir $WORK_DIR
	#Move to wrapper when we create one
	if [ ! $skip_backup ]; then
		BackUpTables xrefs orig
	fi
	
  
	#Need to optionally take array names, as we may not have done the Alignment with this DB
	#i.e. if we are just rerunning the Xrefs due to a new genebuild
    #if [[ ! -e ${WORK_DIR}/arrays.list ]]; then echo " no arrays.list file error here!!!"; return 101; fi



	

    #check meta_coord table
    echo "Checking meta_coord table"
	#We need to restrict this to the specific analyses
    coord_id=$(QueryVal OUT "select coord_system_id from probe_feature pf, seq_region sr where pf.seq_region_id=sr.seq_region_id limit 1");
    meta_ids=$(QueryVal OUT "select coord_system_id from meta_coord where table_name='probe_feature'")

    
    for id in $meta_ids; do

       	if [ $id -eq $coord_id ]; then
       		#echo "Found correct level entry for oligo_feature coord_system"
       		cnt=`expr $cnt + 1`
       	fi
    done

    if [ $cnt -eq 0 ]; then
		echo "Cannot find valid meta_coord entry for probe_feature, did you migrate the tables from a different DB?"
		exit 0;
       	#echo "Generating meta_coord entry for oligo_feature with coord_id $coord_id"
       	#echo "insert into meta_coord values ('probe_feature', $coord_id, 25)" | mysql $DB_MYSQL_ARGS
    fi

	#No healtcheck done anymore as we just delete anyway and fail if checks fail
	#??? The healthcheck was to prevent deleting xrefs if we have associated arrays?

	
	for format in $ARRAY_FORMATS; do
		#Check we have an array.names file
		#To see whether we have imported
		NAMES_FILE="$WORK_DIR/arrays.${format}.names"
		
		if [[ ! -f $NAMES_FILE ]]; then
			echo "Skipping $format, NAMES_FILE not found: $NAMES_FILE"
		else

			echo "Running $format TranscriptXrefs"
			SetArrayNames $format


			if [ $array_names ]; then
				names=$array_names
			else
				names=$ARRAY_NAMES
			fi

			if [[ ! $names ]]; then
				echo "WARNING: No $format ARRAY_NAMES found in: $NAMES_FILE"
				echo "Maybe you want to SetArrayNames $format ...?"
				exit 1;
			else

				workdir


				#Validate names
				valid_names=

				for name in $names; do
				
					valid=$(echo $ARRAY_NAMES | grep -E "^$name | $name | $name$|^$name$")
						
					if [[ $valid ]]; then
						valid_names="$name $valid_names"
					fi
				done
				

				if [[ ! $valid_names ]]; then
					echo "WARNING: No valid custom ARRAY_NAMES: $names($ARRAY_NAMES)"
					echo "A typos in your custom array names or maybe?"
					echo "Or maybe you want to resitrcit the ARRAY_FORMATS by using: SetArrayFormats format?"
					exit 1;
				fi

				
                #This depends on format being VENDOR_FORMAT or just VENDOR
				vendor=$(echo $format | sed 's/_.*//')
				
				if [[ ! $vendor ]]; then
					echo "Could not defined vendor for format $format"
					echo "Format must 'VENDOR_FORMAT'"
					exit 1;
				fi

				#Here we need options to pass params in case format isn't supported by probe2transcript #Also need to calculate memusage by counting nr_fasta #What if not present
					        #echo "bsub cmd is $BSUB_CMD"
		        #Check here to see if queue is hugemem and exit if EFG_DATA does not begin with $HOME?
		        #Or just warn anyway
			    BSUB_CMD=

				if [ -n "$PROBE2TRANSCRIPT_BSUB_CMD" ]; then
					file_name="${format}_probe2transcript"
					job_name="${SPECIES_COMMON}_${file_name}"
					#set musage
 					SetMusage -a "$WORK_DIR/arrays_nr.${format}.fasta" -f $XREF_MEM_FACTOR -t TRANSCRIPT -b $XREF_MEM_BASE
					musage_k=$(echo "$musage * 1000" | bc)
				
					work_dir=

					if [ $musage -gt $MAX_HUGE_MEM ]; then
						echo "Expected memory usage exceeds that available on hugemem queue: $musage. Modify rsuage generation?"
						exit 1;
					elif [ $musage -gt $MAX_NORMAL_MEM ]; then
						queue=hugemem


						#Is this a pipeline method?
						#Do we have to copy anything back afterwards?
						MakeHugememWorkdir $format
						work_dir=$hugemem_workdir

						#Now create links to files which will be created
						link_names="${file_name}.err ${file_name}.out ${DB_NAME}_${format}_probe2transcript.log ${DB_NAME}_${format}_probe2transcript.out"
						
						for link_name in $link_names; do
							
							if [ ! -L $link_name ]; then
								Execute ln -s "${work_dir}/${link_name}" $link_name
							fi

						done
					else
						queue=long
						work_dir=$WORK_DIR
					fi

					#BSUB_CMD=$(eval "echo $PROBE2TRANSCRIPT_BSUB_CMD")
					#Need double quotes for -R?!
					
					BSUB_CMD="$PROBE2TRANSCRIPT_BSUB_CMD -q $queue -R\"select[mem>$musage] rusage[mem=$musage]\" -M $musage_k -e ${work_dir}/${file_name}.err -o ${work_dir}/${file_name}.out -J $job_name"
				fi

				echo $BSUB_CMD
		        #Pipe this through bash to avoid weird problems with -R parameter causing truncation
				echo "$BSUB_CMD  time perl ${EFG_SRC}/scripts/array_mapping/probe2transcript.pl --species $SPECIES --transcript_dbname $DNADB_NAME --transcript_host $DNADB_HOST --transcript_port $DNADB_PORT --transcript_user $DNADB_USER --xref_host $DB_HOST --xref_dbname $DB_NAME --xref_user $DB_USER --xref_pass $DB_PASS $PROBE2TRANSCRIPT_PARAMS $delete $MULTI_SPECIES -vendor $vendor -format $format -arrays $valid_names" | /usr/local/bin/bash
		
				if [ -n "$BSUB_CMD" ]; then
					#bjobs -w -J $job_name
					echo "To monitor probe2transcript job: bjobs -lJ $job_name"

				else
					echo "ps -ef | grep 'probe2transcript' to find out if it is still running"
					echo "Alternatively tail -f $WORK_DIR/$DBNAME_\$format_probe2transcript.log"
				fi
		
			fi
			fi
		done
	
		#echo "Remember to run TranscriptXrefsWait, GenerateCDF"
}
	

XrefCheck(){
	CheckVariables WORK_DIR
	echo "Running XrefCheck"
	
	cd ${SRC}/ensj-healthcheck

	$JAVA_HOME/java -classpath	"lib/ensj-healthcheck.jar:lib/mysql-connector-java-3.0.15-ga-bin.jar" org.ensembl.healthcheck.TextTestRunner -config ${WORK_DIR}/database.properties -d $ODBNAME OligoXrefs

	#catch error

	cd $WORK_DIR
	
	echo "Generating status information"
	mysql $DB_MYSQL_ARGS < ${SRC}/ensj-core/scripts/oligo_report.sql > status.out

}


GetArrayId(){
	echo $(QueryVal select oligo_array_id from oligo_array where name =\"$1\")
}

GetExternalDBID(){
	CheckVariables 1
	
       	#Check array is in external_db
	return=0
	name=$(echo $1 | sed 's/_probe.*//')
	name=$(echo $name | sed 's/*\///')


	#echo "Need to implement oligo_array lookup";
	#exit;

	#this needs cahnging to validate agains the oligo_array table first 
	#before querying the external_db table, exiting if not present

       	db_id=$(QueryVal select external_db_id from external_db where db_name=\"AFFY_${name}\")
		    
	if [ ! $db_id ]
	then
	    return=102
       	    name=$(echo $name | sed 's/-/_/')
       	    db_id=$(QueryVal select external_db_id from external_db where db_name=\"AFFY_${name}\")
	fi

	if [ ! $db_id ]
	then
       	    name=$(echo $name | sed 's/_/-/')
       	    db_id=$(QueryVal select external_db_id from external_db where db_name=\"AFFY_${name}\")
	fi


	if [ ! $db_id ]
	then
	    echo "No db_id found for $1"
	    return=101
	fi

	echo $db_id
	return $return
}

AddExternalDB(){
	CheckVariables 1	

	echo "Adding AFFY_${1} to external_db"
      	db_id=$(QueryVal select external_db_id from external_db where external_db_id\<3200 and external_db_id\>3000 order by external_db_id desc limit 1)
	db_id=`expr $db_id + 1`


	query="insert into external_db values($db_id, \"AFFY_${1}\", 1, \"XREF\", 1, 0, 1, \"Affymx Microarray ${1}\")"

	echo "query is $query"

	echo $query | mysql $DB_MYSQL_ARGS

	#catch error?
	#sanity check, query again?
}




CleanPipelineAnalyses(){
    echo "Removing non-essential pipeline analyses"
    

    query="delete from analysis where logic_name in ('CollapseAffy', 'SubmitAlignAffy', 'Collapse_Wait', 'SubmitAffyCollapse');"
    echo $query | mysql $DB_MYSQL_ARGS


}



Update(){
    #$1 is override for automation
    #Obviously this should not be done on the target/genebuilders/staging DB

    CheckVariables WORK_DIR TDBNAME

    echo "Updating $ODBNAME"

    BackUpTables xrefs pre_update

    #remove all other xrefs object_xrefs and AFFY external_dbs
    #sanity check
    #check for arrays in array.list already present in oligo_feature
    #also check for xrefs


    ## Get all array names
    array_names=$(QueryVal OUT select name from oligo_array);

    

  
    echo "Getting target dbids"
    SetMYSQL_ARGS TARGET
    oa_id=$(QueryVal OUT select oligo_array_id from oligo_array order by oligo_array_id desc limit 1)
    p_id=$(QueryVal OUT select oligo_probe_id from oligo_probe order by oligo_probe_id desc limit 1)
    pf_id=$(QueryVal OUT select oligo_feature_id from oligo_feature order by oligo_feature_id desc limit 1)
    x_id=$(QueryVal OUT select xref_id from xref order by xref_id desc limit 1)
    ox_id=$(QueryVal OUT select object_xref_id from object_xref order by object_xref_id desc limit 1)
    anal_id=$(QueryVal OUT select analysis_id from analysis where logic_name=\"AlignAffy\")

    #Need to insert analysis here if not present
    if [ ! $anal_id ]
    then
	echo "Adding analysis to $TDBNAME"
	query="insert into analysis values('', '', 'AlignAffy', 'NULL', 'NULL', 'NULL', 'exonerate', '0.9.0', 'exonerate-0.9.0', 'NULL', 'AlignAffyProbes', 'NULL', 'NULL', 'NULL')"
	echo $query | mysql $MYSQL_ARGS
	anal_id=$(QueryVal OUT select analysis_id from analysis where logic_name=\"AlignAffy\")
    fi


    echo "Got last ids: array $oa_id, probe $p_id, feature $pf_id, xref $x_id, ox $ox_id"


	#there is a potential here to have duplicate ids as they might over lap?
	#unlikely as will always be dealing with an update of the last version's table
	#tricky....grrrr
	#for update we don't really want any other entries than the ones we're dealing with
	#they should be in the target DB

    query1=""
    query2=""

    if [[ ! -e ${WORK_DIR}/arrays.list ]]
    then
	echo "Cannot find arrays.list file"
	return 101;
    fi


    echo "Validating Arrays: $array_names"
    
    for name in $array_names
    do
	#check is in arrays.list? 
	#do we need this here, 


   	db_id=$(GetExternalDBID $name)
	error=$?

	
	if [ $error -eq 102 ]
 	then
	    echo "external_db db_name does not match oligo_array affy name: $name"
	    echo "Please amend external_db.txt"
	elif [ $error -eq 101 ]
	then
	    echo "$name this 101 should be trapped, go ed dbID $db_id"
	    return 101
	fi

        #update xrefs first external_db_ids first
       	#should really only do this if different, but do it anyway
       	query1="${query1} update xref x, external_db edb set x.external_db_id=$db_id where x.external_db_id=edb.external_db_id and edb.db_name=\"AFFY_${name}\";"
       	#update external_db
       	query2="${query2} update external_db set external_db_id=$db_id where db_name=\"AFFY_${name}\";"

       	#Sanity check, is oligo_array already present?
       	db_id=$(GetArrayId $name)
       	
       	if [ $db_id ]
       	then
       	    echo "Oligo array already present in $TDBNAME"
       	    echo "Check for previous mappings/xrefs"
       	    return 101;
       	fi

    done


    #remove all other xrefs object_xrefs and AFFY external_dbs
    #array.list
    #db_id=3200
    #while read name
    #do
#	tmp=$(GetExternalDBID $name)
#	if [[ $tmp -lt $db_id ]]; then db_id=$tmp; fi#

#    done < ${WORK_DIR}/array.list

    #Get first ODB ids and calculate update value

    echo "Fetching table ids"

    o_x_id=$(QueryVal OUT select xref_id from xref order by xref_id limit 1)
    o_ox_id=$(QueryVal OUT select object_xref_id from object_xref order by object_xref_id limit 1)
    x_id=$(PrependPlus $(expr $x_id - $o_x_id + 1))
    ox_id=$(PrependPlus $(expr $ox_id - $o_ox_id + 1))
    oanal_id=$(QueryVal OUT select analysis_id from analysis where logic_name=\"AlignAffy\")



    if [[ $anal_id != $oanal_id ]]
    then
	query1="update oligo_feature set analysis_id=$anal_id; $query1"
    fi

    query1="${query1}${query2}" 
    query2="update xref set xref_id=xref_id${x_id};"
    query3="update object_xref set xref_id=xref_id${x_id};"
    query4="update object_xref set object_xref_id=object_xref_id${ox_id};" 

    echo "Updating with oligo_feature analysis_id, xref external_db_ids and external_db external_db_ids"
    #Update external_db_ids
    echo $query1 | mysql $DB_MYSQL_ARGS


    echo "Updating xref and object_xref table ids"
    #Update xref and object_xref ids
    for query in  "$query2" "$query3" "$query4"
    do
	if [[ $query = *[+-]* ]]
        then
	    echo $query | mysql $DB_MYSQL_ARGS
	fi
    done
		
	#catch error?

    

	#Only update if target DB already has oligo data
	if [ $oa_id ]
	then
	    echo "Getting out dbids"
	    o_oa_id=$(QueryVal OUT select oligo_array_id from oligo_array order by oligo_array_id limit 1)
	    o_p_id=$(QueryVal OUT select oligo_probe_id from oligo_probe order by oligo_probe_id limit 1)
	    o_pf_id=$(QueryVal OUT select oligo_feature_id from oligo_feature order by oligo_feature_id limit 1)

	    #echo "o_oa_id $o_oa_id, o_p_id $o_p_id, o_pf_id $o_pf_id" 

	    oa_id=$(PrependPlus $(expr $oa_id - $o_oa_id + 1))
	    p_id=$(PrependPlus $(expr $p_id - $o_p_id + 1))
	    pf_id=$(PrependPlus $(expr $pf_id - $o_pf_id + 1))

	    #echo "Update vals oa_id = $oa_id, p_id = $p_id, pf_id = $pf_id"

	    if [ $oa_id ]
	    then
		echo "Updating oligo_array_id with $oa_id"
		echo "update oligo_array set oligo_array_id=oligo_array_id${oa_id}" | mysql $MYSQL_ARGS 
		echo "update oligo_probe set oligo_array_id=oligo_array_id${oa_id}" | mysql $MYSQL_ARGS
	    fi
	
	    if [ $p_id ]
	    then
		echo "Updating oligo_probe_id with $p_id"
		echo "update oligo_probe set oligo_probe_id=oligo_probe_id${p_id}" | mysql $MYSQL_ARGS 
		echo "update oligo_feature set oligo_probe_id=oligo_probe_id${p_id}" | mysql $MYSQL_ARGS 
	    fi
       
	    if [ $pf_id ]
	    then
		echo "Updating oligo_feature_id with $pf_id"
		echo "update oligo_feature set oligo_feature_id=oligo_feature_id${pf_id}" | mysql $MYSQL_ARGS 
	    fi
	
	fi
		
	#also need to handle dumping external_db entry and loading into new DB

	#UpdateSeqRegions

	BackUpTables xrefs updated
	BackUpTables arrays updated

	echo "Ready to patch and MigrateOligos"


}

ListTranscriptXrefsByDBID(){
	#$PROBESET=$1
	CheckVariables 1

	#2nd arg as TARGET|OUT

	query="select e.db_name, x.dbprimary_acc, x.info_text, ts.stable_id, t.seq_region_id, t.seq_region_start, t.seq_region_end, t.seq_region_strand, gs.stable_id, g.status, g.description from external_db e, xref x, object_xref ox, transcript t, transcript_stable_id ts, gene g, gene_stable_id gs where e.external_db_id>3000 and e.external_db_id <3200 and e.external_db_id=x.external_db_id and x.xref_id=ox.xref_id and ox.ensembl_id =40651 and ts.transcript_id=ox.ensembl_id and t.transcript_id=ox.ensembl_id and t.gene_id=g.gene_id and g.gene_id=gs.gene_id";

	echo $query | mysql $MYSQL_ARGS

}

ListUniqueTranscriptXrefsByDBID(){
	#$PROBESET=$1
	CheckVariables 1

	#2nd arg as TARGET|OUT

	query="select x.dbprimary_acc, x.info_text, ts.stable_id, t.seq_region_id, t.seq_region_start, t.seq_region_end, t.seq_region_strand, gs.stable_id, g.status, g.description from xref x, object_xref ox, transcript t, transcript_stable_id ts, gene g, gene_stable_id gs where x.external_db_id>3000 and x.external_db_id <3200 and x.xref_id=ox.xref_id and ox.ensembl_id =40651 and ts.transcript_id=ox.ensembl_id and t.transcript_id=ox.ensembl_id and t.gene_id=g.gene_id and g.gene_id=gs.gene_id group by x.dbprimary_acc";

	echo $query | mysql $MYSQL_ARGS

}

ListProbeSetXrefs(){
	#$PROBESET=$1
	CheckVariables 1

	#2nd arg as TARGET|OUT

	query="select e.db_name, x.dbprimary_acc, x.description, x.info_type, x.info_text, ox.ensembl_id, ts.stable_id, t.seq_region_id, t.seq_region_start, t.seq_region_end, t.seq_region_strand, gs.stable_id from xref x, object_xref ox, external_db e, transcript_stable_id ts, transcript t, gene_stable_id gs where x.xref_id=ox.xref_id and e.external_db_id=x.external_db_id and ox.ensembl_id=ts.transcript_id and ox.ensembl_id=t.transcript_id and t.gene_id=gs.gene_id and x.dbprimary_acc='$1' order by e.db_name, t.seq_region_id, t.seq_region_start;"

	echo $query | mysql $MYSQL_ARGS

}


ListProbeSetMappings(){
	#$PROBESET=$1
	CheckVariables 1

	#2nd arg as TARGET|OUT

	query="select oa.name, p.probeset, p.name, pf.seq_region_start, pf.seq_region_end, pf.seq_region_strand, pf.mismatches from oligo_probe p, oligo_feature pf, oligo_array oa where oa.oligo_array_id=p.oligo_array_id and p.probeset='$1' and p.oligo_probe_id = pf.oligo_probe_id order by oa.name, pf.seq_region_id, pf.seq_region_start;"

	echo $query | mysql $MYSQL_ARGS

}


ContinueOverride(){
    CheckVariables 1

    if [ ! $2 ]
    then
		AskQuestion "$1"

		if [[ $REPLY != [yY]* ]]
		then
			echo "Exiting"
			exit
		fi
    else
		echo "Auto Continue"
    fi

}

MigrateOligos(){
    CheckVariables VERSION

    #add args to only migrate xrefs or oligos?

    pversion=${pversion:=$(expr $VERSION + 1)}

    #echo "Need to implement version lookup here"
    #exit;

    
    #sanity check
    SetMYSQL_ARGS OUT

    echo "Patching to version $pversion"

    echo  ${SRC}/ensembl/misc-scripts/schema_patch.pl --host $ODBHOST --port $ODBPORT \
        --user ensadmin --pass ensembl --pattern $ODBNAME --schema $pversion --interactive=0\
        --logfile ${WORK_DIR}/schema_patch.${pversion}.log

    ${SRC}/ensembl/misc-scripts/schema_patch.pl --host $ODBHOST --port $ODBPORT \
        --user ensadmin --pass ensembl --pattern $ODBNAME --schema $pversion  --interactive=0\
        --logfile ${WORK_DIR}/schema_patch.${pversion}.log


    BackUpTables xrefs patched
    BackUpTables arrays patched

    echo "Loading into new DB here"

    SetMYSQL_ARGS TARGET
    echo "Loading oligo_array"
    mysql $MYSQL_ARGS < ${WORK_DIR}/backup/oligo_array.txtpatched
    echo "Loading oligo_probe"
    mysql $MYSQL_ARGS < ${WORK_DIR}/backup/oligo_probe.txtpatched
    echo "Loading oligo_feature"
    mysql $MYSQL_ARGS < ${WORK_DIR}/backup/oligo_feature.txtpatched
    echo "Loading xref"
    mysql $MYSQL_ARGS < ${WORK_DIR}/backup/xref.txtpatched
    echo "Loading object_xref"
    mysql $MYSQL_ARGS < ${WORK_DIR}/backup/object_xref.txtpatched

    echo "Finished migrating"

    echo "do some counts here for QC"

}


PrependPlus(){
    CheckVariables 1
    num=$1

    if [[ $num -eq 0 ]]
    then
	num= 
    elif [[ $num != -[0-9]* ]]
    then
	num="+${num}"
    fi

    echo $num
}



UpdateTDBSeqRegion(){
    CheckVariables TDBNAME 1 2 3

    #1 array_name
    #2 is current seq_region_id
    #3 is new seq_region_id

    SetMYSQL_ARGS TARGET

    echo "Updating $1 $2 to $3"

    query="update oligo_feature of, oligo_probe op, oligo_array oa set of.seq_region_id=$3 where of.seq_region_id=$2 and of.oligo_probe_id=op.oligo_probe_id and op.oligo_array_id=oa.oligo_array_id and oa.name=\"$1\""

    echo "Updating seq_region_ids with"
    echo $query
    
    echo $query | mysql $MYSQL_ARGS

    #safety
    SetMYSQL_ARGS OUT
}



UpdateSeqRegions(){
    CheckVariables TDBNAME

    SetMYSQL_ARGS OUT

    #This only works for the first cs_id of the first feature returneed
    #This should be chromosome but not guaranteed :(

    #need to dump and compare??
    #should not have different cs_ids within the same assembly as this will affect other ancilliary DBs!!


    #get coord system id
    ocs_id=$(QueryVal select seq_region_id from oligo_feature limit 1)
    osr_ids=($(QueryVal select seq_region_id from seq_region where coord_system_id=$ocs_id order by name))
    osr_names=($(QueryVal select name from seq_region where coord_system_id=2 order by name))

    echo "of cs id is $ocs_id"


    SetMYSQL_ARGS TARGET
    tsr_ids=($(QueryVal select seq_region_id from seq_region where coord_system_id=$ocs_id order by name))
    tsr_names=($(QueryVal select name from seq_region where coord_system_id=2 order by name))

    SetMYSQL_ARGS OUT

    echo "Updating seq_region_ids $osr_ids"


    for i in ${#osr_ids[*]}
    do

	echo "doing expr of $i - 1"
	i=`expr $i - 1`
	found=0

	for p in ${#tsr_names[*]}
	do

	    if [[ ${osr_names[$i]} == ${tsr_names[$p]} ]]
	    then 
		found=1
		
		if [[ ${osr_ids[$i]} != ${tsr_ids[$p]} ]]
		then
		    echo "seq_region_ids for ${osr_names[$i]} do not match...updating in $ODBNAME"
		    
		    #Need to sanity check here that there aren't overlaps between seq_region_ids
		    test=$(QueryVal select oligo_feature_id from oligo_feature where seq_region_id=${tsr_ids[$p]} limit 1)

		    if [ $test ]
		    then
				echo "Found overlapping seq_region_ids between DBs, need to implement temp seq_region_ids"
				exit
		    fi		

		    query="update oligo_feature set seq_region_id=${tsr_ids[$p]} where seq_region_id=${tsr_ids[$p]}"

		fi
	    fi
	done

	if [[ $found == 0 ]]
	then
	    echo "Could not find seq_region_id for ${osr_names[$i]} in $TDBNAME"
 	    return 101
	fi
    done

    BackUpTables arrays seq_reg

}

DumpXrefFiles(){
    CheckVariables 1

    
    if [[ $1 == TARGET ]]
    then
	args="-h $TDBHOST -u ensro -d $TDBNAME -P $TDBPORT"
    elif [[ $1 == OUT ]]
    then
	args="-h $ODBHOST -u ensro -d $ODBNAME -P $ODBPORT"
    else
	echo "Need to specifiy OUT or TARGET db"
	exit
    fi

    ChangeMakeDir $WORK_DIR/xref_dumps

    echo "Dumping XREF files from $args"

    while read f
    do 
	${SRC}/ensembl-personal/npj/oligo_mapping/scripts/dump_array_xrefs.pl -a $f -o xref_dumps $args 
    done < $WORK_DIR/array.list

}




CompareStagingMirrorDBs(){
    SVER=$1
  
    CheckVariables SVER

    echo "Comparing assembly/genebuild infro between staging and live servers"

    MVER=`expr $SVER - 1`


    STAGING="-hens-staging -uensro"
    ENSDB="-hensembldb.ensembl.org -uanonymous"
    CNT=0

    MYSQL_ARGS=$STAGING
    SDBS=$(QueryVal show databases like \"%core_${SVER}_%\")

    for dbname in $SDBS
    do
	CNT=`expr $CNT + 1`

	if [ $CNT -gt 2 ]
	then
      	    MYSQL_ARGS="$STAGING $dbname"
	    GBUILD[$CNT]=$(QueryVal select meta_value from meta where meta_key=\"genebuild.version\")
	    ASSEMBLY[$CNT]=$(QueryVal select meta_value from meta where meta_key=\"assembly.name\")
	fi
    done

    CNT=0
    MYSQL_ARGS=$ENSDB


    OFILE="${AFFY_HOME}/assembly_build_comparison.${SVER}_${MVER}"

    if [ -f $OFILE ]
    then
	rm -f $OFILE
    fi


    for dbname in $SDBS
    do
	CNT=`expr $CNT + 1`

	if [ $CNT -gt 2 ]
	then
	    edbname=$(echo $dbname | sed s"/_core_${SVER}_[0-9]*[a-z]*/_core_${MVER}/")   
      	    MYSQL_ARGS="$ENSDB"
	    edbname=$(QueryVal show databases like \"${edbname}%\");

	    if [[ $edbname ]]
	    then
		OUTLINE=""
		edbname=$(echo $edbname | sed 's/.*)//')
		MYSQL_ARGS="$ENSDB $edbname"
		EGBUILD=$(QueryVal select meta_value from meta where meta_key=\"genebuild.version\")
		EASSEMBLY=$(QueryVal select meta_value from meta where meta_key=\"assembly.name\")
	      
		#compare dbnames here to get genebuild difference
		EGB=$(echo $edbname | sed s'/.*_core_[0-9]*_//')
		SGB=$(echo $dbname | sed s'/.*_core_[0-9]*_//')

		if [[ ${ASSEMBLY[$CNT]} != $EASSEMBLY ]]
 		then
		    OUTLINE="Mapping ({$EASSEMBLY} > ${ASSEMBLY[$CNT]})  &  "
		fi


		if [[ ${GBUILD[$CNT]} != $EGBUILD ]]
 		then
       		    OUTLINE="$OUTLINE    Xrefing (${EGBUILD} > ${GBUILD[$CNT]})"
		elif [[ $EGB != $SGB ]]
		then
		    OUTLINE="$OUTLINE    Xrefing (Patched set $EGB > $SGB contact genebuilder?)"
		elif [[ $OUTFILE ]]
		then
		    OUTLINE="$OUTLINE  WARNING: SAME GENEBUILD FOR NEW ASSEMBLY"
		fi

	  
		if [[ $OUTLINE ]]
		then
		    OUTLINE="$dbname requires:    ${OUTLINE}"
		    echo $OUTLINE
		    echo $OUTLINE >> $OFILE
		fi


	    else
		echo "New DB $dbname!!!"
	    fi
	fi
    done

    #get all names from staging matching core_SVER
    #for each add element to assembly and genebuild arrays
    
    #for each one, sed for core_MVER
    #then test assembly and genebuild vs array values
    

}
