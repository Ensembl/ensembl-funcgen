#!/usr/local/bin/bash

#export trap_exit=1

#Also change so we don't need absolute path as we can source from the same directory
. $EFG_SRC/scripts/environments/pipeline.env

echo ':::: Welcome to the eFG array mapping environment'

### DONE
# DONE Error catch properly > Execute
# DONE implement 'Execute' for critical commands, see func.sh and affy.env Execute
# DONE Add full setup for each rule in config. Each array in ImportArrays, Each format in Probe(Transcript)Align
# DONE DumpSoftMaskedSeq should handle striping on GENOMICSEQS dir.
# DONE Collapses each array format separately merging to one NR_FASTA. Different ARRAY_DESIGN should be run separately.
# DONE enable multiple arrays.list files for each array format
# DONE implement ARRAY_ENV to reduce the amount of vars set e.g. do not extend PERL5LIB ?
# DONE Enable parallelised ImportArrays using dynamic bsub config
# DONE Base ImportArrays bsub -R config on size of files in each format group
# DONE Move All set up to init sub, clean all vars when sourcing, then call init after species_VERSION.arrays has finished set up.  This will stop vars being inherited from previous envs.
# DONE Change ARRAY_DESIGN check to use VALID_DESIGNS array, can we grep this?
# DONE Ask if we want to dump GENOMICSEQS/TRANSCRIPTSEQS if not set. Also test files
# DONE Change BackUpFile to use back up dir if set.
# DONE Change OUTPUT_DIR to WORK_DIR and genera
# DONE Source .pipeline  which sources .efg
# DONE CleanProbAlignOutput
# DONE Clean Unmapped objects! in CleanProbeAlignOutput
# DONE CleanImportArraysOutput

### TO DO LIST ###
# Add exit code handling, 101 for miscleaneous.
# Update ArrayAlignReport to use oligo_array entries
# add ARRAY_FORMAT to arrays.list file and BuildFasta method, so custom list works with multiple formats
# distribute files across work and scratch, soft link from work1
# Set status IMPORTED status for arrays
# Catch ImportArrays $? error and also archive old output files, so whe can test for the rpesenc of new output files when cat'ing
# Add ISKIPLIST/REGEX/FIELD to config to enable skipping of certain non-experimental probes
# Move all interactive stuff to Init to enable full automation of RunMapping
# Set vars for all interactive questions to allow full non-interactive automation, need to Execute here to error catch
# Make _InitEnv take opts passed from instance.env?
# Make sure .efg stuff does not overlap, make .efg source from funcs.sh 
# Move all vars to separate file and just have defaults in here.
# Make batch size dynamic
# Autogenerate BatchQueue config and insert using sed?
# Create CreateAlignIDs RunnableDB? And runnable DBs for all array wide clean steps?
# Remove species common and put all arrays in subdir (ARRAYS) and use latin name?
# Check if we are handling haplotype regions in dump, did I fix the memory problem?
# Carry out appropriate steps given ARRAY_FORMAT? Or dump this and just use MAPPING_TYPES?
# Use DNADB_NAME schema_build to guess transcript dump file 
# Use the db fasta cache?
# Test DB connections in Init
# Grep for mysql reserved characters in DBNAME in Init
# Use default seq file based one validated build?
# Need to reset jobs when we CleanProbeAlignOutput

# Issue, we cannot re-do transcript mapping without reconstituting the nr fasta file with dbIDs
# Cannot assume we will always have the original file, so we need to alter ImportArrays to run with the original sources files, but simply use the DB to retrieve dbIDs, rather than storing the probes.  We could extend this to allow incremental updates of new arrays rather than having to clear the DB and re import everything.



#DOCS
#Need to define format and array specific config in Analysis/Config/ImportArrays
#ARRAY_PARAMS needs to match the content of the file, not the name of the file as this can differ.
#Need to set up format dirs in species home reflecting these array formats
# add doc to the top of this env about dir setup, and required code bases e.g. analysis, pipeline, core, efg
#Add docs to the VALID vars, about what you would need to do to add an an extra valid var, and a summary description of how they are used

### Initialise/Reset Variables ###
#all vars which may be inherited from a previous environment
#All these need to be exported to persist for access by the funcs

export MAPPING_TYPES=
#This is just to restrict the type of arrays we map
#Default is set in _InitEnv, which is all.
#Are we going to have problems with probes exists across DESIGNS?
#i.e. remapping one design will affect another?
#e.g. AFFY_EXPRESSION, NIMBLEGEN_TILING, ILLUMINA_?
#Need to think more about exactly what we want to do with this var
export ARRAY_DESIGNS=


#This is to restrict the particular arrays we map
#We will definitely encounter problems here,as we will be silently remapping probes from 
#other arrays, which may invalidate previous transcript mappings
#We will also have the problem of the array name not matching the file name
#Isn't this what arrays.list is for?
#We should rename this to arrays.files and then generate another file called arrays.names?
#Or we could just query the DB?
export ARRAYS=


## Vars which are exported from the methods
# These can be set in species_VERSION.arrays or passed to method
#Need to change all these to pick up defaults

export DEFAULT_ARRAY_FORMATS=
#do not change this one as this prevent inheritance from previous env
#this is required as we may want to test to see if we have set the ARRAY_FORMATS
#explicitly rather than just using the defaults
export ARRAY_FORMATS=


### Clean only args!
# Never set here!
export GENOMICSEQS=
export TRANSCRIPTSEQS= 

### Define constants ###

export VALID_ARRAY_DESIGNS='AFFY_EXPRESSION NIMBLEGEN_TILING ILLUMINA'
export VALID_ARRAY_FORMATS='AFFY AFFY_ST ILLUMINA'
export VALID_SEQ_TYPES='GENOMICSEQS TRANSCRIPTSEQS'
export VALID_MAPPING_TYPES='GENOMIC TRANSCRIPT'

#hugemem actually has 192GB, but we've set it to 185 for safety
export MAX_HUGE_MEM=185000
export MAX_NORMAL_MEM=15800
#This is the multiplier by which the actual physicial size of the fasta file is multipled 
#to get the the memory usage figure 
export MEM_MULTIPLIER=2




export DATA_HOME=/lustre/work1/ensembl/nj1/array_mapping
export HUGEMEM_HOME=/nfs/acari/nj1/array_mapping

#This is for location of compiled binaries
#Specifically for exonerate
#Will default to arch of machine running env
#Move this to pipeline.env?
export ARCH=



#### Some convinient aliases
#Some also set in pipeline.env
alias arrayshome='cd $ARRAYS_HOME' 
#alias exit="ding_bell"



################################################################################
# Func      : _InitEnv(
# Desc      : Sets up all config defined variables. Should only be called from
#             instance config file (species_VERSION.arrays) e.g. mouse_51.arrays 
# Args [n]  : 
# Return    : none
# Exception : 
################################################################################

_InitEnv(){
	echo ":: Setting config for $SPECIES array mapping"

	#Set all generic pipeline stuff e.g. DB vars, PATH/PERL5LIB etc
	_InitPipelineEnv

	#Mandatory vars, Do we really need to check ARRAY_DESIGN?
	#Can we not run with multipel ARRAY_DESIGNS at once?
	#Is this sensible?
	CheckVariables ARRAY_DESIGN

	#Validate array design here

	if [ $warn ]
	then
		echo 'NOTE: NIMBLEGEN_TILING will never go through ImportArrays or ProbeTranscriptAlign steps?'
	fi

	ValidateVariable ARRAY_DESIGN VALID_ARRAY_DESIGNS
	export MAPPING_TYPES=${MAPPING_TYPES:=$VALID_MAPPING_TYPES}
	

	#Let's trim this and move the host and working dir to the window title if we can

		export PS1="\
\[\033[31m\]\
arrays@\h ${DB_NAME}>\
\[\033[0m\]"

	#This is just used to see whether we are already in the environment and just reset some other vars
	#export ARRAY_ENV=$DB_NAME

		#Move to pipeline.env?
	export ARCH=${ARCH:=`arch`}


	#Can we set these so they are dynamic dependent on the value of $DATA_HOME? 
	#i.e. to facilitate use of $HOME for hugemem I/O
	#Contains only array format data dirs or the GENOMIC or TRANSCRIPTSEQS dirs
	#These should also match config in Analysis/Config/ImportArrays
	export ARRAYS_HOME=${DATA_HOME}/${SPECIES_COMMON}
	export WORK_DIR=${DATA_HOME}/${DB_NAME}
	export BACKUP_DIR=${WORK_DIR}/backup
	export PIPELINE_OUT=${WORK_DIR}/align_out

	#Check some dirs are present
	CheckDirs $ARRAYS_HOME

	#Create db output dir if not present
	MakeDirs $WORK_DIR

	## Test and dump seq files ##
	## Also for TRANSCRIPT MAPPING_TYPE do extra DNADB checks
	## Import transcripts as coord_system if not present

	#Actually version is not right test for transcript set
	#Validate version
	#version_db=$(echo $DB_NAME | grep /[a-z]+_funcgen_${VERSION
	#check standard location and match version

	#Can we add /data/blastdb paths to here, no standard naming :( !!!

	if [ $warn ]
	then
		echo "NOTE:: Separate these into a separate sub to avoid having lengthy seq_dumping happening in the init func?"
		echo "NOTE:: Or do we want this interactive stuff forced at the start, so we don't nohup this and then have it hanging?"
	fi

	for mapping_type in $MAPPING_TYPES
	do
		ValidateVariable mapping_type MAPPING_TYPES

		#Check/SetUp TRANSCRIPT dependant vars


		#Can remove this as this is generic for both seq dumps

		if [ $mapping_type = TRANSCRIPT ]
		then

			if [ $warn ]
			then
				echo 'NOTE: Also need to check DNADB vars if MAPPING_TYPE is TRANSCRIPT'
			fi

			CheckVariables DNADB_NAME DNADB_HOST BUILD
			
			#This could be the admin user, which we don't want for DNADB
			export DNADB_USER=${DNADB_USER:='ensro'}
			#We're not defaulting host, so let's not default port either
			export DNADB_PORT=${DNADB_PORT:='3306'}
			export DNADB_USER=${DNADB_USER:='ensro'}
   			#export DNADB_OPTIONS=' -
			#export DNADB_MYSQLARGS

			

		fi

	
		seq_type="${mapping_type}SEQS"
		found_file=
		seq_path=$(eval "echo \$$seq_type")

		#Test seq path		

		if [ $seq_path ]
		then

			#echo 'seq path defined'
		
			if [ ! -f "$seq_path" ]
			then
				echo "Could not find $seq_type: $seq_path"
			else
				found_file=1
			fi
		fi
	

		#Use default seq file based on validated build?

		if [ ! $found_file ]
		then
			seqs_dir="${ARRAYS_HOME}/${seq_type}"
				
			if [ -d $seqs_dir ]
			then

				if [ ! $found_file ]
				then

					if [ $seq_type = GENOMICSEQS ]
					then
						file_regex=toplevel_*.fasta

					elif [ $seq_type = TRANSCRIPTSEQS ]
					then
						file_regex=transcripts.*.fasta
					fi


					# Now ask if we want to use any of the files currently present					
					#This will only capture the first line
					#Do we have to read here or just do in loop below
					#files=$(ls $ARRAYS_HOME/$seq_type/$file_regex)
					#This puts all the separate lines into the first array element
					#Would need to cahnge separator
				    #files does not have scope outside of this loop
					#Is this because file is only defined in loop?
					# no file=
					#This is because the piped command runs as a separate process in a subshell and can't modify the 
					#files var in the parent environment, so is always locally scoped
					#ls $ARRAYS_HOME/$seq_type/$file_regex | while read file
					#do
					#	echo "file is $file"
					#	files="$file $file"
					#done
			 
					files=

					for file in $(ls $ARRAYS_HOME/$seq_type/$file_regex)
					do
						files=($files $file)
					done		
								
					num_files=${#files[@]}
		  			i=0
					
					
					if [ $num_files -gt 0 ]
					then
						echo "Would you like to use the following $seq_type file?"
					fi


                    #Clean REPLY before while just incase we have inherited it from a previous question
					REPLY=

					#Need to evaluate vars here with ""'s as might be undef, which would cause error
	
					while [ "$REPLY" != y  ] && [ $i -lt "$num_files" ]
					do
				   		#This causes a hang
						#REPLY=$(AskQuestion "Would you like to use the following $seq_type file? [y|n] $file")
						AskQuestion "${files[$i]} [y|n]"
						
						if [ $REPLY = y ]
						then
							eval $seq_type=${files[$i]}
							echo ":: Using $seq_type file: ${files[$i]}"
							found_file=1
						fi
						
						i=`expr $i + 1`
				    done	
				fi
			fi

			if [ ! $found_file ]
			then
				AskQuestion "No $seq_type file found, would you like to DumpSeq? [y|n]"

				if [ $REPLY = y ]
				then
					DumpSeq $mapping_type
					found_file=1
				fi
			fi	
		fi

		if [ ! $found_file ]
	   	then
	   		echo 'WARNING: Could not set valid $seq_type file, please rectify'
			exit 1;
     	fi
	done


		#No longer doing this as we are going to store transcript mappings as genomic mappings with cigarline!
		### Now import transcripts as coord_system if not already present
		#Need to test coord_system_name and schema_build match

#		if [ $mapping_type = TRANSCRIPT ]
#		then
#
#			#We have to count the number of seq_regions to make sure we haven't just got the
#			#coord_system entry, but we have actually loaded the transcripts as seq_regions
#			#We could validate this against the DNADB
#
#
#
#			transcript_count=$(QueryVal select count(seq_region_id) from seq_region sr, coord_system cs where cs.version=\"$build\" and cs.coord_system_id=sr.coord_system_id and sr.schema_build-\"${VERSION}_${BUILD}\")
#			
#			echo "transcript count is $transcript_count"
#
#			if [ ! $transcript_count ]
#			then
#				echo "Could not find transcript seq_regions with schema_build ${VERSION}_${BUILD}"
#				
#				
#				
#				schema_builds=$(QueryVal select distinct(cs.schema_build) from seq_region sr, coord_system cs where cs.version=\"$build\" and cs.coord_system_id=sr.coord_system_id)
#
#				#hopefully we should have all the schema_builds in one string here, but may need to use read in QueryVals?
#
#				echo "schema_builds is $schema_builds"
#
#
#				if [ $schema_builds ]
#				then
#					AskQuestion "Found comparable transcript seq_regions for $schema_builds. Would you like to use these? [y|n]"
#					if [ $REPLY = n ]
#					then
#						echo 'You have chosen not to use the previously stored transcripts seq_region'
#						schema_builds=
#					fi
#				fi
#
#				if [ ! $schema_builds ]
#				then
#					AskQuestion "Would you like to load a transcript coord_system using the following fasta file? [y|n]
#$TRANSCRIPTSEQS
#"
#					if [ $REPLY = n ]
#					then
#						echo 'WARNING: You cannot map probes to transcripts without loading a valid transcript coord_system'
#						exit 101
#					else
#
#
#
#						echo "Not yet implement loading transcripts"
#
#						#Aren't we going to load IdentityXrefs?
#						#If we are then this is slighlty redundant.
#						#Loading features against transcript allows nice slice based access
#						#Where as IdXref allows the addition of extra annotation
#						#Should we use both here?
#						#We don't currently store IdXrefs, just the annotation
#						#But it would be nice to have access to all this info in one place.
#						#Shouldn't remove probe level Xrefs, so we are faced with redundancy
#						#Or migrating probe_features on transcript to IdXrefs and cleaning
#						#and removing slice based access to the probe features o transcripts
#
#
#					fi
#
#
#				fi
#								
#			fi
#		fi
			#Will there be problems loading the transcript cs from different DBs with the same build
			#We will have to duplicate these, for the seq_region schema_build mapping to work correctly
            #This should happen anyway, or can we rely on defaulting to an old import with the same build(cs.version)
            #As we currently do with other cs's?								

	echo "GENOMICSEQS:      $GENOMICSEQS
TRANSCRIPTSEQS:   $TRANSCRIPTSEQS
"
workdir

}


#Could we move this to pipeline.env and make generic?

DumpSeq(){
	seq_type=$1
	#should take output_file var here?

	CheckVariables seq_type

	#Validate
	ValidateVariable seq_type VALID_MAPPING_TYPES

	#seqs_var="${seq_type}SEQS"
   	dir="${ARRAYS_HOME}/${seq_type}SEQS"
   	
	#Create SEQS dir with large stripe size

	if [ ! -d $dir ]
	then
		MakeDirs $dir
		lfs setstripe $dir 0 -1 -1
	fi
	
	if [ $seq_type = "GENOMIC" ]
	then
		export GENOMICSEQS=${dir}/toplevel_${VERSION}.fasta 
		echo ":: Dumping softmasked genomic sequence: $GENOMICSEQS"

		#This sometimes runs out of memory on head nodes??
		#Always after chr 6 and before chr 4 for human
		#Even tho mem usage has remained ~ 20%???

		#Can't do this if we have note made a full copy of the DB!!!
		bsub -e ${dir}/genomic_seq.err -o ${dir}/genomic_seq.our -R"select[mem>3500] rusage[mem=3500]" -M3500000 perl $ANALYSIS_SCRIPTS/sequence_dump.pl -dbhost $DNADB_HOST -dbuser $DNADB_USER -dbname $DNDB_NAME  -coord_system_name toplevel -mask_repeat Dust  -mask_repeat RepeatMask  -softmask -nonref -filename $GENOMICSEQS 
		echo "Now need to wait for dump to finish"

		#Can we write a func to do this?

	else
		#CheckVariables now done in _InitEnv

		export TRANSCRIPTSEQS=${dir}/transcripts.${VERSION}_${BUILD}.fasta
		echo ":: Dumping transcript sequence: $TRANSCRIPTSEQS"

		#If we are doing a TRANSCRIPT mapping, do we have to define the DNADB if we already have the correct transcript dump?
		#The default DNADB may not be the correct one
		#Need to define DNADB options string if we are doing TRANSCRIPT

		#echo "$EFG_SCRIPTS/export/dump_genes.pl \
		#-dbport $DNADB_PORT\
		#-dbuser $DNADB_USER\
        #-dbhost $DNADB_HOST \
        #-dbname $DNADB_NAME\
        #-cdna \
        #-stable_id \
        #-file $TRANSCRIPTSEQS"

		Execute perl $EFG_SCRIPTS/export/dump_genes.pl \
		-dbport $DNADB_PORT\
		-dbuser $DNADB_USER\
        -dbhost $DNADB_HOST \
        -dbname $DNADB_NAME\
        -cdna \
        -stable_id \
        -file $TRANSCRIPTSEQS
	fi
}



################################################################################
# Func      : RunMapping
# Desc      : Wrapper method to run the whole genomic and transcript array 
#             mapping and xref pipeline 
# Args [1]  : Optional - Custom arrays.list file
# Args [2]  : Optional - ARRAY_FORMATS for custom arrays.list file
# Return    : none 
# Exception : none
################################################################################

RunMapping(){
	#Make this take multiple formats?
	#Is this possible to combine with the file?
	#Are we getting confused with logic names here? ProbeAlign ProbeTranscriptAlign?

	array_file=$1
	shift
	array_formats=$*

	BuildFastas $array_file $array_formats
	
    SetUpDB $array_formats

    echo "Array Mapping Pipeline Params are:"
    echo "Funcgen DB:      $DB_ARGS"
    #echo "Raw file:        $RAW_FASTA"
    #echo "NR file:         $NR_FASTA"
    
	#To catch errors we could do a for method in "ImportArrays CreateAlignIds ..."
	#The catch the rtn value and skip the next methods
	#error output should be from the methods themselves

    ImportArrays $array_formats
    CreateAlignIDs $array_formats
    ProbeAlign
    monitor
    
}



################################################################################
# Func      : BuildFastas
# Desc      : Merges array fasta files in to array format specific files for 
#             ImportArrays.  
# Args [1]  : Optional - Custom arrays.list file
# Args [2]  : Optional - array_format for custom arrays.list file
# Return    : none 
# Exception : Exits if custom file specified but not array format specific
################################################################################

BuildFastas(){
    CheckVariables WORK_DIR

	echo ":::: BuildFastas $* ::::"


	echo "need to GetOpts this"
	
    array_list=$1
	shift
	array_formats=$*
	cnt=0



	#This currently doesn't allow for multiple formats in one method
	#We need to either implement format in list file and change this sub to handle
	#Or we need to run spearately for each format, and add format to ARRAY_FORMATS
	#Do we relaly want to have the default used?

	#We also need to be able to FlushFasta or DeleteFastas to enable regeneration from scratch


	#echo "array_list is $array_list"
	#echo "array_formats $array_formats"
		

	ArchiveFile $WORK_DIR/arrays.list

	if [[ $array_list ]]
    then
		echo ":: Building $array_format fasta from custom array list: $array_list"

		if [ ! -f $array_list ]
		then
			echo ":: Array list file does not exist: $array_list"
			exit 1;
		fi


		CheckVariables array_formats
   		cat $array_list
			
		format_fasta=$WORK_DIR/arrays.${array_formats}.fasta
		BackUpFile $format_fasta
		ChangeDir $WORK_DIR

		#We could do with some healtchecks here
		#AskQuestion if files already present for array_format
		#prompt to clean array and probe tables?
	 		
		while read path
		do
			if [[ -f $path ]]
			then
				
			    cp $path $WORK_DIR
				file=$(GetFilename $path)

				UnzipFasta $file

				if [[ $? -eq 1 ]]
				then
					cnt=`expr $cnt + 1`
					dos2unix $file          
					cat $file >> $format_fasta
					echo $file >> $WORK_DIR/arrays.list
					#Need to clean up fasta here?
				fi
					#else wasn't a fasta file

			else
				echo ":: Exiting: Could not find $path"
                return 1
			fi

		done < $array_list

		#Should copy $array_list to WORK_DIR if it isn't the same
	    	    		
    else # ! $array_list
   		#Execute ls $ARRAYS_HOME
	  	#Execute arrayshome
		#Can't use alias in Execute as scope is lost

		arrayshome

		SetArrayFormats $array_formats

		#CheckDirs $ARRAYS_HOME #Done in SetArrayFormats
		arrayshome

	
		#Can we do a healthcheck to ensure we don't import the same arrays by checking the old array.list first before overwriting?
		#Would need to grep the array.list file for each?
		#Or should we just Ask a question if the file is present?
		#WHich file?
		#We will have to make each ImportArrays job write to a separate file
		#So we need to change both the RAW_FASTA and NR_FASTA dependent on the array_format
		#This cannot be set as an env var unless we do it in series :(
		#This is also true for the ARRAY_FORMAT env var
		#We need to write a config file where the job ID is used to key into a list of variables
		#Basically just the ARRAY_FORMAT, as the RAW and NR_FASTA can be generated from the WORK_DIR and ARRAY_FORMAT
		#We also need to set a global array(space separated string) of array formats, to use in the CreateImportArrayIDs step
		#Will the job know about it's ensembl job ID(not lsf job ID)
		#Can we use LSF_BATCH_INDEX?
		#No we have a chunk id or something like that, whatever IIDREGEXP uses to figure out what chunk to use
		

		#We now need to build individual lists for each ARRAY_FORMAT
		#which translates to every subdir in the species home
		#ignoring the TRANSCRIPTSEQS/GENOMICSEQS dirs

		#Surely we can restrict ls to only list directories?
		#ls -d? only lists current dir as .?


		#Need to account for TRANSCRIPTSEQS here to, and maybe a README
		#Also account for ARRAY_FORMATS here, which may have been defined in the env.

		if [ ! -z "$ARRAY_FORMATS" ]
		then
			echo ":: Building fasta from specified formats: ${ARRAY_FORMATS}"
			list=$ARRAY_FORMATS
		else
			echo ":: Building fasta from default array formats in ${ARRAYS_HOME}"
			list=$(ls)
		fi


		echo "We need to check whether arrays.fasta has been removed"
		#If so backupmv  arrays.list and start a fresh

		for format in $ARRAY_FORMATS
		do
		  echo ":: Building fasta file for $format"

		#We could do with some healthchecks here
		#AskQuestion if files already present
		#prompt to clean array and probe tables?
		#Or will pipeline fail when we see a duplicate array?
		#We have should have an array_format subdir

		  format_home=${ARRAYS_HOME}/${format}
		  format_fasta=$format_home/arrays.${format}.fasta
			
		#BackUp when we are dealing with the WORK_DIR version

		#this is necessary to allow the grep below
		  if [[ ! -f ${format_home}/arrays.list ]]
		  then
			  touch ${format_home}/arrays.list
		  fi

		  ChangeDir $format_home

		  for file in $(ls) 
			do
				
			if [[ $file != arrays.${format}.fasta ]] 	#Skip previous arrays.fasta
			then

				if [ ! $(grep "$file" ${format_home}/arrays.list) ]
				then
					ofile=$file
				
					#ofile may not be a super string match of the unzipped file!?
					UnzipFasta $file
		
					if [[ $? -eq 1 ]]
					then
						echo ":: Found new file: $ofile"
						cnt=`expr $cnt + 1`
						dos2unix $file 
						cat $file >> $format_fasta
						echo $ofile >> $format_home/arrays.list
					fi
				fi
			fi
		  done

		#This copy is always done, can we skip this?
		#Yes, but we need to provide Delete/FlushFasta func
				
		#Now we need to copy it to the WORK_DIR

	  	if [ $warn ]
	   	then
			echo "TO DO: We need to soft link the default fastas to avoid redundant files"
		fi


		#cp $format_fasta $WORK_DIR
		ln -s $format_fasta $WORK_DIR/arrays.${format}.fasta
		


		cat $format_home/arrays.list >> $WORK_DIR/arrays.list
   
		  #Change back to species/array home here
		  arrayshome
		done
	fi


	#Now just print out some details

	if [[ $cnt -eq 0 ]]
	then
	    echo ":: No new fasta files found"
	else
		echo ":: Found $cnt new array fasta files"
	fi

    cnt=($(wc -l $WORK_DIR/arrays.list))
	cnt=${cnt[0]}
    echo ":: $cnt array fasta files cat'd to $WORK_DIR/arrays.list:"
	cat $WORK_DIR/arrays.list
	echo ""
   
	workdir
}



################################################################################
# Func      : UnzipFasta
# Desc      : Unzips fasta file if required and tests for ^> fasta headers
# Args [1]  : Fasta file path
# Return    : Boolean - true is succesful, false if not fasta file
# Exception : None
################################################################################


#We need to remove this or make it aware of files that are already unzipped
#so we don't get duplication

UnzipFasta(){
    file=$1


	#Need to implement IsCompressed here?

    if [[ $file = *zip ]] || [[ $file = *gz ]];
		then
    
		if [[ $file = *zip ]]
		then

			#NEED TO CHANGE THIS SO WE ARE USING UNZIP OR GUNZIP, NOT RENAMING THE SUFFIX!
			unzip $file
			#mv $file $(echo $file | sed 's/zip/gz/')
			#file=$(echo $file | sed 's/zip/gz/')
		else	
			gunzip $file
		fi

		#gunzip $file
       	#file=$(echo $file | sed 's/\.gz//')
    fi

    
	#Rather arbitrary fasta test, should also end in fa or fasta

    count=$(grep "^>" $file | wc -l)

    if [[ $count -gt 0 ]]
		then
		echo ":: Found array fasta file: $file"
		echo ":: Contains $count probes"
		fasta=1
    else
		echo ":: Skipping non-fasta file: $file"
		fasta=0
    fi

    return $fasta
}

################################################################################
# Func      : SetArrayFormats
# Desc      : Set the ARRAY_FORMATS vraible with passed args, or defaults to
#             previously set ARRAY_FORMATS, DEFAULT_ARRAY_FORMATS or the array
#             formats present in $ARRAYS_HOME   
# Args [*]  : optional - list of valid array formats
# Return    : none 
# Exception : none
################################################################################

SetArrayFormats(){
	echo ":: SetArrayFormats $*"
	formats=$*
	#This should overwrite if passed params
	#else use previously set or use defaults

	CheckDirs $ARRAYS_HOME
	cwd=$PWD
	arrayshome
	
	skip=

	if [ -z "$formats" ]
	then
		#echo ":: Using specified formats: ${formats}"


		#Simple return if we already have them set
		if [ ! -z "$ARRAY_FORMATS" ]
		then
			skip=1
		elif [ ! -z "$DEFAULT_ARRAY_FORMATS" ]
		then	
			formats=$DEFAULT_ARRAY_FORMATS
	        echo ":: Using default formats: $formats"
		else
			echo ":: Using default formats in: ${ARRAYS_HOME}"
			formats=$(ls $ARRAYS_HOME)
		fi
	fi



	if [ ! $skip ]
	then

	    #We need to populate this to avoid overwriting/appending to ARRAY_FORMATS
		valid_formats=

		for format in $formats
		  do

		  if [ ! -d $format ]
			  then
			  echo "You have specified an array format which does not exist in your input dir: $PWD/$format"
			  exit 1
		  fi

		  if [[ $format = GENOMICSEQS ]] || [[ $format = TRANSCRIPTSEQS ]]
			  then
			  echo "Skipping sequence directory: $format"
		  else
			  #we need to capture error here and warn about only having valid format dirs in arrayshoms

			  ValidateVariable format VALID_ARRAY_FORMATS
			  valid_formats="$format $valid_formats"
		  fi


		done		 

		export ARRAY_FORMATS=$valid_formats
	fi


	echo ":: ARRAY_FORMATS are: $ARRAY_FORMATS"
	cd $cwd

}


################################################################################
# Func      : SetUpPipeline
# Desc      : Imports pipeline tables, analyses and rules into DB
# Args [1]  : 
# Args [2]  : Optional - ARRAY_FORMAT for custom arrays.list file
# Return    : none 
# Exception : none
################################################################################

SetUpPipeline(){
	SetArrayFormats $*

	#Set up analysis and rule config files
	analysis_conf_file=$WORK_DIR/probe_transcript_analysis.conf
	rules_conf_file=$WORK_DIR/probe_transcript_rules.conf

	BackUpFile $analysis_conf_file
	rm -f $analysis_conf_file
	touch $analysis_conf_file

	BackUpFile $rules_conf_file
	rm -f $rules_conf_file
	touch $rules_conf_file


	#Can't add params to analysis here as we don't have access yet, but we should do that somewhere?
	#Could do it in the Runnable DB, but highly redundant, as this will be tested/overwritten for each job i.e. thousands of times.
	

	#Space separate logic names to allow readability in Analysis::Config modules


	#This is not right?
	#We can map AFFY and AFFY_ST together, as they have the same probe design, i.e. 25 mers
	#Just keep separate for now for simplicity
	#We also don't want ProbeTranscriptAlign for Tiling arrays?
	#We need to this dependent on ARRAY_DESIGN?  This is current AFFY or AFFY_ST
	#or CLASS. TILING/EXPRESSION.
	#This concept is a little weird as it's the application of the array, not the array design itlsef, 
	#as you can use TILING designs for WG expression analysis.  Then the idea of probe sets becomes a little
	#awry.

	if [ $warn ]
	then
		echo 'TO DO: Need to fix analysis and rules set up dependant on array type, i.e. tiling should not be transcript mapped'
		echo 'NOTE: also need to test for transcript coord_system if we are mapping to transcript?'
	fi

	echo ":: Writing analysis and rules config"

    #added null modules to stop warnings on import

    #These will not overwrite current entries in analysis, so we may have some old data listed
	#Need to validate this?





	#Build mutil-condition wait first
	echo "[Import_Wait]" > $rules_conf_file

	for format in $ARRAY_FORMATS
	do
		echo "condition=Import_${format}_Arrays" >> $rules_conf_file
	done


	#Do we not need to add another rule here for the Submit jobs to wait for Import_Wait?
	#Or is this what the Import_Wait accumulator is doing?


	echo "[Import_Wait]
module=Accumulator
input_id_type=ACCUMULATOR
"> $analysis_conf_file


	for format in $ARRAY_FORMATS
	do
		echo "[Import_${format}_Arrays]
module=ImportArrays
input_id_type=PROBE_SET

[Submit_Import_${format}_Arrays]
input_id_type=PROBE_SET

[Submit_${format}_ProbeAlign]
input_id_type=PROBE_CHUNK

[${format}_ProbeAlign]
program=exonerate
program_version=2.2.0
program_file=/lustre/work1/ensembl/gs2/local/${ARCH}/bin/exonerate
module=ProbeAlign
input_id_type=PROBE_CHUNK

[Submit_${format}_ProbeTranscriptAlign]
input_id_type=PROBE_TRANSCRIPT_CHUNK

[${format}_ProbeTranscriptAlign]
program=exonerate
program_version=2.2.0
program_file=/lustre/work1/ensembl/gs2/local/${ARCH}/bin/exonerate
module=ProbeAlign
input_id_type=PROBE_TRANSCRIPT_CHUNK
">> $analysis_conf_file

		echo "
[Import_${format}_Arrays]
condition=Submit_Import_${format}_Arrays

[${format}_ProbeAlign]
condition=Import_Wait
condition=Submit_${format}_ProbeAlign

[${format}_ProbeTranscriptAlign]
condition=Import_Wait
condition=Submit_${format}_ProbeTranscriptAlign
" >> $rules_conf_file

done



	CreatePipelineTables
   


	#Could do with testing for files here
	echo ":: Importing analysis config" 

	#EFG
    Execute perl $PIPELINE_SCRIPTS/analysis_setup.pl $PDB_SCRIPT_ARGS -read -file $WORK_DIR/probe_transcript_analysis.conf 

    echo ":: Importing rules config" 
	#EFG
    Execute perl $PIPELINE_SCRIPTS/rule_setup.pl  $PDB_SCRIPT_ARGS -read -file $WORK_DIR/probe_transcript_rules.conf

	#Could we test output for 'Not storing' This will not detect whether there are other rules in the DB

	if [ $warn ]
	then
		echo "NOTE: Need to clean analysis table? Or is this a setup problem"
		echo "NOTE: check for genomic seq here, are all the chrs present in $GENOMICSEQS"
		#This would requite getting all toplevel (inc non-ref) and greping file
		#Is this another case for PipeLineHelper.pm
	fi
	
    Execute perl $PIPELINE_SCRIPTS/setup_batchqueue_outputdir.pl 
	
	#This will always fail and return 0?!
	#As we don't have any input_id written for the accumulator yet
	#This needs to be done after we have created the input ids for ImportArrays
	CheckPipelineSanity

	echo ""

}






################################################################################
# Func      : ImportArrays
# Desc      : Collapses arrays of related formats in to unique probe records 
#             based on probeset and sequence identity
# Arg[1..n] : optional - ARRAY_FORMATS, space separated array formats to collapse
# Return    : none 
# Exception : Exits if ARRAY_FORMATS not defined
################################################################################


#Need to add no collapse/straight import functionality for nr_arrays i.e. ST arrays

ImportArrays(){
	echo ":::: ImportArrays $* ::::"

	SetArrayFormats $*
	CheckVariables ARRAY_FORMATS ARRAYS_HOME

	echo "Need to check pipeline sanity here?"
	echo "Need to set up separate pipeline DB"
	echo "Should clean up job_status here too?"

	#pipeline_sanity.pl -dbhost $DB_HOST -dbport $DB_PORT -dbuser $DB_USER -dbpass $DB_PASS -dbname $DB_NAME
	#This is currently barfing due to presence of other analyses which do not have modules.

	workdir

	BackUpTables arrays

	#We need to write the config file for each format here.
	cnt=0

	#Create input_ids and config file for ImportArrays
	#Let's have 1:genome etc to maintain some continuity
	#export ARRAY_FORMAT_FILE=$WORK_DIR/ImportArrays.config
	#BackUpFile $ARRAY_FORMAT_FILE
	#rm -f $ARRAY_FORMAT_FILE
	#touch $ARRAY_FORMAT_FILE


	#Need to sub relevant parts of this loop to enable TestImportArray?
	#Would not write or bsub, but then were not testing storage code
	#This loop only works as the rule manager finishes after every submit?
	#Or does it...is this running in series?
	#If so we need to kick off with no -analysis flag 
	#And get it to stop after Import_Wait somehow, so we can create the Align IDs etc
	#Or should we just put this in a RunnableDB and then pipeline the whole thing?

	for format in $ARRAY_FORMATS; do
		logic_name="Import_${format}_Arrays"
		CleanJobs $logic_name
		
	    #Do we want to do this for every run, what if we've had a failure and we're just re-running
	    cmdline="make_input_ids $PDB_SCRIPT_ARGS -single -logic_name Submit_Import_${format}_Arrays -single_name ${format}:genome"
		echo ":: Creating Import_${format}_Arrays input IDs:  $cmdline"
		Execute $cmdline
		
        #We should archive old outputs here so we can check for presence of output after job completion
        #do we need this as we've already built the RAW fasta.
	
	    format_fasta=$ARRAYS_HOME/$format/arrays.${format}.fasta


		#do we really need the genome designation?
		#Can we pass the RAW_FASTA as part of the ID to avoid hardcoding the NR_FASTA file
		#i.e. decouple the dependancy of ImportArrays to this env?
			

		#input_id="${format}:genome"
		
		#echo "$input_id    $format    $format_fasta" >> $ARRAY_FORMAT_FILE

		#datetime=$(date +"%Y-%m-%d %T")
		#analysis_id=$(QueryVal select analysis_id from analysis where logic_name=\"SubmitImport${format}Arrays\")
		#sql="insert into input_id_analysis values('$input_id', 'PROBE_SET', $analysis_id, '$datetime', '', '', 0)"
		#echo "$sql" | mysql $MYSQL_ARGS


		#Now set env vars for dynamic BatchQueue config
		#This should be dependant on size of files in each format
		#Would need to account for soft links
		#But lets keep it simple for now
		#Do we need to define resource string dependent on farm platform?


		#Need to sub this var setting if we are going to enable TestImportArrays AFFY AFFY_ST


		export RAW_FASTA=$WORK_DIR/arrays.${format}.fasta

		#extra brackets to set it as a true array, rather than just a space separated string in the first element
		#We need to handle softlinks here

		if [ ! -f $RAW_FASTA ]
		then
			echo "$RAW_FASTA does not exist, maybe you need to BuildFastas"
			exit 1;
		fi

		fasta_size=($(ls -lk $RAW_FASTA))

		#Set it to the 4th size element, to avoid using array access in expr
		fasta_size=${fasta_size[4]} 
		fasta_size=$(echo "$fasta_size * $MEM_MULTIPLIER" | bc)

		#/ always rounds down so add safety buffer of 500 MB
		musage=`expr $fasta_size / 1000 + 500`

		#Now we have decimals that we can't use as bsub vars so we need to sed?
		#Or can we format the outpuyt using bc?
		musage=$(echo $musage | sed 's/\..*//')
		musage=$(echo  "$musage * 1.2" | bc)
		musage=$(echo $musage | sed 's/\..*//')
		musage_k=$(( $musage * 1000 ))
		#Always base queue on rusage and round up musage if it is too small
		#This is to avoid bsubin to a normal node with an rsusage it can never meet
		#Which would result in the job never starting



		if [ $musage -gt $MAX_HUGE_MEM ]
		then
			echo "Expected memory usage exceeds that available on hugemem queue: $musage. Modify rsuage generation?"
			exit 1;
		elif [ $musage -gt $MAX_NORMAL_MEM ]
 		then
			queue=hugemem
			echo "Need to move things to home dir for hugemem jobs. This will go away when we import ST arrays directly?"
			exit 1;
			

			#Don't really need to change mem value
			#mem=$MIN_HUGE_MEM
		else
			queue=normal
		fi


		#do we need to set some sensible mimimums?
				

		resource="-R 'select[mem>${musage}] rusage[mem=${musage}]' -M $musage_k"
				
		

		#We need to copy input files from lustre to somewhere where hugemem can see it
		#Also change respective vars and handle moving data back to lustre after ImportArrays
		#Or shall we move everything for Import and then move everything back later?
		

		export IMPORT_ARRAYS_QUEUE=$queue
		export IMPORT_ARRAYS_RESOURCE=$resource
	
		echo ":: $format bsub params: -q $IMPORT_ARRAYS_QUEUE $IMPORT_ARRAYS_RESOURCE"
		#We are just running the pipeline through one cycle to submit here
		#This is because we need to create the input_ids for each ProbeAlign step before we carry on
		#We should create another RunnableDB to do this automatically so we can run the pipeline all in one go!
		cmdline="perl $PIPELINE_SCRIPTS/rulemanager.pl $PDB_SCRIPT_ARGS -once -analysis $logic_name -input_id_type PROBE_SET"
		echo ":: Import_${format}_Arrays: $cmdline"
		Execute $cmdline


		#THis is running with jobname like nj_arrays_mus_musculus_funcgen_51_37d:default
		#How do we replace the default with the analysis?
		#Look in rulemanager
		#This should have used the analysis!!


	   	#echo "arrays.env ImportArrays:Need to capture error here for dollar question $?"

	done


	echo "Completed incremental submission of ImportArrays for formats: $ARRAY_FORMATS"


	#ImportWait $ARRAY_FORMATS


	#Removed this now as we are running on separate files

	#Now cat all the files so we just have one nr file for mapping
	#ArchiveFile $NR_FASTA
	#touch $NR_FASTA

	#for format in $ARRAY_FORMATS
	#do

		#maybe sanity check here, does file exist?
		#Will this effect chunking?
		#Does the chunking required IDs to be in order?

	#	cat $WORK_DIR/arrays_nr.${format}.fasta >> $NR_FASTA
	#done


	#Do some more sanity checking here, does number of probes match that in nr fasta?

	
}

################################################################################
# Func      : ImportWait
# Desc      : Accumulator step to wait for all ImportArrays jobs
# Arg[1..n] : None
# Return    : None 
# Exception : None
################################################################################


ImportWait(){
	echo ":: ImportWait $*"
	#add opts here for -force_accumulators and -once?

	cmdline="perl $PIPELINE_SCRIPTS/rulemanager.pl $PDB_SCRIPT_ARGS -force_accumulators -analysis Import_Wait"

	echo ":: ImportWait $cmdline"

	Execute $cmdline
	
	#echo "Can we cat the AFFY and AFFY_ST arrays_nr files?"
	#As these should use the same rules?  This would mean running with one logic name for both
	#Just keep separate for now

	#Now cat the array names files output by ImportArrays
	#This is to enable any ProbeFeature/IDXref/UnmappedObject deletes

	export NAMES_FILE="${WORK_DIR}/arrays.names"

	ArchiveFile NAMES_FILE

	CheckVariables ARRAY_FORMATS

	for format in $ARRAY_FORMATS; do
		format_names="${WORK_DIR}/arrays.names"
		cat $format_names > $NAMES_FILE
	done
		



	#We could validate array names or at least number of arrays here?
	echo "Finished waiting...ready for CreateAlignIds [ formats ]"
}



SetArrayNames(){
	#Remove the echos from this method?
	#echo ": SetArrayNames $*"
	formats=$*


	echo "setting names for formats $formats"

	format_files=

	if [ ! -z "$formats" ]; then

		for format in $formats; do
			format_files="${WORK_DIR}/arrays.${format}.names $format_files"
			echo "building format files $format_files"
		done
	else
		format_files="${WORK_DIR}/arrays.names"
	fi


	ARRAY_NAMES=

	for file in $format_files; do

		echo "reading $file"

		if [ ! -f $file ]; then
			echo "Could not find $file"
			echo "You need to ImportArrays for this file"
		else
			
			while read name; do
				ARRAY_NAMES="$name $ARRAY_NAMES"			
			done < $file
		fi
	done

	#Can't do this as we may want to call this format by format
	#and some may not have names files yet
	#if [ -z $ARRAY_NAMES ]; then
	#	echo "Found no valid array names"
	#	exit 1;
	#fi

	#So must test return value in caller		

}

#getopts doesn't parse arrays of opts
#Multiple switches didn'twork

GetOptArgArray(){

	args=$*
	echo "args are $args"
	echo "optind is $OPTIND"
	
	inarg=

	#for (( i=$OPTIND; $inarg; i++ )); do			
	#	echo "arg is $arg"
				
	#	if [ $inarg ]; then
	
	#		echo "inarg"
			
			#if [[ "${args[$i]}" = -* ]]; then
			#	inarg=0
			#	OPTIND=$i
			#	echo "next arg is at $OPTIND"
			#fi
			
	#	fi
	#done	
}

################################################################################
# Func      : CreateAlignIDs
# Desc      : Creates ProbeAlign or ProbeTranscriptAlign input IDs
# Opt -t    : Mandatory - Mapping types e.g. GENOMIC and/or TRANSCRIPT
# Opt -f    : Optional - Array formats e.g. 'AFFY AFFY_ST', defaults will be used if absent
# Opt -c    : Optional - Chunks, chunks will be calculated if omitted
# Arg[3]    : would need to parse opts
#			  ARRAY_DESIGN????? Would need to create different sets of IDs for different designs
#             This would probably screw up  the chunking based on the IDs, would also need different source file
#             Hence another config file would need to be written. 
# Return    : none 
# Exception : None
################################################################################
	
	
CreateAlignIDs(){
	
	echo ":::: CreateAlignIDs $*"

	mapping_types=
	formats=
	chunk_arg=


	#This makes sure we reset the getopts ind if we have used it previously
	OPTIND=1

	while getopts ":t:f:c:" opt; do
		case $opt in 
			t  ) echo "opt is $opt with arg $OPTARG"; mapping_types="$mapping_types $OPTARG" ;; #mapping_types=$(GetOptArgArray $*) ;;
	        f  ) formats="$formats $OPTARG" ;; 
            c  ) chunk_arg=$OPTARG ;;
		    \? ) echo 'usage: CreateAlignIDs -t mapping_type [-f format] [-c chunk] -t and -f can be speficied more than once'
		    exit 1;;
		esac 
	done

	
    #No shift required as we don't expect any other args
    #shift $(($OPTIND - 1))


	echo "vars are $mapping_types $formats $chunk_arg"
	mapping_types=${mapping_types:='GENOMIC TRANSCRIPT'}
	CheckVariables mapping_types
	SetArrayFormats $formats
	#echo ":: Mapping types: $mapping_types"
	#echo ":: Formats: $ARRAY_FORMATS"


	#We need to back up the input ids before deleting and recreating
	



	for format in $ARRAY_FORMATS; do
		echo "::"

		#Just do the fasta count once if required
		if [ ! $chunks_arg ]; then
			NR_FASTA="$WORK_DIR/arrays_nr.${format}.fasta"
			#num_lines=$(wc -l $NR_FASTA | sed "s| $NR_FASTA||")
			num_probes=`expr $(wc -l $NR_FASTA | sed "s| $NR_FASTA||") / 2`
			echo ":: Found $num_probes $format NR probe sequences"
		fi


		for mapping_type in $mapping_types; do	 
			
			#This is slightly redundant as we're doing it for each loop.
			ValidateVariable mapping_type VALID_MAPPING_TYPES
	
			if [ $mapping_type = GENOMIC ]; then
				id_type='PROBE_CHUNK' 
				logic_name="${format}_ProbeAlign"
			
			elif [ $mapping_type = TRANSCRIPT ]; then
				id_type='PROBE_TRANSCRIPT_CHUNK'
				logic_name="${format}_ProbeTranscriptAlign"
			fi

            #Delete the old input IDs here before importing!?
			CleanJobs $logic_name

            #Calculate the number of chunks
			if [ $chunk_arg ]; then
				chunks=$chunk_arg
			else
                #Tune the chunk numbers wrt array type and mapping type
				
                #This is roughly based on nimblegen probes being 50-60mers
	    	    #And affy probes being roughly 25mers				
				if [[ $ARRAY_DESIGN = NIMBLEGEN_TILING ]]; then
					probes_per_chunk=8000
					#May need to change this
				else
					#probes_per_chunk=8000
					#Takes ~ 7 mins for Genomic ProbeAlign
					#On head node
					#probes_per_chunk=30000
					#With batch size 5, this should take between 2.5-5 hours!!
					#This is taking >3 hours?
					#On a fully loaded node
					#probes_per_chunk=10000
					#Takes >1.5 hours on fully loaded node
					#probes_per_chunk=5000

					probes_per_chunk=3000


				fi


				echo 

				if [[ $logic_name = *ProbeTranscriptAlign ]]; then	
					#Multiply for transcripts, 10 for now
                    probes_per_chunk=`expr $probes_per_chunk \* 10`
				fi
	
				#expr
				#Add 1 to avoid running with 0
				chunks=$((($num_probes / $probes_per_chunk) + 1))	
				
			fi

		
    		#This was used in test methods, but each analysis now potentially has different chunks values
	    	#export chunks
			
			echo ":: Creating $chunks $logic_name IDs with $probes_per_chunk probes/chunk (e.g. 1:$chunks:$format)"	
			#cmd="for \$i(1..$chunks){ print \"insert into input_id_analysis(input_id,analysis_id,input_id_type) values (\\\"\$i:$chunks\\\",$anal_id,'$id_type'); \n\" }"
			anal_id=$(QueryVal PIPELINE "select analysis_id from analysis where logic_name='Submit_${logic_name}'")


			cmd="for \$i(1..$chunks){ print \"insert into input_id_analysis(input_id,analysis_id,input_id_type) values (\\\"\$i:$chunks:$format\\\",$anal_id,\\\"$id_type\\\"); \n\" }"	
			
			#Execute "perl -e '$cmd' > ${WORK_DIR}/probe_input_ids.sql"
			#Cannot get Execute to work for this perl -e
			perl -e "$cmd" > ${WORK_DIR}/probe_input_ids.sql
			#Execute "mysql $PDB_MYSQL_ARGS < ${WORK_DIR}/probe_input_ids.sql"
			mysqlpipe < ${WORK_DIR}/probe_input_ids.sql

			#Catch error here
			#This is currently causing problems as you can't have the same input_id for two different analyses!!!
            #We now need to parse the input_id differently in ProbeAlign, or is this done in ExonerateProbe?

		done
	done
	
	echo ":: Ready for SubmitAlign"

}



#Add verbose option here

TestProbeAlign(){
	echo ":: TestProbeAlign $*"


	#remove -w and just pass on $* after removing other opts

	mapping_type=
	formats=
	chunk=
	write=
	logic_name=
	help=
	usage='usage: TestProbeAlign -t GENOMIC|TRANSCRIPT -f "array_format" -c "chunk" [-w write flag]'

	#This makes sure we reset the getopts ind if we have used it previously
	OPTIND=1

	while getopts ":t:f:c:w" opt; do
		case $opt in 
			t  ) mapping_type=$OPTARG ;;
	        f  ) format=$OPTARG ;;
            c  ) chunk=$OPTARG ;;
			w  ) write=' -write ' ;;
			h  ) echo $usage; help=1 ;;
		    \? ) echo $usage; help=1 ;;
		esac 
	done

	if [ ! $help ]; then

		CheckVariables format mapping_type

		ValidateVariable format VALID_ARRAY_FORMATS
		ValidateVariable mapping_type VALID_MAPPING_TYPES
	

	#Check nr fasta file
		CheckFile "${WORK_DIR}/arrays_nr.${format}.fasta"


	#Set the logic name
		if [[ $mapping_type = "GENOMIC" ]]; then
			CheckVariables GENOMICSEQS
			logic_name="${format}_ProbeAlign";
		else
			CheckVariables TRANSCRIPTSEQS
			logic_name="${format}_ProbeTranscriptAlign";	
		fi

	#Get the chunk count for this analysis
		chunks=$(QueryVal PIPELINE "select count(*) from input_id_analysis i, analysis a where a.logic_name='Submit_${logic_name}' and a.analysis_id=i.analysis_id")
		
		if [ $chunks = 0 ]; then
			echo "Could not find any input_ids for analysis $logic_name";
			exit 1;
		fi

	#Take the first chunk if it is not defined
		chunk=${chunk:=1}
		input_id="${chunk}:${chunks}:$format"
		
	#Need to query for inpud_id 
		
		cmd="$ANALYSIS_SCRIPTS/test_RunnableDB $PDB_SCRIPT_ARGS -logic_name $logic_name -input_id $input_id $write"
		echo $cmd
		
		time perl $cmd
	fi
	
}

#This should only be used to test the parsing of the fasta file
#-write should not be specific 
#Altho this will still write Array and ArrayChip info to the DB
#This will automatically be rolled back if not fully 'IMPORTED'

TestImportArrays(){
	echo "TestImportArrays $*"

	#Need to test for jsut one format

	ValidateVariable 

	format=$1
	
	
	#No write here!
	#We should turn on debugging here to test the output
	#-verbose only works in the test script not in the RunnableDB!
	#Probably best not to over load the Runnable with iterative tests for verbose

	cmd="$ANALYSIS_SCRIPTS/test_RunnableDB $PDB_SCRIPT_ARGS -logic_name Import_${format}_Arrays -input_id ${format}:genome"
	echo $cmd

	time perl $cmd

}


CleanImportRules(){
	echo ":: CleanImportRules $*"

	sql='delete rc from rule_conditions rc where rc.rule_condition like "Import_%"'
	echo $sql | mysqlpipe
	sql='delete rg from rule_goal rg, analysis a where rg.goal=a.analysis_id and a.logic_name like "Import_%"'
	echo $sql | mysqlpipe
	#Do we need to add input_id_analysis and input_is_type_analysis here?
}

#Do we need ReSubmitAlign?
#This would set batch size to user defined amount
#



#Should this also CleanJobs?
#Can we not do this from the RunnableDB and use the roll_back_ArrayChip method?

CleanImportArraysOutput(){
	echo ":: CleanImportArraysOutput $*"

	SetArrayFormats $*

	#Not sensible to do this on an individual array basis as we collapse
	#them all together otherwise we can get duplicate probe records

	for format in $ARRAY_FORMATS; do

		SetArrayNames $format

		#We need to delete probes, array_chips..and arrays?

	done
}


#This is really only the Unmapped objects from the ProbeAlign or ProbeTranscriptAlign
#Not probe2transcript
#Remove array_names options from here
#As we would need to ProbeAlign at least the whole format fasta?
#Most of this will be done automatically by ImportArrays
#ImportArrays will not however remove any IDXrefs
#Which would be orphaned by deleting the underlying probes.

CleanProbeAlignOutput(){
	echo ":: CleanProbeAlignOutput $*"

	#Currently we can't do this on a per array basis as the environment doesn't necessarily 
	#know the exact names of each array being mapping
	#We need to ouput these from the Import step into array format files and then cat to give arrays.names

	#This will also delete any unmapped object from the previous probe2transcript
	#Do we want to BackUpTables here?


	#We should extend this to probe features

	formats=
	array_names=
	mapping_types=

	usage='usage: CleanUnmappedObject -t GENOMIC|TRANSCRIPT [ -f "array_format" | -a "array_name"]*'
	
	#This makes sure we reset the getopts ind if we have used it previously
	OPTIND=1

	while getopts ":t:f:a:" opt; do
		case $opt in 
	        f  ) formats="$OPTARG $formats";;
		    a  ) array_names="$OPTARG $array_names";;
		    t  ) mapping_types="$OPTARG $mapping_types";;
		    \? ) echo $usage; exit 1;;
		esac 
	done


	#we need to catch if array_names and formats are both defined here
	#if [[ $formats -a $array_names ]]; then
	#	echo $usage;
	#	exit 1;
	#fi

	#We're going to have to filter on the array names for all formats so we
	#we should remove above test as there may be two arrays with the same name but different formats?		

	SetArrayFormats $formats
	#This was failing if we used $formats as this is redefined in SetArrayFormats
	#SetArrayNames $ARRAY_FORMATS

	#validate array_names here again all ARRAY_NAMES

	mapping_types=${mapping_types:='GENOMIC TRANSCRIPT'}
	
	
	for mapping_type in $mapping_types; do 
	
		ValidateVariable mapping_type VALID_MAPPING_TYPES
	
		for format in $ARRAY_FORMATS; do
			
			SetArrayNames $format
			
			names=${array_names:=$ARRAY_NAMES}


			if [ ! $names ]; then
				echo "No array names found for format $format"
			else

			#Need to test for names here

				logic_name="${format}_ProbeAlign"
				
				if [ $mapping_type = TRANSCRIPT ]; then
					logic_name="${format}_ProbeTranscriptAlign"
				fi
				
				anal_id=$(QueryVal OUT "select analysis_id from analysis where logic_name='$logic_name'")
				
	        #There may be no analyses loaded yet if this is a fresh DB
		    #However, there could be orphaned unmapped_object entries due to people 'cleaning' the analysis
		    #table or migrating xref tables
		    #Only likely in core DBs, leave for now
				
				if [ $anal_id ]; then
					valid_names=
					
					for name in $names; do
					
				    #Need to test versus ARRAY_NAMES here
						
					#grep -E '[[:space:]^]$name[[:space:]$]'
						echo "NOTE: We need to fix the array names regex"
						
						valid=$(echo $ARRAY_NAMES | grep "$name")
						
						if [ $valid ]; then
							valid_names="'$name' $valid_names"
						fi
					done
					
					valid_names=$(echo $valid_names | sed 's/ /, /')
					
					
									
					if [ $mapping_type = TRANSCRIPT ]; then
						#UnmappedObjects
						sql="delete uo from unmapped_object uo, probe p, array_chip ac, array a where uo.analysis_id=$anal_id and uo.identifier=p.probe_id and p.array_chip_id=ac.array_chip_id and ac.array_id=a.array_id and uo.type='ProbeTranscriptAlign' and a.name in($valid_names)";
						echo ": Deleting $format $logic_name $valid_names UnmappedObjects"
						Execute echo $sql | mysqlefg



						#Also do IDXrefs
						sql="delete ix, ox from identity_xref ix, object_xref ox, probe p, array_chip ac, array a where ix.analysis_id=$anal_id and ix.object_xref_id=ox.object_xref_id and ox.ensembl_object_type='Probe' and ox.ensembl_id=p.probe_id and p.array_chip_id=ac.array_chip_id and ac.array_id=a.array_id and a.name in($valid_names)";
						
						echo ": Deleting $format $logic_name $valid_names IdentityXrefs"
						#echo $sql
#delete ix, ox from identity_xref ix, object_xref ox, probe p, array_chip ac, array a where ix.analysis_id=17 and ix.object_xref_id=ox.object_xref_id and ox.ensembl_object_type='Probe' and ox.ensembl_id=p.probe_id and p.array_chip_id=ac.array_chip_id and ac.array_id=a.array_id and a.name in('HT_MG-430A')
						Execute echo $sql | mysqlefg
					else
						#UnmappedObjects
						sql="delete uo from unmapped_object uo, probe p, array_chip ac, array a where uo.analysis_id=$anal_id and uo.identifier=p.probe_id and p.array_chip_id=ac.array_chip_id and ac.array_id=a.array_id and uo.type='ProbeAlign' and a.name in($valid_names)";
						echo ": Deleting $format $logic_name $valid_names UnmappedObjects"
						Execute echo $sql | mysqlefg


				        #ProbeFeatures
						sql="delete pf from probe_feature pf, probe p, array_chip ac, array a where pf.probe_id=p.probe_id and p.array_chip_id=ac.array_chip_id and ac.array_id=a.array_id and a.name in($valid_names)";
						
						echo ": Deleting $format $logic_name $valid_names ProbeFeatures"
					#echo $sql
						Execute echo $sql | mysqlefg
					fi
					
				else
					echo "No analysis found for $logic_name. Skipping UnmappedObject, IdentityXref/ProbeFeature delete."
				fi
			fi
		done
	done
}


SubmitAlign(){
	
	echo ":: SubmitAlign $*"

	#This will kick off the rest of the pipeline
	#So if we have cleaned the import job details
	#Then it will try and do the collapse step again.
	#Use CleanImportRules here just to be safe?
	CleanImportRules

	#sql to remove import rules
	#delete rc from rule_conditions rc where rc.rule_condition like "Import_%";
	#delete rg from rule_goal rg, analysis a where rg.goal=a.analysis_id and a.logic_name like "Import_%";

	#No, we don't want to do this as we may be re-starting a pipeline
	#CleanProbeAlignOutput

	Execute perl $PIPELINE_SCRIPTS/rulemanager.pl $PDB_SCRIPT_ARGS

	#This is giving weird batching behaviour where the stdout files
	#for some jobs list the analysis of another
	#This is due to the BatchQueue config not having logic name config for each format.


	return;

	#We can only run this once, so we need to remove this loop and check conflicting options
	


	mapping_types=
	formats=
	chunk_arg=

	echo "before getopts $*"


	#If we reource the env whilst already in the env, then this fails to set vars and mapping_types check fails!?

	#getopts does not run if we resource the environemnt whilst we are still in the environment!!!

	while getopts ":t:f:c:" opt; do
		case $opt in 
			t  ) echo "found $opt with $OPTARG"; mapping_types="$mapping_types $OPTARG" ;; #mapping_types=$(GetOptArgArray $*) ;;
	        f  ) formats=$OPTARG ;;
            c  ) chunk_arg=$OPTARG ;;
			? ) echo 'usage: CreateAlignIDs -t "Mapping types" [-f "array formats"] [-c chunks]'; exit 1;;
		esac 
	done


	echo "vars are types: $mapping_types formats: $formats chunks: $chunk_arg"

	CheckVariables mapping_types
	SetArrayFormats $formats

	echo ":: Mapping types: $mapping_types"



    #Could do with knowing probe size for these calculations
    #Can we pass this as an arg?

	#We should delete the input IDs here before importing!
	

	echo "Submitting ProbeAlign jobs"

	for format in $ARRAY_FORMATS; do
	
		#Need to set this in the runnable as we have to submit in one go.
		#Same with TARGET_SEQS?
		NR_FASTA="$WORK_DIR/arrays_nr.${format}.fasta"

		for mapping_type in $mapping_types; do

			#This is slightly redundant as we're doing it for each loop.
			ValidateVariable mapping_type VALID_MAPPING_TYPES
			seqs_var="${mapping_type}SEQS" 
			CheckVariables $seqs_var
			
			if [ $mapping_type = GENOMIC ]; then
				logic_name="Submit_${format}_ProbeAlign"
				
			elif [ $mapping_type = TRANSCRIPT ]; then
				logic_name="Submit_${format}_ProbeTranscriptAlign"
			fi
		
					
			#We could tune the batch size here
			#But we're already tuning the chunks to ~30mins
			#So we know that this should complete within about 3 hours
			#as we have a batch size of 5.
			#This doesn't always seem to work
			#As I have seen 126 running jobs from 500 chunks when batch
			#size is 5.  This should have just been 100, so batch size has been reduced
			#somehow

			#Execute perl $PIPELINE_SCRIPTS/rulemanager.pl $PDB_SCRIPT_ARGS -analysis $logic_name
			#Do we need input_id_type...will this not be picked up?
			#-input_id_type PROBE_CHUNK

         	#append to array list for species???
        	#echo $ARRAY >> ${WORK_DIR}/array.list
		done
	done

	echo "Now do ArrayAlignReport when finished"
}





ProbeAlignReport(){
	echo ":: ProbeAlignReport $*"
	
		
	formats=
	mapping_types=

	usage='usage: CleanUnmappedObject -t GENOMIC|TRANSCRIPT [ -f "array_format" | -a "array_name"]*'

	while getopts ":t:f:c:w" opt; do
		case $opt in 
	        f  ) formats="$OPTARG $formats";;
		    a  ) array_names="$OPTARG $array_names";;
		    t  ) mapping_types="$OPTARG $mapping_types";;
		    \? ) echo $usage; exit 1;;
		esac 
	done		

	SetArrayFormats $formats




	for type in $mapping_types; do



		BackUpFile ${WORK_DIR}/ProbeAlign.$type.report

		echo "Generating Mapping report for $SPECIES"	

#Need to validate we have the same arrays as listed in arrays.list and that they contain all the arrays from the previous release

	#Count total array probes
	#oa_id=$(GetArrayId $ARRAY)
	#array=$(echo $ARRAY | sed 's/\-/\\-/')

	#How are we going to restrict this to just those array which we are transcript mapping?
	#We need to do this by format or by a list of array names?

	

	query="select a.name, an.logic_name, count(distinct of.oligo_probe_id) as 'Total Probes Mapped', count(of.oligo_feature_id) as 'Total Probe Features' from analysis an , array a, array_chip ac, probe p, probe_feature pf where a.array_id=ac..array_id and ac.array_chip_id=p.array_chip_id and p.probe_id=pf.probe_id and pf.analysis_id=an.analysis_id and a.format='EXPRESSION' group by op.oligo_array_id, of.analysis_id;"
	#This is reporting all probes are mapped?
	

	##List probe feature frequencies
   	#Don't need oligo array here as we can have same probe on different array
	#Also can't print probe name as differes between arrays
	query="${query} select ''; select 'Probe Frequencies';"
 	query="${query} select t.cnt as 'Feature Count', count(t.cnt) as 'Probe Count' from (select count(of.oligo_probe_id) as cnt, of.oligo_probe_id from oligo_feature of, oligo_probe op where of.oligo_probe_id=op.oligo_probe_id group by oligo_probe_id) as t group by 'Feature Count' order by 'Feature Count';"

	#Now we want to list the top most abundant probes?


	echo $query | mysql $DB_MYSQL_ARGS > ${WORK_DIR}/Align.report

}

XrefReport(){
	
	echo "this is here to suppress warning as we have no code in this function"
	#select edb.db_name, count(ox.xref_id) as 'TotalXrefs', count(distinct ox.xref_id) as 'DistinctXrefs', count(distinct ox.ensembl_id) as 'DistinctTranscripts' from external_db edb, xref x, object_xref ox where edb.db_name like "AFFY_%" and edb.external_db_id=x.external_db_id and x.xref_id=ox.xref_id group by edb.db_name;


}





##Recover AWOL jobs?
#Lists disinct states of AWOL jobs which have had there AWOL status deleted?
#select distinct status from job_status where job_id not in(select j.job_id from job_status js, job j where j.job_id=js.job_id and is_current='y');

#Not sure about this as there is no SUCCESFUL state?
#Are jobs simply removed from table when successful?


#change this to test 2nd stage?
Run2ndStage(){
    #$1 is update flag, i.e. this will delete all xrefs and object_xrefs
    #Speed up update process

    clean_xrefs="";

    if [ $1 ]
    then 
	clean_xrefs="--delete";
    fi



    #CheckVariables JAVA_HOME ODBNAME WORK_DIR
    #don't need Java any more and others are checked/created as standard

    #need to do this better, as we need to test for /usr/opt/java142 and bin in PATH
    #Need to check for code too?

    ChangeMakeDir $WORK_DIR
    BackUpTables xrefs orig

    #this can be passed as an arg to probe2transcript.pl
    #CleanAffyXrefs $UPDATE

    if [[ ! -e ${WORK_DIR}/arrays.list ]]; then echo " no arrays.list file error here!!!"; return 101; fi

    #Need to test whether all arrays are complete!
    #Need to cnt/list arrays

    #array.list

	echo "Validating $WORK_DIR/arrays.list"

    while read name
    do

	#Need to get array name from file rather than filename

		#echo "got name $name"
		name=$(echo $name | sed 's/\.zip//')
		name=$(echo $name | sed 's/_probe.*fasta//')
		name=$(echo $name | sed 's/-/_/');

		#echo "got name after sed $name"

		oa_id=$(QueryVal OUT "select oligo_array_id from oligo_array where name='$name'")

	
		if [ ! $oa_id ]
		then
			echo "No oligo_array_id found for $name"
			echo "This needs updating to get the oligo_array names for those in arrays.list"
			return;
		fi

     	#check probeset_size
       	ps_size=$(QueryVal "select probe_setsize from oligo_array where name='$name'")

		#echo "ps_size is $ps_size";

       	if [[ $ps_size = *[[:space:]]0[[:space:]]* ]]
       	then
       		echo "Some of your arrays do not have a probe_setsize"
       		echo "echo \"update oligo_array set probe_setsize=? where name=\\\"$name\\\"\" | mysql \$DB_MYSQL_ARGS"
       		return #this is only exiting from loop!! will fail health check anyway but will miss rest of checks 
       	fi
	
       	#check whether features exist for each array!!!


      
		#echo "Checking for external_db $name"
		#we need to strip path here


		#we need to loop through arrays.list and build the name arrays for probe2Transcript

		db_id=$(GetExternalDBID $name)

		#echo "db_id is $db_id"

		echo "$name has dbID $db_id with probe_setsize $ps_size"

		if [ $? -eq 102 ]
		then
			echo "external_db db_name does not match oligo_array affy name $name"
			echo "Please amend external_db.txt or oligo_array"
			
		elif [ $? -eq 101 ]
		then
			echo "this 101 should be trapped $name"
			return 101
		fi
		
    done < ${WORK_DIR}/arrays.list


	echo "Need to add --arrays option to probe2Transcript here, currently defaulting to all"

    #check meta_coord table
    echo "Checking meta_coord table"
    coord_id=$(QueryVal "select coord_system_id from oligo_feature af, seq_region sr where af.seq_region_id=sr.seq_region_id limit 1");
    meta_ids=$(QueryVal "select coord_system_id from meta_coord where table_name='oligo_feature'")

    #can't get regexs to work properly here?!
    #if [[ $meta_ids != *$coord_ids* ]]

    cnt=0

    for id in $meta_ids
    do
       	if [[ $id -eq $coord_id ]]
       	then
       		echo "Found correct level entry for oligo_feature coord_system"
       		cnt=`expr $cnt + 1`
       	fi
    done

    if [[ $cnt -eq 0 ]]
    then
       	echo "Generating meta_coord entry for oligo_feature with coord_id $coord_id"
       	echo "insert into meta_coord values ('oligo_feature', $coord_id, 25)" | mysql $DB_MYSQL_ARGS
    fi

    #echo "Generating ${WORK_DIR}/database.properties"
    #echo "database $ODBNAME
#host $ODBHOST
#port $ODBPORT
#user ensadmin
#password ensembl
#driver com.mysql.jdbc.Driver" >${WORK_DIR}/database.properties
    #echo "Running healthcheck on $ODBNAME"
    #cd ${SRC}/ensj-healthcheck
    #${JAVA_HOME}/java -classpath "lib/ensj-healthcheck.jar:lib/mysql-connector-java-3.0.15-ga-bin.jar" org.ensembl.healthcheck.TextTestRunner -config ${WORK_DIR}/database.properties -d $ODBNAME OligoProbes2Genome
    #this always returned 0 even if it failed.


    echo ":: Performing Healthcheck ::"

    perl ${SRC}/ensembl/misc-scripts/probe_mapping/probe2transcript.pl --transcript_dbname $DBNAME --transcript_host $DBHOST --transcript_port $DBPORT --transcript_user ensro --xref_host $ODBHOST --xref_dbname $ODBNAME --xref_user ensadmin --xref_pass ensembl $clean_xrefs --health_check


    if [ $? -ne 0 ]
    then
	echo ":: Healthcheck Failed ::"
    else
	echo ":: Healthcheck Passed ::"
	echo "Running 2nd stage in background"
       	cd $WORK_DIR
       	#nohup ${SRC}/ensj-web/bin/run_probeset_2_transcript_mapping.sh -v database.properties >mapper.out &
     
		nohup time perl ${SRC}/ensembl/misc-scripts/probe_mapping/probe2transcript.pl --transcript_dbname $DBNAME --transcript_host $DBHOST --transcript_port $DBPORT --transcript_user ensro --xref_host $ODBHOST --xref_dbname $ODBNAME --xref_user ensadmin --xref_pass ensembl >mapper.out &

#bsub -J ${SPECIES_COMMON}_affy_xrefs -q hugemem -R "select[mem>30000] rusage[mem=30000]" -M 30000000 -o ${DB_HOME}/mapper.out -e ${DB_HOME}/mapper.err time perl ${SRC}/ensembl/misc-scripts/probe_mapping/probe2transcript.pl --transcript_dbname $DBNAME --transcript_host $DBHOST --transcript_port $DBPORT --transcript_user ensro --xref_host $ODBHOST --xref_dbname $ODBNAME --xref_user ensadmin --xref_pass ensembl --utr_length annotated -calculate_utrs


#chromosome:NCBI36:16:28797000:28845000

       	echo "Remember to run XrefCheck"
	echo "ps -ef |grep 'probe2transcript' to find out if it is still running"
    fi

}

XrefCheck(){
	CheckVariables WORK_DIR
	echo "Running XrefCheck"
	
	cd ${SRC}/ensj-healthcheck

	$JAVA_HOME/java -classpath	"lib/ensj-healthcheck.jar:lib/mysql-connector-java-3.0.15-ga-bin.jar" org.ensembl.healthcheck.TextTestRunner -config ${WORK_DIR}/database.properties -d $ODBNAME OligoXrefs

	#catch error

	cd $WORK_DIR
	
	echo "Generating status information"
	mysql $DB_MYSQL_ARGS < ${SRC}/ensj-core/scripts/oligo_report.sql > status.out

}


GetArrayId(){
	echo $(QueryVal select oligo_array_id from oligo_array where name =\"$1\")
}

GetExternalDBID(){
	CheckVariables 1
	
       	#Check array is in external_db
	return=0
	name=$(echo $1 | sed 's/_probe.*//')
	name=$(echo $name | sed 's/*\///')


	#echo "Need to implement oligo_array lookup";
	#exit;

	#this needs cahnging to validate agains the oligo_array table first 
	#before querying the external_db table, exiting if not present

       	db_id=$(QueryVal select external_db_id from external_db where db_name=\"AFFY_${name}\")
		    
	if [ ! $db_id ]
	then
	    return=102
       	    name=$(echo $name | sed 's/-/_/')
       	    db_id=$(QueryVal select external_db_id from external_db where db_name=\"AFFY_${name}\")
	fi

	if [ ! $db_id ]
	then
       	    name=$(echo $name | sed 's/_/-/')
       	    db_id=$(QueryVal select external_db_id from external_db where db_name=\"AFFY_${name}\")
	fi


	if [ ! $db_id ]
	then
	    echo "No db_id found for $1"
	    return=101
	fi

	echo $db_id
	return $return
}

AddExternalDB(){
	CheckVariables 1	

	echo "Adding AFFY_${1} to external_db"
      	db_id=$(QueryVal select external_db_id from external_db where external_db_id\<3200 and external_db_id\>3000 order by external_db_id desc limit 1)
	db_id=`expr $db_id + 1`


	query="insert into external_db values($db_id, \"AFFY_${1}\", 1, \"XREF\", 1, 0, 1, \"Affymx Microarray ${1}\")"

	echo "query is $query"

	echo $query | mysql $DB_MYSQL_ARGS

	#catch error?
	#sanity check, query again?
}




CleanArrayXrefs(){
    echo "This cleans all array xrefs"
    
    #should do check here against TDBNAME
    #truncate if TDBNAME deinfed and not same as ODBNAME

    
    ContinueOverride "Continue" $1

    #if [ $TDBNAME ]
    #then 
#	query="delete from object_xref; delete from xref;"
#    else

	#this does not work if only xref present rather than xref and object_xref

    query="delete ox from xref x, object_xref ox where x.xref_id=ox.xref_id and x.external_db_id<3200 and x.external_db_id>3000;"
#    fi

    #echo "query is $query"

    echo $query | mysql $DB_MYSQL_ARGS

    query="delete from xref where external_db_id<3200 and external_db_id>3000;"
    
    echo $query | mysql $DB_MYSQL_ARGS

}

CleanPipelineAnalyses(){
    echo "Removing non-essential pipeline analyses"
    

    query="delete from analysis where logic_name in ('CollapseAffy', 'SubmitAlignAffy', 'Collapse_Wait', 'SubmitAffyCollapse');"
    echo $query | mysql $DB_MYSQL_ARGS


}



CleanArrayFeatures(){
    CheckVariables ARRAY

    ContinueOverride "Delete all the oligo_feature entries for $ARRAY" $1

    adb_id=$(GetArrayId $ARRAY)

    if [ $adb_id ]
    then
	echo "Deleting features for $ARRAY with dbid $adb_id"
	query="delete of from oligo_feature of, oligo_probe op where of.oligo_probe_id=op.oligo_probe_id and op.oligo_array_id=${adb_id};"
	echo $query | mysql $DB_MYSQL_ARGS
    else
	echo "$ARRAY not found in DB"
    fi

}



Update(){
    #$1 is override for automation
    #Obviously this should not be done on the target/genebuilders/staging DB

    CheckVariables WORK_DIR TDBNAME

    echo "Updating $ODBNAME"

    BackUpTables xrefs pre_update

    #remove all other xrefs object_xrefs and AFFY external_dbs
    #sanity check
    #check for arrays in array.list already present in oligo_feature
    #also check for xrefs


    ## Get all array names
    array_names=$(QueryVal OUT select name from oligo_array);

    

  
    echo "Getting target dbids"
    SetMYSQL_ARGS TARGET
    oa_id=$(QueryVal OUT select oligo_array_id from oligo_array order by oligo_array_id desc limit 1)
    p_id=$(QueryVal OUT select oligo_probe_id from oligo_probe order by oligo_probe_id desc limit 1)
    pf_id=$(QueryVal OUT select oligo_feature_id from oligo_feature order by oligo_feature_id desc limit 1)
    x_id=$(QueryVal OUT select xref_id from xref order by xref_id desc limit 1)
    ox_id=$(QueryVal OUT select object_xref_id from object_xref order by object_xref_id desc limit 1)
    anal_id=$(QueryVal OUT select analysis_id from analysis where logic_name=\"AlignAffy\")

    #Need to insert analysis here if not present
    if [ ! $anal_id ]
    then
	echo "Adding analysis to $TDBNAME"
	query="insert into analysis values('', '', 'AlignAffy', 'NULL', 'NULL', 'NULL', 'exonerate', '0.9.0', 'exonerate-0.9.0', 'NULL', 'AlignAffyProbes', 'NULL', 'NULL', 'NULL')"
	echo $query | mysql $MYSQL_ARGS
	anal_id=$(QueryVal OUT select analysis_id from analysis where logic_name=\"AlignAffy\")
    fi


    echo "Got last ids: array $oa_id, probe $p_id, feature $pf_id, xref $x_id, ox $ox_id"


	#there is a potential here to have duplicate ids as they might over lap?
	#unlikely as will always be dealing with an update of the last version's table
	#tricky....grrrr
	#for update we don't really want any other entries than the ones we're dealing with
	#they should be in the target DB

    query1=""
    query2=""

    if [[ ! -e ${WORK_DIR}/arrays.list ]]
    then
	echo "Cannot find arrays.list file"
	return 101;
    fi


    echo "Validating Arrays: $array_names"
    
    for name in $array_names
    do
	#check is in arrays.list? 
	#do we need this here, 


   	db_id=$(GetExternalDBID $name)
	error=$?

	
	if [ $error -eq 102 ]
 	then
	    echo "external_db db_name does not match oligo_array affy name: $name"
	    echo "Please amend external_db.txt"
	elif [ $error -eq 101 ]
	then
	    echo "$name this 101 should be trapped, go ed dbID $db_id"
	    return 101
	fi

        #update xrefs first external_db_ids first
       	#should really only do this if different, but do it anyway
       	query1="${query1} update xref x, external_db edb set x.external_db_id=$db_id where x.external_db_id=edb.external_db_id and edb.db_name=\"AFFY_${name}\";"
       	#update external_db
       	query2="${query2} update external_db set external_db_id=$db_id where db_name=\"AFFY_${name}\";"

       	#Sanity check, is oligo_array already present?
       	db_id=$(GetArrayId $name)
       	
       	if [ $db_id ]
       	then
       	    echo "Oligo array already present in $TDBNAME"
       	    echo "Check for previous mappings/xrefs"
       	    return 101;
       	fi

    done


    #remove all other xrefs object_xrefs and AFFY external_dbs
    #array.list
    #db_id=3200
    #while read name
    #do
#	tmp=$(GetExternalDBID $name)
#	if [[ $tmp -lt $db_id ]]; then db_id=$tmp; fi#

#    done < ${WORK_DIR}/array.list

    #Get first ODB ids and calculate update value

    echo "Fetching table ids"

    o_x_id=$(QueryVal OUT select xref_id from xref order by xref_id limit 1)
    o_ox_id=$(QueryVal OUT select object_xref_id from object_xref order by object_xref_id limit 1)
    x_id=$(PrependPlus $(expr $x_id - $o_x_id + 1))
    ox_id=$(PrependPlus $(expr $ox_id - $o_ox_id + 1))
    oanal_id=$(QueryVal OUT select analysis_id from analysis where logic_name=\"AlignAffy\")



    if [[ $anal_id != $oanal_id ]]
    then
	query1="update oligo_feature set analysis_id=$anal_id; $query1"
    fi

    query1="${query1}${query2}" 
    query2="update xref set xref_id=xref_id${x_id};"
    query3="update object_xref set xref_id=xref_id${x_id};"
    query4="update object_xref set object_xref_id=object_xref_id${ox_id};" 

    echo "Updating with oligo_feature analysis_id, xref external_db_ids and external_db external_db_ids"
    #Update external_db_ids
    echo $query1 | mysql $DB_MYSQL_ARGS


    echo "Updating xref and object_xref table ids"
    #Update xref and object_xref ids
    for query in  "$query2" "$query3" "$query4"
    do
	if [[ $query = *[+-]* ]]
        then
	    echo $query | mysql $DB_MYSQL_ARGS
	fi
    done
		
	#catch error?

    

	#Only update if target DB already has oligo data
	if [ $oa_id ]
	then
	    echo "Getting out dbids"
	    o_oa_id=$(QueryVal OUT select oligo_array_id from oligo_array order by oligo_array_id limit 1)
	    o_p_id=$(QueryVal OUT select oligo_probe_id from oligo_probe order by oligo_probe_id limit 1)
	    o_pf_id=$(QueryVal OUT select oligo_feature_id from oligo_feature order by oligo_feature_id limit 1)

	    #echo "o_oa_id $o_oa_id, o_p_id $o_p_id, o_pf_id $o_pf_id" 

	    oa_id=$(PrependPlus $(expr $oa_id - $o_oa_id + 1))
	    p_id=$(PrependPlus $(expr $p_id - $o_p_id + 1))
	    pf_id=$(PrependPlus $(expr $pf_id - $o_pf_id + 1))

	    #echo "Update vals oa_id = $oa_id, p_id = $p_id, pf_id = $pf_id"

	    if [ $oa_id ]
	    then
		echo "Updating oligo_array_id with $oa_id"
		echo "update oligo_array set oligo_array_id=oligo_array_id${oa_id}" | mysql $MYSQL_ARGS 
		echo "update oligo_probe set oligo_array_id=oligo_array_id${oa_id}" | mysql $MYSQL_ARGS
	    fi
	
	    if [ $p_id ]
	    then
		echo "Updating oligo_probe_id with $p_id"
		echo "update oligo_probe set oligo_probe_id=oligo_probe_id${p_id}" | mysql $MYSQL_ARGS 
		echo "update oligo_feature set oligo_probe_id=oligo_probe_id${p_id}" | mysql $MYSQL_ARGS 
	    fi
       
	    if [ $pf_id ]
	    then
		echo "Updating oligo_feature_id with $pf_id"
		echo "update oligo_feature set oligo_feature_id=oligo_feature_id${pf_id}" | mysql $MYSQL_ARGS 
	    fi
	
	fi
		
	#also need to handle dumping external_db entry and loading into new DB

	#UpdateSeqRegions

	BackUpTables xrefs updated
	BackUpTables arrays updated

	echo "Ready to patch and MigrateOligos"


}

ListTranscriptXrefsByDBID(){
	#$PROBESET=$1
	CheckVariables 1

	#2nd arg as TARGET|OUT

	query="select e.db_name, x.dbprimary_acc, x.info_text, ts.stable_id, t.seq_region_id, t.seq_region_start, t.seq_region_end, t.seq_region_strand, gs.stable_id, g.status, g.description from external_db e, xref x, object_xref ox, transcript t, transcript_stable_id ts, gene g, gene_stable_id gs where e.external_db_id>3000 and e.external_db_id <3200 and e.external_db_id=x.external_db_id and x.xref_id=ox.xref_id and ox.ensembl_id =40651 and ts.transcript_id=ox.ensembl_id and t.transcript_id=ox.ensembl_id and t.gene_id=g.gene_id and g.gene_id=gs.gene_id";

	echo $query | mysql $MYSQL_ARGS

}

ListUniqueTranscriptXrefsByDBID(){
	#$PROBESET=$1
	CheckVariables 1

	#2nd arg as TARGET|OUT

	query="select x.dbprimary_acc, x.info_text, ts.stable_id, t.seq_region_id, t.seq_region_start, t.seq_region_end, t.seq_region_strand, gs.stable_id, g.status, g.description from xref x, object_xref ox, transcript t, transcript_stable_id ts, gene g, gene_stable_id gs where x.external_db_id>3000 and x.external_db_id <3200 and x.xref_id=ox.xref_id and ox.ensembl_id =40651 and ts.transcript_id=ox.ensembl_id and t.transcript_id=ox.ensembl_id and t.gene_id=g.gene_id and g.gene_id=gs.gene_id group by x.dbprimary_acc";

	echo $query | mysql $MYSQL_ARGS

}

ListProbeSetXrefs(){
	#$PROBESET=$1
	CheckVariables 1

	#2nd arg as TARGET|OUT

	query="select e.db_name, x.dbprimary_acc, x.description, x.info_type, x.info_text, ox.ensembl_id, ts.stable_id, t.seq_region_id, t.seq_region_start, t.seq_region_end, t.seq_region_strand, gs.stable_id from xref x, object_xref ox, external_db e, transcript_stable_id ts, transcript t, gene_stable_id gs where x.xref_id=ox.xref_id and e.external_db_id=x.external_db_id and ox.ensembl_id=ts.transcript_id and ox.ensembl_id=t.transcript_id and t.gene_id=gs.gene_id and x.dbprimary_acc='$1' order by e.db_name, t.seq_region_id, t.seq_region_start;"

	echo $query | mysql $MYSQL_ARGS

}


ListProbeSetMappings(){
	#$PROBESET=$1
	CheckVariables 1

	#2nd arg as TARGET|OUT

	query="select oa.name, p.probeset, p.name, pf.seq_region_start, pf.seq_region_end, pf.seq_region_strand, pf.mismatches from oligo_probe p, oligo_feature pf, oligo_array oa where oa.oligo_array_id=p.oligo_array_id and p.probeset='$1' and p.oligo_probe_id = pf.oligo_probe_id order by oa.name, pf.seq_region_id, pf.seq_region_start;"

	echo $query | mysql $MYSQL_ARGS

}


ContinueOverride(){
    CheckVariables 1

    if [ ! $2 ]
    then
		AskQuestion "$1"

		if [[ $REPLY != [yY]* ]]
		then
			echo "Exiting"
			exit
		fi
    else
		echo "Auto Continue"
    fi

}

MigrateOligos(){
    CheckVariables VERSION

    #add args to only migrate xrefs or oligos?

    pversion=${pversion:=$(expr $VERSION + 1)}

    #echo "Need to implement version lookup here"
    #exit;

    
    #sanity check
    SetMYSQL_ARGS OUT

    echo "Patching to version $pversion"

    echo  ${SRC}/ensembl/misc-scripts/schema_patch.pl --host $ODBHOST --port $ODBPORT \
        --user ensadmin --pass ensembl --pattern $ODBNAME --schema $pversion --interactive=0\
        --logfile ${WORK_DIR}/schema_patch.${pversion}.log

    ${SRC}/ensembl/misc-scripts/schema_patch.pl --host $ODBHOST --port $ODBPORT \
        --user ensadmin --pass ensembl --pattern $ODBNAME --schema $pversion  --interactive=0\
        --logfile ${WORK_DIR}/schema_patch.${pversion}.log


    BackUpTables xrefs patched
    BackUpTables arrays patched

    echo "Loading into new DB here"

    SetMYSQL_ARGS TARGET
    echo "Loading oligo_array"
    mysql $MYSQL_ARGS < ${WORK_DIR}/backup/oligo_array.txtpatched
    echo "Loading oligo_probe"
    mysql $MYSQL_ARGS < ${WORK_DIR}/backup/oligo_probe.txtpatched
    echo "Loading oligo_feature"
    mysql $MYSQL_ARGS < ${WORK_DIR}/backup/oligo_feature.txtpatched
    echo "Loading xref"
    mysql $MYSQL_ARGS < ${WORK_DIR}/backup/xref.txtpatched
    echo "Loading object_xref"
    mysql $MYSQL_ARGS < ${WORK_DIR}/backup/object_xref.txtpatched

    echo "Finished migrating"

    echo "do some counts here for QC"

}


PrependPlus(){
    CheckVariables 1
    num=$1

    if [[ $num -eq 0 ]]
    then
	num= 
    elif [[ $num != -[0-9]* ]]
    then
	num="+${num}"
    fi

    echo $num
}



UpdateTDBSeqRegion(){
    CheckVariables TDBNAME 1 2 3

    #1 array_name
    #2 is current seq_region_id
    #3 is new seq_region_id

    SetMYSQL_ARGS TARGET

    echo "Updating $1 $2 to $3"

    query="update oligo_feature of, oligo_probe op, oligo_array oa set of.seq_region_id=$3 where of.seq_region_id=$2 and of.oligo_probe_id=op.oligo_probe_id and op.oligo_array_id=oa.oligo_array_id and oa.name=\"$1\""

    echo "Updating seq_region_ids with"
    echo $query
    
    echo $query | mysql $MYSQL_ARGS

    #safety
    SetMYSQL_ARGS OUT
}



UpdateSeqRegions(){
    CheckVariables TDBNAME

    SetMYSQL_ARGS OUT

    #This only works for the first cs_id of the first feature returneed
    #This should be chromosome but not guaranteed :(

    #need to dump and compare??
    #should not have different cs_ids within the same assembly as this will affect other ancilliary DBs!!


    #get coord system id
    ocs_id=$(QueryVal select seq_region_id from oligo_feature limit 1)
    osr_ids=($(QueryVal select seq_region_id from seq_region where coord_system_id=$ocs_id order by name))
    osr_names=($(QueryVal select name from seq_region where coord_system_id=2 order by name))

    echo "of cs id is $ocs_id"


    SetMYSQL_ARGS TARGET
    tsr_ids=($(QueryVal select seq_region_id from seq_region where coord_system_id=$ocs_id order by name))
    tsr_names=($(QueryVal select name from seq_region where coord_system_id=2 order by name))

    SetMYSQL_ARGS OUT

    echo "Updating seq_region_ids $osr_ids"


    for i in ${#osr_ids[*]}
    do

	echo "doing expr of $i - 1"
	i=`expr $i - 1`
	found=0

	for p in ${#tsr_names[*]}
	do

	    if [[ ${osr_names[$i]} == ${tsr_names[$p]} ]]
	    then 
		found=1
		
		if [[ ${osr_ids[$i]} != ${tsr_ids[$p]} ]]
		then
		    echo "seq_region_ids for ${osr_names[$i]} do not match...updating in $ODBNAME"
		    
		    #Need to sanity check here that there aren't overlaps between seq_region_ids
		    test=$(QueryVal select oligo_feature_id from oligo_feature where seq_region_id=${tsr_ids[$p]} limit 1)

		    if [ $test ]
		    then
				echo "Found overlapping seq_region_ids between DBs, need to implement temp seq_region_ids"
				exit
		    fi		

		    query="update oligo_feature set seq_region_id=${tsr_ids[$p]} where seq_region_id=${tsr_ids[$p]}"

		fi
	    fi
	done

	if [[ $found == 0 ]]
	then
	    echo "Could not find seq_region_id for ${osr_names[$i]} in $TDBNAME"
 	    return 101
	fi
    done

    BackUpTables arrays seq_reg

}

DumpXrefFiles(){
    CheckVariables 1

    
    if [[ $1 == TARGET ]]
    then
	args="-h $TDBHOST -u ensro -d $TDBNAME -P $TDBPORT"
    elif [[ $1 == OUT ]]
    then
	args="-h $ODBHOST -u ensro -d $ODBNAME -P $ODBPORT"
    else
	echo "Need to specifiy OUT or TARGET db"
	exit
    fi

    ChangeMakeDir $WORK_DIR/xref_dumps

    echo "Dumping XREF files from $args"

    while read f
    do 
	${SRC}/ensembl-personal/npj/oligo_mapping/scripts/dump_array_xrefs.pl -a $f -o xref_dumps $args 
    done < $WORK_DIR/array.list

}




CompareStagingMirrorDBs(){
    SVER=$1
  
    CheckVariables SVER

    echo "Comparing assembly/genebuild infro between staging and live servers"

    MVER=`expr $SVER - 1`


    STAGING="-hens-staging -uensro"
    ENSDB="-hensembldb.ensembl.org -uanonymous"
    CNT=0

    MYSQL_ARGS=$STAGING
    SDBS=$(QueryVal show databases like \"%core_${SVER}_%\")

    for dbname in $SDBS
    do
	CNT=`expr $CNT + 1`

	if [ $CNT -gt 2 ]
	then
      	    MYSQL_ARGS="$STAGING $dbname"
	    GBUILD[$CNT]=$(QueryVal select meta_value from meta where meta_key=\"genebuild.version\")
	    ASSEMBLY[$CNT]=$(QueryVal select meta_value from meta where meta_key=\"assembly.name\")
	fi
    done

    CNT=0
    MYSQL_ARGS=$ENSDB


    OFILE="${AFFY_HOME}/assembly_build_comparison.${SVER}_${MVER}"

    if [ -f $OFILE ]
    then
	rm -f $OFILE
    fi


    for dbname in $SDBS
    do
	CNT=`expr $CNT + 1`

	if [ $CNT -gt 2 ]
	then
	    edbname=$(echo $dbname | sed s"/_core_${SVER}_[0-9]*[a-z]*/_core_${MVER}/")   
      	    MYSQL_ARGS="$ENSDB"
	    edbname=$(QueryVal show databases like \"${edbname}%\");

	    if [[ $edbname ]]
	    then
		OUTLINE=""
		edbname=$(echo $edbname | sed 's/.*)//')
		MYSQL_ARGS="$ENSDB $edbname"
		EGBUILD=$(QueryVal select meta_value from meta where meta_key=\"genebuild.version\")
		EASSEMBLY=$(QueryVal select meta_value from meta where meta_key=\"assembly.name\")
	      
		#compare dbnames here to get genebuild difference
		EGB=$(echo $edbname | sed s'/.*_core_[0-9]*_//')
		SGB=$(echo $dbname | sed s'/.*_core_[0-9]*_//')

		if [[ ${ASSEMBLY[$CNT]} != $EASSEMBLY ]]
 		then
		    OUTLINE="Mapping ({$EASSEMBLY} > ${ASSEMBLY[$CNT]})  &  "
		fi


		if [[ ${GBUILD[$CNT]} != $EGBUILD ]]
 		then
       		    OUTLINE="$OUTLINE    Xrefing (${EGBUILD} > ${GBUILD[$CNT]})"
		elif [[ $EGB != $SGB ]]
		then
		    OUTLINE="$OUTLINE    Xrefing (Patched set $EGB > $SGB contact genebuilder?)"
		elif [[ $OUTFILE ]]
		then
		    OUTLINE="$OUTLINE  WARNING: SAME GENEBUILD FOR NEW ASSEMBLY"
		fi

	  
		if [[ $OUTLINE ]]
		then
		    OUTLINE="$dbname requires:    ${OUTLINE}"
		    echo $OUTLINE
		    echo $OUTLINE >> $OFILE
		fi


	    else
		echo "New DB $dbname!!!"
	    fi
	fi
    done

    #get all names from staging matching core_SVER
    #for each add element to assembly and genebuild arrays
    
    #for each one, sed for core_MVER
    #then test assembly and genebuild vs array values
    

}


