#!/usr/local/bin/bash

# This is deploymentwide configuration
# Do not change these unless you intend to change the 
# behaviour of all instances of the array mapping environment
# To make instance specific changes you can override these defaults 
# by setting them in your instance.env file

# DO NOT REMOVE/COMMENT ANY OF THE VARIABLES
# Or you may inherit unwanted variables from a 
# previous environment.

#Probably need to move some of these 'true' constants back to array.env?
#Or maybe clean them all before sourcing this to prevent against deleting them from here
#And inheriting from previous env?

### Define constants ###

#Formats must always be "VENDOR_FORMATTYPE"
#Or can simply be "VENDOR"
#This is used in RunTranscriptXrefs

export PIPELINE_PACKAGE='ensembl-pipeline'

export VALID_ARRAY_FORMATS='AFFY_UTR AFFY_ST ILLUMINA_WG CODELINK PHALANX LEIDEN AGILENT ILLUMINA_INFINIUM STEMPLE_LAB_SANGER WUSTL'
export VALID_SEQ_TYPES='GENOMICSEQS TRANSCRIPTSEQS'
export VALID_ALIGN_TYPES='GENOMIC TRANSCRIPT'


#These FACTORS are the multipliers by which the actual physicial size of the fasta file is multipled 
#to get the the memory usage figure for the import and xref steps
#9.3 comes to just under ~14.5GB for the the human exon and gene ST arrays
#Actual mem used is ~10.7GB for human STs so can drop this a little
#>9.3 tends to never get submitted
#Which is handy as this is below the max we can get on a normal node
#These are now used to multiply sum of the seq and array file sizes
#To get the amount of KB required
#Do not change these!
export IMPORT_MEM_FACTOR=7
export IMPORT_MEM_BASE=2500

#This comes to about 18GB for ST
#But only 280MB for ILLUMINA_WG
#ALIGN_MEM_FACTOR will be used in conjuction with CHUNKS
#Can we just use one mem factor to generate all?

#Actual figures HUMAN Import
#ST 11GB??? So why are we hugememming this?
#ILLUMINA WG 141MB
#UTR 4.2GB?
#Let's test this.



#Do not change!! XREF_MEM_FACTOR=11
export XREF_MEM_FACTOR=11
#Upped to 8500 to handle Human AFFY_STs
export XREF_MEM_BASE=8500



#XREF actual figures
#HUMAN ILLUMINA 1.4GB
#HUMAN ST 4GB
#HUMAN UTR 1.9GB

#RAT AND MOUSE FAILING for ST with 11 0
#upped to 11 1000



#Can't get this to work with exponent weighting, see SetMusage

#GENOMIC_CHUNKS was 3000
#Reduced to 2500 as we we getting the odd job >2GB mem
#Need to up the batch number accodingly and throttle?
#Either that or set SetMusage for particular format and set as env var in ProbeAlign config
export GENOMIC_CHUNKS=2500
#do we set a hard chunk number or just multiply by 10?
export TRANSCRIPT_CHUNKS=25000

#Can we set these in pipeline.env?
export DATA_HOME=$HOME/data/array_mapping
export HUGEMEM_HOME=$HOME/huge_array_mapping
#HUGMEM_HOME is only required is the HUGEMEM_QUEUE hosts cannot see DATA_HOME

#This is for location of compiled binaries
#Cannot interpolate directly in EXONERATE_PATH as
#pipeline tries to locate the exectuable and fails somehow
#This is not ideal as we are now using the arch of the submitter node to run all the farm jobs
export ARCH=`uname -m`
export EXONERATE_PATH="/path/to/your/version/of/exonerate"
export EXONERATE_VERSION='2.2.0'
export PROBE2TRANSCRIPT_PARAMS='--calculate_utrs --utr_multiplier 1'
#export PROBE2TRANSCRIPT_BSUB_CMD='bsub -q hugemem -R "select[mem>30000] rusage[mem=30000]" -M 30000000'
#Set to undef to run locally
#This is now actually just a flag to bsub, the contents of the var are not
export BSUB_PROBE2TRANSCRIPT=1
#Now calculate requirements dynamically
# -R \\"select[mem>$musage] rusage[mem=$musage]\\" -M $musage_k'



### Initialise/Reset Variables ###
### Clean only args - Never set here!
export ALIGN_TYPES=
export ARRAY_NAMES=
export ARRAY_FORMATS=
export GENOMICSEQS=
export TRANSCRIPTSEQS=
export MULTI_SPECIES=
