#!/usr/local/bin/bash

# Copyright [1999-2015] Wellcome Trust Sanger Institute and the EMBL-European Bioinformatics Institute
# Copyright [2016-2017] EMBL-European Bioinformatics Institute
# 
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
#      http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

echo ':::: Welcome to the eFG sequencing analysis environment'

#Source in some handy functions
. $EFG_SRC/scripts/environments/funcs.sh

export ENV_NAME='sequencing'
#env colour is red
export PS1_COLOUR=31

SEQUENCING_CONFIG=$EFG_SRC/scripts/environments/sequencing.config
echo ":: Sourcing SEQUENCING CONFIG: $SEQUENCING_CONFIG"
. $SEQUENCING_CONFIG

#To do

#In theory you could stop ("pause") the pipeline, add more jobs, and rerun it without major problems...
# - test if this works...

# AddAnalysis to add more analysis to the Database... THIS SHOULD BE PART OF THE EFG env...

# Integrate Alignment step which runs bwa? (just extend the pipeline so it also includes that step...)
# run_bwa.sh or mapping pipeline needs to move sam output to parent dir, then we can remove working bwa dir?

################################################################################
# Func      : _InitEnv(
# Desc      : Sets up all config defined variables. Should only be called from
#             instance config file (species_VERSION.arrays) e.g. mouse_51.arrays 
# Args [n]  : 
# Return    : none
# Exception : 
################################################################################

_InitEnv(){

    echo ":: Setting config for $SPECIES sequence pipeline"

    export PS1="\[\033[${PS1_COLOUR}m\]${ENV_NAME}:${DB_NAME}>\[\033[0m\]"

    #check if all required variables are checked here...
    CheckVariables DB_PORT DB_NAME DB_HOST DB_USER DB_PASS DB_READ_USER
    CheckVariables DNADB_NAME DNADB_HOST DNADB_PORT DNADB_USER 
    CheckVariables SPECIES ASSEMBLY VALID_PEAK_ANALYSIS
    CheckVariables SRC EFG_SRC DATA_HOME BIN_DIR

    CheckDirs $DATA_HOME $PIPELINE_SCRIPTS $EFG_SRC    

    #Check if all the necessary data is in $DATA_HOME appropriate folders
    echo "Check if all the necessary data is where it is supposed to be: see $EFG_SRC/docs/sequencing_analysis.txt"

    export VALID_FILE_TYPES='sam bed'
    export LSF_RESOURCE_HOST=my$(echo $DB_HOST | sed 's/-/_/')
    export LSF_DNA_DB==my$(echo $DNADB_HOST | sed 's/-/_/')

    export EFG_SCRIPTS=${EFG_SRC}/scripts:${EFG_SRC}/scripts/miscellaneous
    export PIPELINE_SCRIPTS=${SRC}/ensembl-hive/scripts
    export PATH=${PIPELINE_SCRIPTS}:${EFG_SCRIPTS}:$PATH

    #Setup PERL5LIB here too, otherwise assume it is ok from the base environment (eg. bashrc)  

    export WORK_DIR=${DATA_HOME}/output/${DB_NAME}
    export BACKUP_DIR=${WORK_DIR}/backup
    alias workdir='cd $WORK_DIR'

    #Need to change this in case DNADB has a password...
    export DNADB_SCRIPT_ARGS="-dnadb_host $DNADB_HOST -dnadb_user $DNADB_USER -dnadb_name $DNADB_NAME -dnadb_port $DNADB_PORT"
    export DB_SCRIPT_ARGS="-dbhost $DB_HOST -dbuser $DB_USER -dbpass $DB_PASS -dbname $DB_NAME -dbport $DB_PORT"
    export DB_READ_SCRIPT_ARGS="-dbhost $DB_HOST -dbuser $DB_READ_USER -dbname $DB_NAME -dbport $DB_PORT"

    alias mysqlefg='mysql -h$DB_HOST -P$DB_PORT -u$DB_USER -p$DB_PASS $DB_NAME'
    alias mysqlcore='mysql -h$DNADB_HOST -P$DNADB_PORT -u$DNADB_USER $DNADB_NAME'

    alias mysqlmotifspipe='mysql -h$DB_HOST -P$DB_PORT -u$DB_USER -p$DB_PASS ${USER}_motif_import_${DB_NAME}'

    #update the meta entry for the files
    exists=$(echo "select meta_value from meta where meta_key='species.production_name'" | mysql -h$DB_HOST -u$DB_READ_USER -P$DB_PORT $DB_NAME)
    if [[ ! $exists ]]; then
	echo "Adding species.production_name in meta as $SPECIES"
	echo "insert into meta (species_id, meta_key, meta_value) values (1,'species.production_name','$SPECIES')" | mysql -h$DB_HOST -u$DB_USER -P$DB_PORT -p$DB_PASS $DB_NAME
    fi

    #Create pipeline output dir if not present
    MakeDirs $WORK_DIR
    
    #Start at the working directory
    cd $WORK_DIR 
}




FilterBlacklist(){
	echo ":: FilterBlacklist $*"
	
	usage='usage: FilterBlacklist [ -f(eature set)  name ]+ [-h help]'

	OPTIND=1
	sets=
	while getopts ":f:h" opt; do
	    set=
	    case $opt in 
		f  ) set=$OPTARG;;
	        \? ) echo $usage; exit 1;;
	        h  ) echo $usage; return 0;;
	     esac 
	     
	     if [[ $set ]]; then
		 sets="$sets $set"
	     fi

	done		
	
	report_file=${WORK_DIR}/blacklist_${DB_NAME}.report
	echo ": Generating Report: $report_file"	
	BackUpFile $report_file

	if [[ ! "$sets" ]]; then
	    sets="-all "
	else
	    sets="-feature_sets $sets"
	fi

	cmd="perl $EFG_SRC/scripts/miscellaneous/check_overlaps_with_encode_blacklist.pl $DB_SCRIPT_ARGS $DNADB_SCRIPT_ARGS -remove $sets -output $report_file"
	echo $cmd
	Execute $cmd 

}


PeaksReport(){
	echo ":: PeaksReport $*"
	
	usage='usage: PeaksReport [-i(nfer sets by default reports all in the efg db)] [-h help]'

	OPTIND=1
	infer=
	while getopts "ih" opt; do
	    case $opt in 
		i  ) infer="1";;
	        \? ) echo $usage; exit 1;;
	        h  ) echo $usage; return 0;;
	     esac 
	done		

	sets=
	if [[ "$infer" ]]; then
	    #infer set names from peaks pipeline output to avoid printing all sets (can be many!)
	    for analysis in $VALID_PEAK_ANALYSIS; do 
		#Some assumptions
		asets=$(ls $WORK_DIR/peaks/results/* | grep -i $analysis | sed 's/^.*\///' | perl -pe "s/.samse.sam.gz.${analysis}\S*/_${analysis}/i" | uniq)
		sets="$sets $asets"
	    done 
	    sets="-feature_sets $sets"
	fi
	
	name=peaks_${DB_NAME}.report
	report_file=${WORK_DIR}/${name}.pdf
	echo ": Generating Report: $report_file"	
	BackUpFile $report_file

	cmd="perl $EFG_SRC/scripts/peaks_report.pl $DB_READ_SCRIPT_ARGS $DNADB_SCRIPT_ARGS -R -compare -feature_table annotated -no_outliers -name $name -outdir $WORK_DIR $sets"
	echo $cmd
	Execute $cmd 

}


ImportPWM(){
	echo ":: ImportPWM $*"

	usage='usage: ImportPWM -j(aspar matrix e.g. MA0139.1) -f(eature type e.g. CTCF) [-d(irectory for Jaspar data file matrix_only.txt default is $DATA_HOME/binding_matrices/Jaspar] [-h help]'

	folder=${DATA_HOME}/binding_matrices/Jaspar
	jaspar=
	feature=
	OPTIND=1

	while getopts ":j:f:d:h" opt; do
	    case $opt in 
		j  ) jaspar=$OPTARG;;
                f  ) feature=$OPTARG;;
		d  ) folder=$OPTARG;;
	        \? ) echo $usage; exit 1;;
	        h  ) echo $usage; return 0;;
	     esac 
	done		
	
	if [[ ! $jaspar || ! $feature ]]; then
	    echo "Need matrix and feature type"
	    return 0
	fi

	cmd="perl $EFG_SRC/scripts/import/import_matrix.pl $DB_SCRIPT_ARGS $DNADB_SCRIPT_ARGS -species $SPECIES -matrix_id $jaspar -feature_type $feature -matrix_data_dir $folder"
	echo $cmd
	Execute $cmd 

}


RunMotifMatch(){
	echo ":: RunMotifMatch $*"

	usage='usage: RunMotifMatch [-f(eature type) e.g. Max]+ [-h help]'

	folder=${DATA_DIR}/binding_matrices/Jaspar
	feature_list=
	OPTIND=1

	while getopts ":f:h" opt; do
	    feature=
	    
      case $opt in 
          f  ) feature=$OPTARG;;
	        \? ) echo $usage; exit 1;;
	        h  ) echo $usage; return 0;;
	    esac 
	     
	    if [[ $feature ]]; then
		    feature_list="$feature_list $feature"
	    fi  
	done		
	
	map_dir=$WORK_DIR/pwm_mappings
	Execute "mkdir -p $map_dir"

	if [[ $feature_list ]]; then
	    feature_list="-feature_type_list $feature_list"
	fi
	     
	#At the moment this needs to be run in a specific folder. TODO update the scripts
	cd $map_dir

  CheckGlobalVariables SCHEMA_BUILD
	
	#TODO extra job management...
	bsub -J pwm_mappings_${DB_NAME} -e $map_dir/pwm_mappings_${DB_NAME}.err -o $map_dir/pwm_mappings_${DB_NAME}.out \
    -q long -M35000 -R"select[mem>35000] rusage[mem=35000]" \
    perl $EFG_SRC/scripts/pwm_mappings/run_pwm_mapping_pipeline.pl $DB_SCRIPT_ARGS $DNADB_SCRIPT_ARGS \
    -species $SPECIES -assembly $ASSEMBLY -workdir $DATA_HOME -outputdir $map_dir $feature_list
	#echo $cmd
	#Execute  $cmd 

	echo "Now wait for the job to finish and check logs and output"

}


ImportMotifMatch(){
	echo ":: ImportMotifMatch $*"

	usage="usage: ImportMotifMatch [-s(slice) e.g. 22]+ [-h help]"
	#TODO do some tests when pipeline already exists... may need a DropPipeline before...

	slice_list=
	OPTIND=1

	while getopts ":s:h" opt; do
	    slice=
	    case $opt in 
                s  ) slice=$OPTARG;;
	        \? ) echo $usage; exit 1;;
	        h  ) echo $usage; return 0;;
	     esac 
	     
	     if [[ $slice ]]; then
		 slice_list="$slice_list $slice"
	     fi
     
	done		


        #Make this more generic...
	export PDB_NAME=${USER}_motif_import_${DB_NAME}

	exists=$(echo "show databases like '$PDB_NAME'" | mysql -h$DB_HOST -u$DB_READ_USER -P$DB_PORT)
	if [[ $exists ]]; then
	    echo "Pipeline DB $PDB_NAME already exists: Need to start from scratch. Use DropPipelineDB motif_import"
	    echo "Aborting"
	    return 0
	fi
	
	if [[ $slice_list ]]; then
	    slice_list="-slices $slice_list"
	fi

	map_dir="$WORK_DIR/pwm_mappings/"
    
	cmd="perl $EFG_SRC/scripts/pwm_mappings/run_binding_site_import_pipeline.pl $DB_SCRIPT_ARGS $DNADB_SCRIPT_ARGS -workdir $map_dir -output_dir $WORK_DIR $slice_list"
	echo $cmd
	Execute $cmd 

	cmd="beekeeper.pl -url mysql://${DB_USER}:${DB_PASS}@${DB_HOST}:${DB_PORT}/${PDB_NAME} -sync"
	Execute $cmd

	#Cannot use Execute in case we want to interrupt it
	echo "beekeeper.pl -url mysql://${DB_USER}:${DB_PASS}@${DB_HOST}:${DB_PORT}/${PDB_NAME} -loop"
	beekeeper.pl -url mysql://${DB_USER}:${DB_PASS}@${DB_HOST}:${DB_PORT}/${USER}_motif_import_${DB_NAME} -loop

	echo "Now check results in $WORK_DIR/motif_features/results"

}

InferPWM(){
	echo ":: InferPWM $*"

	usage="usage: InferPWM [-a(nalysis to consider) by default all in $VALID_PEAK_ANALYSIS]+ [-h help]"

	OPTIND=1

	analist=
	while getopts ":a:h" opt; do
	    slice=
	    case $opt in
		#TODO verify if entered analysis is valid...
		a  ) analist="$analist $OPTARG";;
	        \? ) echo $usage; exit 1;;
	        h  ) echo $usage; return 0;;
	     esac 	     
	done		

	if [[ ! $analist ]]; then
	    analist=$VALID_PEAK_ANALYSIS
	fi
        
        #Make this more generic...
	PDB_NAME=${USER}_motif_finder_${DB_NAME}

	IS_SETUP=0
	exists=$(echo "show databases like '$PDB_NAME'" | mysql -h$DB_HOST -u$DB_READ_USER -P$DB_PORT)
	if [[ $exists ]]; then
	    echo "Reusing existing DB $PDB_NAME. Use DropPipelineDB to start from scratch"
	    IS_SETUP=1
	fi
	
	dir=$WORK_DIR/peaks/results
	for analysis in $analist; do 
	    #Some assumptions here
	    asets=$(ls $dir/* | grep -i $analysis | sed 's/^.*\///' | perl -pe "s/.samse.sam.gz.${analysis}\S*/_${analysis}/i" | uniq)

	    for fset in $asets; do

		job_topup=
		if [[ $IS_SETUP == 1 ]]; then
		    job_topup=' -job_topup'
		fi

		cmd="$PIPELINE_SCRIPTS/init_pipeline.pl Bio::EnsEMBL::Funcgen::HiveConfig::MotifFinder_conf -work_dir $WORK_DIR $DB_READ_SCRIPT_ARGS $DNADB_SCRIPT_ARGS -species $SPECIES -pipeuser $DB_USER -pipepass $DB_PASS -pipedb_name $PDB_NAME -feature_set $fset $job_topup"
		Execute perl $cmd
		
		if [ $? == 0 ]; then 
		    IS_SETUP=1
		fi
		
	    done

	done 

	if [[ $IS_SETUP == 0 ]]; then
	    echo "No jobs created"
	    return 1
	fi
	
	cmd="$PIPELINE_SCRIPTS/beekeeper.pl -url mysql://${DB_USER}:${DB_PASS}@${DB_HOST}:${DB_PORT}/${PDB_NAME} -sync"
	Execute perl $cmd

	cmd="$PIPELINE_SCRIPTS/beekeeper.pl -url mysql://${DB_USER}:${DB_PASS}@${DB_HOST}:${DB_PORT}/${PDB_NAME} $logic_name -loop -sleep 15"
	Execute $cmd  2>&1 | tee -a $WORK_DIR/motif_inference.${PDB_NAME}.log

	echo "Now check results in $WORK_DIR/motifs/results"

}



GetFailedJobs(){
  #options to get info without fail not retry, just report jobs which have retried without reachign FAILED state
  #would need to accomodate relations ship between retry/retry_count and current jhob status
  #job_message.is_error can be used as proxy for FAILED attempt (i.e. not 'FAIL_NO_RETRY')
  #look at procedures?
  #job_search - contains nice join to analysis_data. (takes quite a long time as uses like)

  #add ability to get running jobs aswell as failed?	


  OPTIND=1
  type=
  analysis=
  limit=
  retry=
  long=
  job_id=
  debug=0
  usage='usage: GetFailedReadJobs -t(ype) e.g. reads [ -l(ong format) -a(nalysis) logic_name -r(retry) COUNT -j(ob_id) ID -L(imit) NUMBER -h(elp) ]'

  fields='j.job_id, j.retry_count, a.logic_name, jm.msg, jf.path'
  long_fields='jm.status as status_at_failure, w.process_id as lsf_id, jm.time, j.input_id'
  #Could do with left joining to analysis_data for long format

  while getopts ":t:la:r:j:L:dh" opt; do
	  case $opt in
		  t  ) type=$OPTARG ;;
		  l  ) fields="${fields}, $long_fields" ;;
          a  ) analysis="and a.logic_name='${OPTARG}'" ;;
          r  ) retry="and j.retry_count=${OPTARG}" ;;
          j  ) job_id="and j.job_id=${OPTARG} " ;;
          L  ) limit=" LIMIT $OPTARG";;
		  d  ) debug=1 ;;
		  h  ) echo -e $usage; return 0;;
          \? ) echo -e $usage; return 1;;
	esac
  done

  #Should validate type against valid hive types here

  #could LPAD some of these to make the output nicer
  #can we printf in sql
  #only list the err files to avoid redundancy. Assume people can sub err for out manually
  sql="select $fields from job j, analysis a, job_message jm, job_file jf, worker w where j.status='FAILED' and jm.worker_id=w.worker_id and j.analysis_id=a.analysis_id and j.job_id=jm.job_id and j.job_id=jf.job_id and j.retry_count=(jf.retry+1) and j.retry_count=(jm.retry+1) and jf.path like '%.err' $job_id $analysis $retry $limit"

  if [[ $debug = 1 ]]; then
	  echo $sql
  fi

  #eval required as alias not available in function
  echo $sql | eval "mysql${type}pipe"
}


#This needs to be redefined here, since it's not the same as the old pipeline (which we are not using anyway) 
#This actually drops the whole DB
DropPipelineDB(){
    echo ":: DropPipelineDB $*"

    OPTIND=1
    logic_name=
    pipeline=
    usage='usage: DropPipelineDB -p(ipeline) pipeline (alignments | peaks | motif_import | read_counts | motif_finder) [ -h(elp) ]';
    
    while getopts ":p:h" opt; do
	case $opt in 
	    p  ) pipeline=$OPTARG ;;
	    h  ) echo -e $usage; return 0;;
            \? ) echo -e $usage; return 1;;
	esac 
    done

    if [ ! "$pipeline" ]; then
	echo -e $usage; return 1;	
    fi

    PDB_NAME=${USER}_${pipeline}_${DB_NAME}
    exists=$(echo "show databases like '$PDB_NAME'" | mysql -h$DB_HOST -u$DB_USER -P$DB_PORT -p$DB_PASS)

    if [[ ! $exists ]]; then
	echo "Pipeline DB $PDB_NAME does not exist"
	return 0
    fi

    AskQuestion "Drop the database $PDB_NAME ? [y|n]"
    if [[ $REPLY != [yY]* ]]; then
	echo "Aborting"
	return
    fi	

    echo ":: Dropping pipeline database: $PDB_NAME"
    echo "drop database $PDB_NAME" | mysql -h$DB_HOST -u$DB_USER -P$DB_PORT -p$DB_PASS
    
    if [ $? == 0 ]; then 
	export IS_SETUP=0
    else 
	echo "Could not drop Database $PDB_NAME"
	return 1
    fi

}

# This should be moved back to the hive env or pipeline.env? 
# Or just omitted?
RollbackSet(){
	echo ":: RollbackSet $*"

	force=
	full=
	
	#This makes sure we reset the getopts ind if we have used it previously
	OPTIND=1

	usage='usage: RollbackSet -d(force delete) -f(ull delete, otherwise just removes features and status) -h(elp) ] [Set_names]+';

	while getopts ":d:f:h" opt; do
	    case $opt in 
                d  ) force=' -force_delete ' ;;
                f  ) full=' -full_delete ' ;;
                h  ) echo $usage; return 0;;
                \? ) echo $usage; return 1;;
	    esac 
	done


        #Get trailing file name arguments
	idx=1
	
	while [[ $idx -lt $OPTIND ]]; do 
	    idx=$(($idx + 1))
	    shift 
	done
	
	set_names=($*)

	for set in $set_names; do
	    
	    #Maybe validate set name before trying to delete it?
	    cmd="$EFG_SRC/scripts/rollback/rollback_experiment.pl $DB_SCRIPT_ARGS $DNADB_SCRIPT_ARGS -species $SPECIES  -feature_set $set $force $full"
	    echo $cmd
	    Execute perl $cmd 
		
	done

}
