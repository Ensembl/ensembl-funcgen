#!/usr/local/bin/bash

# Copyright [1999-2015] Wellcome Trust Sanger Institute and the EMBL-European Bioinformatics Institute
# 
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
#      http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

echo ':::: Welcome to the eFG sequencing analysis environment'

#Source in some handy functions
. $EFG_SRC/scripts/environments/funcs.sh

export ENV_NAME='sequencing'
#env colour is red
export PS1_COLOUR=31

SEQUENCING_CONFIG=$EFG_SRC/scripts/environments/sequencing.config
echo ":: Sourcing SEQUENCING CONFIG: $SEQUENCING_CONFIG"
. $SEQUENCING_CONFIG

#To do

#In theory you could stop ("pause") the pipeline, add more jobs, and rerun it without major problems...
# - test if this works...

# AddAnalysis to add more analysis to the Database... THIS SHOULD BE PART OF THE EFG env...

# Integrate Alignment step which runs bwa? (just extend the pipeline so it also includes that step...)
# run_bwa.sh or mapping pipeline needs to move sam output to parent dir, then we can remove working bwa dir?

################################################################################
# Func      : _InitEnv(
# Desc      : Sets up all config defined variables. Should only be called from
#             instance config file (species_VERSION.arrays) e.g. mouse_51.arrays 
# Args [n]  : 
# Return    : none
# Exception : 
################################################################################

_InitEnv(){

    echo ":: Setting config for $SPECIES sequence pipeline"

    export PS1="\[\033[${PS1_COLOUR}m\]${ENV_NAME}:${DB_NAME}>\[\033[0m\]"

    #check if all required variables are checked here...
    CheckVariables DB_PORT DB_NAME DB_HOST DB_USER DB_PASS DB_READ_USER
    CheckVariables DNADB_NAME DNADB_HOST DNADB_PORT DNADB_USER 
    CheckVariables SPECIES ASSEMBLY VALID_PEAK_ANALYSIS
    CheckVariables SRC EFG_SRC DATA_HOME BIN_DIR

    CheckDirs $DATA_HOME $PIPELINE_SCRIPTS $EFG_SRC    

    #Check if all the necessary data is in $DATA_HOME appropriate folders
    echo "Check if all the necessary data is where it is supposed to be: see $EFG_SRC/docs/sequencing_analysis.txt"

    export VALID_FILE_TYPES='sam bed'
    export LSF_RESOURCE_HOST=my$(echo $DB_HOST | sed 's/-/_/')
    export LSF_DNA_DB==my$(echo $DNADB_HOST | sed 's/-/_/')

    export EFG_SCRIPTS=${EFG_SRC}/scripts:${EFG_SRC}/scripts/miscellaneous
    export PIPELINE_SCRIPTS=${SRC}/ensembl-hive/scripts
    export PATH=${PIPELINE_SCRIPTS}:${EFG_SCRIPTS}:$PATH

    #Setup PERL5LIB here too, otherwise assume it is ok from the base environment (eg. bashrc)  

    export WORK_DIR=${DATA_HOME}/output/${DB_NAME}
    export BACKUP_DIR=${WORK_DIR}/backup
    alias workdir='cd $WORK_DIR'

    #Need to change this in case DNADB has a password...
    export DNADB_SCRIPT_ARGS="-dnadb_host $DNADB_HOST -dnadb_user $DNADB_USER -dnadb_name $DNADB_NAME -dnadb_port $DNADB_PORT"
    export DB_SCRIPT_ARGS="-dbhost $DB_HOST -dbuser $DB_USER -dbpass $DB_PASS -dbname $DB_NAME -dbport $DB_PORT"
    export DB_READ_SCRIPT_ARGS="-dbhost $DB_HOST -dbuser $DB_READ_USER -dbname $DB_NAME -dbport $DB_PORT"

    alias mysqlefg='mysql -h$DB_HOST -P$DB_PORT -u$DB_USER -p$DB_PASS $DB_NAME'
    alias mysqlcore='mysql -h$DNADB_HOST -P$DNADB_PORT -u$DNADB_USER $DNADB_NAME'

    alias mysqlalignpipe='mysql -h$DB_HOST -P$DB_PORT -u$DB_USER -p$DB_PASS ${USER}_alignments_${DB_NAME}'
    alias mysqlpeakspipe='mysql -h$DB_HOST -P$DB_PORT -u$DB_USER -p$DB_PASS ${USER}_peaks_${DB_NAME}'
    alias mysqlmotifspipe='mysql -h$DB_HOST -P$DB_PORT -u$DB_USER -p$DB_PASS ${USER}_motif_import_${DB_NAME}'
    alias mysqlreadspipe='mysql -h$DB_HOST -P$DB_PORT -u$DB_USER -p$DB_PASS ${USER}_read_counts_${DB_NAME}'

    #update the meta entry for the files
    exists=$(echo "select meta_value from meta where meta_key='species.production_name'" | mysql -h$DB_HOST -u$DB_READ_USER -P$DB_PORT $DB_NAME)
    if [[ ! $exists ]]; then
	echo "Adding species.production_name in meta as $SPECIES"
	echo "insert into meta (species_id, meta_key, meta_value) values (1,'species.production_name','$SPECIES')" | mysql -h$DB_HOST -u$DB_USER -P$DB_PORT -p$DB_PASS $DB_NAME
    fi

    #Create pipeline output dir if not present
    MakeDirs $WORK_DIR
    
    #Start at the working directory
    cd $WORK_DIR 
}




AddAlignmentDataSets(){
    echo ":: AddAlignmentDataSets $*"

    #Add this somewhere else...
    PDB_NAME=${USER}_alignments_${DB_NAME}
    IS_SETUP=0
    exists=$(echo "show databases like '$PDB_NAME'" | mysql -h$DB_HOST -u$DB_USER -P$DB_PORT -p$DB_PASS)

    if [[ $exists ]]; then
        echo "Pipeline DB $PDB_NAME already exists: If you wish to start from scratch use DropPipelineDB alignments"
        IS_SETUP=1
    fi

    OPTIND=1 	#This makes sure we reset the getopts index if we have used it previously
    analysis=
    input_dir=
    exp_name=
    group=
    skip=
    control=
    usage="Usage:\tAddAlignmentDataSets [ -I(nput dir) -e(xperiment name e.g. AUTHOR_PMID default is parsed from -input) ]"

    while getopts ":a:I:e:h" opt; do
	case $opt in 
            I  ) input_dir=$OPTARG ;;
            e  ) exp_name=$OPTARG ;;
	    h  ) echo -e $usage; return 0;;
	    \? ) echo -e $usage; return 1;;
	esac 
    done
   
    CheckVariables input_dir
    CheckDirs $input_dir

    #Set default exp_suffix based on input dir
    if [ ! "$exp_name" ]; then
	exp_name=$(echo $input_dir |sed 's/\/$//') # Strip trailing /
	exp_name=$(echo $exp_name |sed 's/.*\///')
	echo  -e "Setting experiment name to $exp_name"
    fi

    # now go through the folder and add as many jobs as required... 
    files=$(ls ${input_dir})
    if [ ! "$files" ]; then
	echo "error : could not find any files"
	return 1
    fi

    for file in $files; do

	#Remove folder names...
	file=$(echo $file |sed 's/.*\///')

	#extract the cell type and feature type from the file name
	#This is doing many strict assumptions on file name structure...
	#Could instead get these as parameters... though that would require adding one set at a time...
	cell_type=$(echo $file | perl -pe 's/_.*$//') #sed 's/_.*//')
	feature_type=$(echo $file | perl -pe 's/^.*?_//;s/_.*$//') #sed 's/^.*_//' | sed 's/\..*$//')

	job_topup=
	if [[ $IS_SETUP == 1 ]]; then
	    job_topup=' -job_topup'
	fi

	echo "Submitting job for Experiment: $exp_name ; Cell Type: $cell_type ; Feature Type: $feature_type ; File Type: $file_type"
	cmd="$PIPELINE_SCRIPTS/init_pipeline.pl Bio::EnsEMBL::Funcgen::HiveConfig::Alignment_conf $DNADB_SCRIPT_ARGS $DB_SCRIPT_ARGS -pipedb_name $PDB_NAME -bin_dir $BIN_DIR -work_dir $DATA_HOME -output_dir $WORK_DIR -species $SPECIES -assembly $ASSEMBLY -experiment_name $exp_name -cell_type $cell_type -feature_type $feature_type $job_topup"

	echo $cmd

	Execute perl $cmd

	if [ $? == 0 ]; then 
	    IS_SETUP=1
	else 
	    echo -e "An error occured while inserting job for $file.\nIt maybe necessary to DropPipelineDB alignments"
	fi

    done
    
    if [[ $IS_SETUP == 1 ]]; then
	echo "Ready for SubmitAlignments"
    else
	echo "Pipeline is empty"
    fi

}

SubmitAlignments(){	
    echo ":: SubmitAlignments $*"

    OPTIND=1
    logic_name=
    usage='usage: SubmitAlignments [ -h(elp) ]
Simply submits Alignment jobs defined by AddAlignmentDataSets';
    
    while getopts ":h" opt; do
	case $opt in 
	    h  ) echo -e $usage; return 0;;
            \? ) echo -e $usage; return 1;;
	esac
    done
    
    Submit -p ${USER}_alignments_${DB_NAME} $*

    #Add basic statistics to data tracking DB

    #Print Basic Statistics
    
    #echo "Check QC with GenerateQCReport"	  
    
}


# TODO Use replicates...
# This also 
AddPeakDataSets(){
    echo ":: AddPeakDataSets $*"

    OPTIND=1 	#This makes sure we reset the getopts index if we have used it previously
    analysis=
    input_dir=
    exp_regex="*.sam.gz"
    exp_name=
    skip=
    control="WCE"
    ctrl_file=
    group=
    usage="Usage:\tAddPeakDataSets -a(nalysis_name e.g. SWEMBL_R015) [ -I(nput dir) -r(egex e.g. 'CD4_H3.*gz' default is '*.sam.gz') -e(xperiment name e.g. AUTHOR_PMID default is parsed from -input) -s(kip control, by default control is used) -c(ontrol feature, by default WCE) -f(ile for control, if specific file needed to override) -g(roup name default is efg) ]"

    while getopts ":a:I:r:e:sc:f:g:h" opt; do
	case $opt in 
	    a  ) analysis=$OPTARG ;;
            I  ) input_dir=$OPTARG ;;
            r  ) exp_regex=$OPTARG ;;
            e  ) exp_name=$OPTARG ;;
	    s  ) skip="-skip_control 1" ;;
            c  ) control=$OPTARG ;;
            f  ) ctrl_file=$OPTARG ;;
            g  ) group="-group $OPTARG" ;;
	    h  ) echo -e $usage; return 0;;
	    \? ) echo -e $usage; return 1;;
	esac 
    done


    #Make this more generic...
    PDB_NAME=${USER}_peaks_${DB_NAME}

    IS_SETUP=0
    exists=$(echo "show databases like '$PDB_NAME'" | mysql -h$DB_HOST -u$DB_USER -P$DB_PORT -p$DB_PASS)
    if [[ $exists ]]; then
	echo "Pipeline DB $PDB_NAME already exists: If you wish to start from scratch use DropPipelineDB peaks"
        #Add option to continue or stop here...
	AskQuestion "Do you wish to continue using the existing DB $PDB_NAME ? [y|n]"
	if [[ $REPLY != [yY]* ]]; then
	    echo "Aborting"
	    return 0
	fi
	IS_SETUP=1
    fi

    CheckVariables analysis 
    ValidateVariable analysis VALID_PEAK_ANALYSIS
	
    CheckVariables input_dir
    CheckDirs $input_dir

    #Set default exp_suffix based on input dir
    if [ ! "$exp_name" ]; then
	exp_name=$(echo $input_dir |sed 's/\/$//') # Strip trailing /
	exp_name=$(echo $exp_name |sed 's/.*\///')
	echo  -e "Setting experiment name to $exp_name"
    fi

    
    # now go through the folder and add as many jobs as required... 
    files=$(ls ${input_dir}/${exp_regex})
    if [ ! "$files" ]; then
	echo "error : could not find any files"
	return 1
    fi

    #Remember the generic global parameters...
    tmp_analysis=$analysis
    tmp_skip=$skip

    for file in $files; do

	#Remove folder names...
	file=$(echo $file |sed 's/.*\///')

	#If not zipped, zip it
	is_compressed=$(isCompressedFile $file)
	if [ ! $is_compressed ]; then 
	    gzip $file 
	fi

	#extract the file type, the cell type and feature type from the file name
	#This is doing many strict assumptions on file name structure...
	#Could instead get these as parameters... though that would require adding one set at a time...
	cell_type=$(echo $file | perl -pe 's/_.*$//') #sed 's/_.*//')
	feature_type=$(echo $file | perl -pe 's/^.*?_//;s/_.*$//') #sed 's/^.*_//' | sed 's/\..*$//')
	file_type=$(echo $file | perl -pe 's/^.*\.([^\.]*)\..*$/$1/')
	
	#Some warnings just to avoid forgetting!!
	if [[ $feature_type == 'DNase1' || $feature_type == 'FAIRE' ]]; then
	    if [[ $analysis != 'SWEMBL_R0025' ]]; then
		#Ask Question
		echo "Your feature type is DNase1 or FAIRE but you're using $analysis instead of SWEMBL_R0025"
		AskQuestion "Do you wish to continue using it (no will swap to SWEMBL_R0025) ? [y|n]"
		if [[ $REPLY != [yY]* ]]; then
		    analysis='SWEMBL_R0025'
		fi
	    fi
	    if [[ ! $skip ]]; then
		#Ask Question
		echo "Your feature type is DNase1 or FAIRE but you're not skipping control"
		AskQuestion "Do you wish to continue using control for this dataset ? [y|n]"
		if [[ $REPLY != [yY]* ]]; then
		    skip='-skip_control 1'
		fi
	    fi
	fi

	#Do not process control features...
	#TODO Make Control Features an Environment Variable...
	#The reverse of ValidateVariables ...
	#if [[ $feature_type == 'WCE' || $feature_type == 'GFP' ]]; then
	if [[ $feature_type == $control ]]; then
	    echo "Your feature type $feature_type seems to be a control feature"
	    #AskQuestion "Do you wish to call peaks on it (no will ignore it) ? [y|n]"
	    #if [[ $REPLY != [yY]* ]]; then
	    echo "Skipping $feature_type"
		continue   
	    #fi
	fi

	#ValidateVariable file_type VALID_FILE_TYPES
	#Maybe do validation here for cell type and feature type, but there may be many...
	
	job_topup=
	if [[ $IS_SETUP == 1 ]]; then
	    job_topup=' -job_topup'
	fi

	ctrl_cmd=
	if [[ $ctrl_file ]]; then
	    ctrl_cmd="-control_file $ctrl_file"
	else
	    ctrl_cmd="-control_feature $control"
	fi

	echo "Entering peaks job for Experiment: $exp_name ; Cell Type: $cell_type ; Feature Type: $feature_type ; File Type: $file_type"
	cmd="$PIPELINE_SCRIPTS/init_pipeline.pl Bio::EnsEMBL::Funcgen::HiveConfig::Peaks_conf $DNADB_SCRIPT_ARGS $DB_SCRIPT_ARGS -pipedb_name $PDB_NAME -analysis_name $analysis -bin_dir $BIN_DIR -work_dir $DATA_HOME -output_dir $WORK_DIR -species $SPECIES -assembly $ASSEMBLY -experiment_name $exp_name -cell_type $cell_type -feature_type $feature_type -file_type $file_type $skip $ctrl_cmd $group $job_topup"
	echo $cmd
	Execute perl $cmd


	if [ $? == 0 ]; then 
	    IS_SETUP=1
	else 
	    echo "An error occured while inserting job for $file"
	fi

	#Go back to the generic global parameters
	skip=$tmp_skip
	analysis=$tmp_analysis

    done
    
    if [[ $IS_SETUP == 1 ]]; then
	echo "Ready for SubmitPeaks"
    else
	echo "Pipeline is empty"
    fi

    
}

SubmitPeaks(){	
    echo ":: SubmitPeaks $*"

    OPTIND=1
    logic_name=
    usage='usage: SubmitPeaks [ -l(ogic_name e.g. SWEmbl) ] [ -h(elp) ]
Simply submits the Peak jobs defined by CreateInputIDs';
    
    while getopts ":l:h:" opt; do
	case $opt in 
	    h  ) echo -e $usage; return 0;;
            \? ) echo -e $usage; return 1;;
	esac
    done
    
    Submit -p ${USER}_peaks_${DB_NAME} $*
    
    echo "Now do PeaksReport when finished"	  
    
}

#Generic submit that can be used for many of the sub pipelines
#Maybe all pipelines should be made into one?
Submit(){	
    echo ":: Submit $*"
 
    OPTIND=1
    logic_name=
    PDB_NAME=
    while getopts ":p:l" opt; do
	case $opt in 
	    p ) PDB_NAME=$OPTARG ;;
	    l ) logic_name=$OPTARG ;;
	esac 
    done

    if [ "$logic_name" ]; then
	logic_name=" -logic_name $logic_name"	
    fi

    cmd="$PIPELINE_SCRIPTS/beekeeper.pl -url mysql://${DB_USER}:${DB_PASS}@${DB_HOST}:${DB_PORT}/${PDB_NAME} -sync"
    echo $cmd
    Execute $cmd  2>&1 | tee -a $WORK_DIR/sequencing.${PDB_NAME}.log

    cmd="$PIPELINE_SCRIPTS/beekeeper.pl -url mysql://${DB_USER}:${DB_PASS}@${DB_HOST}:${DB_PORT}/${PDB_NAME} $logic_name -loop -sleep 2"
    Execute $cmd  2>&1 | tee -a $WORK_DIR/sequencing.${PDB_NAME}.log

}

FilterBlacklist(){
	echo ":: FilterBlacklist $*"
	
	usage='usage: FilterBlacklist [ -f(eature set)  name ]+ [-h help]'

	OPTIND=1
	sets=
	while getopts ":f:h" opt; do
	    set=
	    case $opt in 
		f  ) set=$OPTARG;;
	        \? ) echo $usage; exit 1;;
	        h  ) echo $usage; return 0;;
	     esac 
	     
	     if [[ $set ]]; then
		 sets="$sets $set"
	     fi

	done		
	
	report_file=${WORK_DIR}/blacklist_${DB_NAME}.report
	echo ": Generating Report: $report_file"	
	BackUpFile $report_file

	if [[ ! "$sets" ]]; then
	    sets="-all "
	else
	    sets="-feature_sets $sets"
	fi

	cmd="perl $EFG_SRC/scripts/miscellaneous/check_overlaps_with_encode_blacklist.pl $DB_SCRIPT_ARGS $DNADB_SCRIPT_ARGS -remove $sets -output $report_file"
	echo $cmd
	Execute $cmd 

}


PeaksReport(){
	echo ":: PeaksReport $*"
	
	usage='usage: PeaksReport [-i(nfer sets by default reports all in the efg db)] [-h help]'

	OPTIND=1
	infer=
	while getopts "ih" opt; do
	    case $opt in 
		i  ) infer="1";;
	        \? ) echo $usage; exit 1;;
	        h  ) echo $usage; return 0;;
	     esac 
	done		

	sets=
	if [[ "$infer" ]]; then
	    #infer set names from peaks pipeline output to avoid printing all sets (can be many!)
	    for analysis in $VALID_PEAK_ANALYSIS; do 
		#Some assumptions
		asets=$(ls $WORK_DIR/peaks/results/* | grep -i $analysis | sed 's/^.*\///' | perl -pe "s/.samse.sam.gz.${analysis}\S*/_${analysis}/i" | uniq)
		sets="$sets $asets"
	    done 
	    sets="-feature_sets $sets"
	fi
	
	name=peaks_${DB_NAME}.report
	report_file=${WORK_DIR}/${name}.pdf
	echo ": Generating Report: $report_file"	
	BackUpFile $report_file

	cmd="perl $EFG_SRC/scripts/peaks_report.pl $DB_READ_SCRIPT_ARGS $DNADB_SCRIPT_ARGS -R -compare -feature_table annotated -no_outliers -name $name -outdir $WORK_DIR $sets"
	echo $cmd
	Execute $cmd 

}


ImportPWM(){
	echo ":: ImportPWM $*"

	usage='usage: ImportPWM -j(aspar matrix e.g. MA0139.1) -f(eature type e.g. CTCF) [-d(irectory for Jaspar data file matrix_only.txt default is $DATA_HOME/binding_matrices/Jaspar] [-h help]'

	folder=${DATA_HOME}/binding_matrices/Jaspar
	jaspar=
	feature=
	OPTIND=1

	while getopts ":j:f:d:h" opt; do
	    case $opt in 
		j  ) jaspar=$OPTARG;;
                f  ) feature=$OPTARG;;
		d  ) folder=$OPTARG;;
	        \? ) echo $usage; exit 1;;
	        h  ) echo $usage; return 0;;
	     esac 
	done		
	
	if [[ ! $jaspar || ! $feature ]]; then
	    echo "Need matrix and feature type"
	    return 0
	fi

	cmd="perl $EFG_SRC/scripts/import/import_matrix.pl $DB_SCRIPT_ARGS $DNADB_SCRIPT_ARGS -species $SPECIES -matrix_id $jaspar -feature_type $feature -matrix_data_dir $folder"
	echo $cmd
	Execute $cmd 

}


RunMotifMatch(){
	echo ":: RunMotifMatch $*"

	usage='usage: RunMotifMatch [-f(eature type) e.g. Max]+ [-h help]'

	folder=${DATA_DIR}/binding_matrices/Jaspar
	feature_list=
	OPTIND=1

	while getopts ":f:h" opt; do
	    feature=
	    case $opt in 
                f  ) feature=$OPTARG;;
	        \? ) echo $usage; exit 1;;
	        h  ) echo $usage; return 0;;
	     esac 
	     
	     if [[ $feature ]]; then
		 feature_list="$feature_list $feature"
	     fi
     
	done		
	
	map_dir=$WORK_DIR/pwm_mappings
	Execute "mkdir -p $map_dir"

	if [[ $feature_list ]]; then
	    feature_list="-feature_type_list $feature_list"
	fi
	     
	#At the moment this needs to be run in a specific folder. TODO update the scripts
	cd $map_dir

  CheckGlobalVariables SCHEMA_BUILD
	
	#TODO extra job management...
	bsub -J pwm_mappings_${DB_NAME} -e $map_dir/pwm_mappings_${DB_NAME}.err -o $map_dir/pwm_mappings_${DB_NAME}.out \
    -q long -M35000 -R"select[mem>35000] rusage[mem=35000]" \
    perl $EFG_SRC/scripts/pwm_mappings/run_pwm_mapping_pipeline.pl $DB_SCRIPT_ARGS $DNADB_SCRIPT_ARGS \
    -species $SPECIES -assembly $ASSEMBLY -workdir $DATA_HOME -outputdir $map_dir $feature_list
	#echo $cmd
	#Execute  $cmd 

	echo "Now wait for the job to finish and check logs and output"

}


ImportMotifMatch(){
	echo ":: ImportMotifMatch $*"

	usage="usage: ImportMotifMatch [-s(slice) e.g. 22]+ [-h help]"
	#TODO do some tests when pipeline already exists... may need a DropPipeline before...

	slice_list=
	OPTIND=1

	while getopts ":s:h" opt; do
	    slice=
	    case $opt in 
                s  ) slice=$OPTARG;;
	        \? ) echo $usage; exit 1;;
	        h  ) echo $usage; return 0;;
	     esac 
	     
	     if [[ $slice ]]; then
		 slice_list="$slice_list $slice"
	     fi
     
	done		


        #Make this more generic...
	export PDB_NAME=${USER}_motif_import_${DB_NAME}

	exists=$(echo "show databases like '$PDB_NAME'" | mysql -h$DB_HOST -u$DB_READ_USER -P$DB_PORT)
	if [[ $exists ]]; then
	    echo "Pipeline DB $PDB_NAME already exists: Need to start from scratch. Use DropPipelineDB motif_import"
	    echo "Aborting"
	    return 0
	fi
	
	if [[ $slice_list ]]; then
	    slice_list="-slices $slice_list"
	fi

	map_dir="$WORK_DIR/pwm_mappings/"
    
	cmd="perl $EFG_SRC/scripts/pwm_mappings/run_binding_site_import_pipeline.pl $DB_SCRIPT_ARGS $DNADB_SCRIPT_ARGS -workdir $map_dir -output_dir $WORK_DIR $slice_list"
	echo $cmd
	Execute $cmd 

	cmd="beekeeper.pl -url mysql://${DB_USER}:${DB_PASS}@${DB_HOST}:${DB_PORT}/${PDB_NAME} -sync"
	Execute $cmd

	#Cannot use Execute in case we want to interrupt it
	echo "beekeeper.pl -url mysql://${DB_USER}:${DB_PASS}@${DB_HOST}:${DB_PORT}/${PDB_NAME} -loop"
	beekeeper.pl -url mysql://${DB_USER}:${DB_PASS}@${DB_HOST}:${DB_PORT}/${USER}_motif_import_${DB_NAME} -loop

	echo "Now check results in $WORK_DIR/motif_features/results"

}

InferPWM(){
	echo ":: InferPWM $*"

	usage="usage: InferPWM [-a(nalysis to consider) by default all in $VALID_PEAK_ANALYSIS]+ [-h help]"

	OPTIND=1

	analist=
	while getopts ":a:h" opt; do
	    slice=
	    case $opt in
		#TODO verify if entered analysis is valid...
		a  ) analist="$analist $OPTARG";;
	        \? ) echo $usage; exit 1;;
	        h  ) echo $usage; return 0;;
	     esac 	     
	done		

	if [[ ! $analist ]]; then
	    analist=$VALID_PEAK_ANALYSIS
	fi
        
        #Make this more generic...
	PDB_NAME=${USER}_motif_finder_${DB_NAME}

	IS_SETUP=0
	exists=$(echo "show databases like '$PDB_NAME'" | mysql -h$DB_HOST -u$DB_READ_USER -P$DB_PORT)
	if [[ $exists ]]; then
	    echo "Reusing existing DB $PDB_NAME. Use DropPipelineDB to start from scratch"
	    IS_SETUP=1
	fi
	
	dir=$WORK_DIR/peaks/results
	for analysis in $analist; do 
	    #Some assumptions here
	    asets=$(ls $dir/* | grep -i $analysis | sed 's/^.*\///' | perl -pe "s/.samse.sam.gz.${analysis}\S*/_${analysis}/i" | uniq)

	    for fset in $asets; do

		job_topup=
		if [[ $IS_SETUP == 1 ]]; then
		    job_topup=' -job_topup'
		fi

		cmd="$PIPELINE_SCRIPTS/init_pipeline.pl Bio::EnsEMBL::Funcgen::HiveConfig::MotifFinder_conf -work_dir $WORK_DIR $DB_READ_SCRIPT_ARGS $DNADB_SCRIPT_ARGS -species $SPECIES -pipeuser $DB_USER -pipepass $DB_PASS -pipedb_name $PDB_NAME -feature_set $fset $job_topup"
		Execute perl $cmd
		
		if [ $? == 0 ]; then 
		    IS_SETUP=1
		fi
		
	    done

	done 

	if [[ $IS_SETUP == 0 ]]; then
	    echo "No jobs created"
	    return 1
	fi
	
	cmd="$PIPELINE_SCRIPTS/beekeeper.pl -url mysql://${DB_USER}:${DB_PASS}@${DB_HOST}:${DB_PORT}/${PDB_NAME} -sync"
	Execute perl $cmd

	cmd="$PIPELINE_SCRIPTS/beekeeper.pl -url mysql://${DB_USER}:${DB_PASS}@${DB_HOST}:${DB_PORT}/${PDB_NAME} $logic_name -loop -sleep 15"
	Execute $cmd  2>&1 | tee -a $WORK_DIR/motif_inference.${PDB_NAME}.log

	echo "Now check results in $WORK_DIR/motifs/results"

}


SetupReadCounts(){
    echo ":: SetupReadCounts $*"

    usage="usage: SetupReadCounts [-c(control feature type) - by default 'WCE' is already considered]+ [-d(ata root folder) by default /nfs/ensembl_data/${DB_NAME}] [-h help]"
    #TODO maybe add option to do just certain slices? [-s(slice) e.g. 22]+
    #TODO do some tests when pipeline already exists... may need a DropPipeline before...
	#TODO restrict to one set
    
    data_root=/nfs/ensembl_data/${DB_NAME}
	#This is now out of date
	#We currently have a mismatch between outdir and dbfile_data_dir
	#e.g. MSG: ResultSet dbfile_data_dir(/nfs/ensnfs-dev/staging/homo_sapiens_funcgen_65_37//result_feature/GM12878_H3K79me2_ENCODE_Broad) and -output_dir(/lustre/scratch103/ensembl/funcgen//output/dev_homo_sapiens_funcgen_66_37/SEQUENCING/GM12878_H3K79me2_ENCODE_Broad)


    controls='WCE'
    while getopts ":c:d:h" opt; do
	slice=
	case $opt in 
	    c  ) controls="$controls $OPTARG";;
	    d  ) data_root=$OPTARG;;	    
            \? ) echo $usage; exit 1;;
            h  ) echo $usage; return 0;;
        esac 
    done

    exists=$(echo "select meta_value from meta where meta_key='dbfile.data_root'" | mysql -h$DB_HOST -u$DB_READ_USER -P$DB_PORT $DB_NAME)
    if [[ ! $exists ]]; then
	AskQuestion "Will create meta key dbfile.data_root with $data_root. Do you wish to continue ? [y|n]"
	if [[ $REPLY != [yY]* ]]; then
	    echo "Aborting: You need to DropPipelineDB -p read_counts"
	    return 0
	fi
	echo "insert into meta (species_id, meta_key, meta_value) values (1,'dbfile.data_root','$data_root')" | mysql -h$DB_HOST -u$DB_USER -P$DB_PORT -p$DB_PASS $DB_NAME
	echo "Meta key updated"
    else

		AskQuestion "Do you want to update meta key dbfile.data_root with $data_root ? [y|n]"
		
		#what is the consequence here?

	if [[ $REPLY != [yY]* ]]; then
	    echo "Skipping dbfile.data_root update"
	else
	    echo "update meta set meta_value='$data_root'  where meta_key='dbfile.data_root'" | mysql -h$DB_HOST -u$DB_USER -P$DB_PORT -p$DB_PASS $DB_NAME		
	    echo "Meta key updated"
	fi
    fi

    PDB_NAME=${USER}_read_counts_${DB_NAME}
    
    dir=$WORK_DIR/peaks/results
    folders=$(ls $dir)

    IS_SETUP=0
    exists=$(echo "show databases like '$PDB_NAME'" | mysql -h$DB_HOST -u$DB_USER -P$DB_PORT -p$DB_PASS)
    
	if [[ $exists ]]; then
		echo "Pipeline DB $PDB_NAME already exists: If you wish to start from scratch use DropPipelineDB -p read_counts"
        #Add option to continue or stop here...
		AskQuestion "Do you wish to continue using the existing DB $PDB_NAME ? [y|n]"

		if [[ $REPLY != [yY]* ]]; then
			echo "Aborting"
			return 0
		fi

		IS_SETUP=1
    fi

    for experiment in $folders; do
		#An assumption... 
	    #Will warn if files not present
	    #Assumes file have not already been converted and removed!
		
	#Let's change this to look for unique prefixes, then check for bed or else sam and convert
	echo "Getting experiments..."
	prefixes=$(ls $dir/${experiment}/*.samse.* | grep -i $experiment | sed 's/.samse.*//' | sort | uniq)
		
	for prefix in $prefixes; do

	    prefix=$(echo $prefix | sed 's/^.*\///')
	    #Some more assumptions
	    ctype=$(echo $prefix | sed 's/_.*$//')
	    ftype=$(echo $prefix | perl -pe 's/^[^_]+_//' | sed 's/_.*$//')

	    if [[ ! $ctype || ! $ftype ]]; then
			echo -e "SKIPPING:\tCould not idenftify cell/feature type for $prefix"
			continue
	    fi
	    
		
	    is_control=
	    for control in $controls; do
		if [ $ftype = $control ]; then 
		    is_control=1
		fi
	    done
	
	    if [ $is_control ]; then 
			echo "SKIPPING:\tFound control feature type $ftype"
			continue   
	    fi
	
            # Convert sam to bed if necessary for the read counts pipeline
	    if [ ! -e $dir/${experiment}/${prefix}.samse.bed.gz ]; then

			if [ ! -e $dir/${experiment}/${prefix}.samse.sam.gz ]; then
				echo -e "SKIPPING:\tFailed to find bed or sam file for $prefix"
				continue
			fi
	   
			echo "Converting ${prefix}.samse.sam.gz to BED"
		    #Use bam to bed from BedTools instead?
			Execute perl "${EFG_SRC}/scripts/miscellaneous/sam2bed.pl -on_farm -files $dir/${experiment}/${prefix}.samse.sam.gz"
	    fi
		
	    job_topup=
	    if [[ $IS_SETUP == 1 ]]; then
		job_topup=' -job_topup'
	    fi
    

		#output dir here is base output_dir
		#Importer is then cuilding the incorrect output dir
		#Need to override this to output to result_feature

	    BUILD=$(echo $SCHEMA_BUILD | sed 's/^.*_//')


		#Some of these can be moved further up this function/env if they are defined and used beforehand
		#All variables should be checked before exectuting on command line 
		CheckVariables DB_HOST DB_PORT DB_USER DB_PASS DB_NAME PDB_NAME BUILD dir DATA_HOME WORK_DIR SPECIES ctype ftype prefix REGISTRY_VERSION REGISTRY_HOST experiment


	    echo "Entering read_counts job for Experiment: $experiment ; Cell Type: $ctype ; Feature Type: $ftype"
	    cmd="$PIPELINE_SCRIPTS/init_pipeline.pl Bio::EnsEMBL::Funcgen::HiveConfig::Import_conf -host $DB_HOST -port $DB_PORT -user $DB_USER -pass $DB_PASS -dbname $DB_NAME -pipedb_name $PDB_NAME -assembly $BUILD -input_dir $dir -data_dir $DATA_HOME -output_dir $WORK_DIR -species $SPECIES -cell_type $ctype -feature_type $ftype -input_set $prefix -registry_version $REGISTRY_VERSION -registry_host $REGISTRY_HOST -result_file $dir/${experiment}/${prefix}.samse.bed.gz $job_topup -dnadb_host $DNADB_HOST -dnadb_port $DNADB_PORT -dnadb_user $DNADB_USER -dnadb_name $DNADB_NAME"
	    echo $cmd
	    Execute perl $cmd
	    
	    if [ $? == 0 ]; then 
		IS_SETUP=1
	    fi

	done
	
    done

    echo "Wait for potential file conversion jobs to finish correctly before submitting the pipeline with SubmitReadCounts"

}
	
SubmitReadCounts(){	
    echo ":: SubmitReadCounts $*"

    OPTIND=1
    logic_name=
    usage='usage: SubmitReadCounts [ -h(elp) ]
Simply submits Read Count jobs defined by SetupReadCounts';
    
    while getopts ":h" opt; do
	case $opt in 
	    h  ) echo -e $usage; return 0;;
            \? ) echo -e $usage; return 1;;
	esac
    done
    
    Submit -p ${USER}_read_counts_${DB_NAME} $*

}


GetFailedJobs(){
  #options to get info without fail not retry, just report jobs which have retried without reachign FAILED state
  #would need to accomodate relations ship between retry/retry_count and current jhob status
  #job_message.is_error can be used as proxy for FAILED attempt (i.e. not 'FAIL_NO_RETRY')
  #look at procedures?
  #job_search - contains nice join to analysis_data. (takes quite a long time as uses like)

  #add ability to get running jobs aswell as failed?	


  OPTIND=1
  type=
  analysis=
  limit=
  retry=
  long=
  job_id=
  debug=0
  usage='usage: GetFailedReadJobs -t(ype) e.g. reads [ -l(ong format) -a(nalysis) logic_name -r(retry) COUNT -j(ob_id) ID -L(imit) NUMBER -h(elp) ]'

  fields='j.job_id, j.retry_count, a.logic_name, jm.msg, jf.path'
  long_fields='jm.status as status_at_failure, w.process_id as lsf_id, jm.time, j.input_id'
  #Could do with left joining to analysis_data for long format

  while getopts ":t:la:r:j:L:dh" opt; do
	  case $opt in
		  t  ) type=$OPTARG ;;
		  l  ) fields="${fields}, $long_fields" ;;
          a  ) analysis="and a.logic_name='${OPTARG}'" ;;
          r  ) retry="and j.retry_count=${OPTARG}" ;;
          j  ) job_id="and j.job_id=${OPTARG} " ;;
          L  ) limit=" LIMIT $OPTARG";;
		  d  ) debug=1 ;;
		  h  ) echo -e $usage; return 0;;
          \? ) echo -e $usage; return 1;;
	esac
  done

  #Should validate type against valid hive types here

  #could LPAD some of these to make the output nicer
  #can we printf in sql
  #only list the err files to avoid redundancy. Assume people can sub err for out manually
  sql="select $fields from job j, analysis a, job_message jm, job_file jf, worker w where j.status='FAILED' and jm.worker_id=w.worker_id and j.analysis_id=a.analysis_id and j.job_id=jm.job_id and j.job_id=jf.job_id and j.retry_count=(jf.retry+1) and j.retry_count=(jm.retry+1) and jf.path like '%.err' $job_id $analysis $retry $limit"

  if [[ $debug = 1 ]]; then
	  echo $sql
  fi

  #eval required as alias not available in function
  echo $sql | eval "mysql${type}pipe"
}


#This needs to be redefined here, since it's not the same as the old pipeline (which we are not using anyway) 
#This actually drops the whole DB
DropPipelineDB(){
    echo ":: DropPipelineDB $*"

    OPTIND=1
    logic_name=
    pipeline=
    usage='usage: DropPipelineDB -p(ipeline) pipeline (alignments | peaks | motif_import | read_counts | motif_finder) [ -h(elp) ]';
    
    while getopts ":p:h" opt; do
	case $opt in 
	    p  ) pipeline=$OPTARG ;;
	    h  ) echo -e $usage; return 0;;
            \? ) echo -e $usage; return 1;;
	esac 
    done

    if [ ! "$pipeline" ]; then
	echo -e $usage; return 1;	
    fi

    PDB_NAME=${USER}_${pipeline}_${DB_NAME}
    exists=$(echo "show databases like '$PDB_NAME'" | mysql -h$DB_HOST -u$DB_USER -P$DB_PORT -p$DB_PASS)

    if [[ ! $exists ]]; then
	echo "Pipeline DB $PDB_NAME does not exist"
	return 0
    fi

    AskQuestion "Drop the database $PDB_NAME ? [y|n]"
    if [[ $REPLY != [yY]* ]]; then
	echo "Aborting"
	return
    fi	

    echo ":: Dropping pipeline database: $PDB_NAME"
    echo "drop database $PDB_NAME" | mysql -h$DB_HOST -u$DB_USER -P$DB_PORT -p$DB_PASS
    
    if [ $? == 0 ]; then 
	export IS_SETUP=0
    else 
	echo "Could not drop Database $PDB_NAME"
	return 1
    fi

}

# This should be moved back to the EFG environment??
RollbackSet(){
	echo ":: RollbackSet $*"

	force=
	full=
	
	#This makes sure we reset the getopts ind if we have used it previously
	OPTIND=1

	usage='usage: RollbackSet -d(force delete) -f(ull delete, otherwise just removes features and status) -h(elp) ] [Set_names]+';

	while getopts ":d:f:h" opt; do
	    case $opt in 
                d  ) force=' -force_delete ' ;;
                f  ) full=' -full_delete ' ;;
                h  ) echo $usage; return 0;;
                \? ) echo $usage; return 1;;
	    esac 
	done


        #Get trailing file name arguments
	idx=1
	
	while [[ $idx -lt $OPTIND ]]; do 
	    idx=$(($idx + 1))
	    shift 
	done
	
	set_names=($*)

	for set in $set_names; do
	    
	    #Maybe validate set name before trying to delete it?
	    cmd="$EFG_SRC/scripts/rollback/rollback_experiment.pl $DB_SCRIPT_ARGS $DNADB_SCRIPT_ARGS -species $SPECIES  -feature_set $set $force $full"
	    echo $cmd
	    Execute perl $cmd 
		
	done

}
