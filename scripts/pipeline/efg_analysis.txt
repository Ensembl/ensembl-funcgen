This document wraps up how to configure and run the Ensembl Functional Genomics (eFG) 
analysis pipeline.

Prerequisites
=============

Before you can start you need to make sure you have installed the following 
Ensembl API code:

ensembl
ensembl-functgenomics
ensembl-pipeline
ensembl-analysis

as well as

bioperl-live (bioperl-release-1-2-3)

For details please see http://www.ensembl.org/info/using/api/index.html


Background
==========

The Ensembl Functional Genomics (eFG) analysis pipeline is based on the 
pipeline system that has been developed for the Ensembl Genebuild. The 
documents overview.txt, quick_start_pipeline_guide, 
the_ensembl_pipeline_infrastructure.txt, and custom_analyses.txt 
in ensembl-doc/pipeline_docs give a good overview.

At the bottom of this document are a list of utlity methods which may come in 
useful if you go wrong.  Take a brief look at these first before running the configuration 
steps.

Setup/Configuration
===================

To get started first you need to fill out a couple of configuration files.


1. Global shell environment
---------------------------

The eFG analysis pipeline system uses various environmental variables
and functions to facilitate common tasks. Some of the functions we will 
refer to later on are defined within this environment file:

   $SRC/ensembl-functgenomics/scripts/pipeline/.efg_pipeline


This file contains common settings and functions utilised by the pipeline
environment and hence does not usually need to be edited. A second config 
file is used to define project specific settings, this is used to source the
pipeline environment as follows: 

  . $SRC/ensembl-functgenomics/scripts/pipeline/human_50.env

Such a file has to be set up and maintained according to your project. To ease
the access to the environment it is handy to set up an alias in your shell
configuration like follows (in bash syntax).

alias eFG_human='. $SRC/ensembl-functgenomics/scripts/pipeline/human_50.env'


2. Configuration of the eFG runnables
-------------------------------------

All the configuration for the eFG analysis runnables can be found at

        ensembl-analysis/modules/Bio/EnsEMBL/Analysis/Config/Funcgen

This directory contains all the example config modules (.pm.example) for the
analysis that are currently implemented:

        Nessie (Flicek et al., unpublished)
        TileMap (Ji and Wong (2005), Bioinformatics)
        Chipotle
        ACME
        Splitter
        SWEmbl (Wilder et al., unpublished)
                
Rename the config files for the analysis you are going to use so that it 
ends on ".pm" and edit this file if required. The environment variables used in
the config files should should already been set in your efg_pipeline environment
and should not need to be edited, with the exception of the EFGDB password, which
will need to be harcoded (This is to be removed shortly and configured via the cmdline)



3. Configuration of the analysis pipeline
-----------------------------------------

To configure the pipeline system you need to rename and edit the following two
files:

    ensembl-pipeline/modules/Bio/EnsEMBL/Pipeline/Config/General.pm.example
    ensembl-pipeline/modules/Bio/EnsEMBL/Pipeline/Config/BatchQueue.pm.Example

For details see the documentation in ensembl-doc. To help especially filling
in the BatchQueue.pm module there is a script called write_pipeline_config.pl
that prints a config hash into a file (see section 4. for further details). For eFG
purposes the variables you must set as follows:


DEFAULT_OUTPUT_DIR => $ENV{'ANALYSIS_WORK_DIR'}
DEFAULT_BATCH_QUEUE => 'your queue name'
DEFAULT_RETRY_QUEUE => 'your retry queue name'


Or is you intend to run the pipeline locally, you can disregard the above QUEUE 
variables and set the following:

QUEUE_MANAGER       => 'Local',

BatchQueue.pm will also need the relevant analysis config hashes adding, these are 
autogenerated and detailed below.

4. Setting up pipeline tables
-----------------------------

The eFG analysis pipeline system uses a separate pipeline database to control
the pipeline and to avoid messing around with the actual eFG database analysis 
table (!!! maybe after the most recent changes we can even drop this step and 
do everything on the eFG database directly!!!). 

Therefore we first need to create this pipeline database with the command

    CreatePipelineDB <password>

which has been defined in the global shell environment configuration file. It 
1.) creates the pipeline database, 2.) creates the pipeline tables, and 3.) adds 
schema version to meta table

The next step is to configure the pipeline. Hence we first need to generate the 
analysis and rules config (see the_ensembl_pipeline_infrastructure.txt) as well
as the pipeline BatchQueue config. To generate the necessary files the
perl script called 

     $SCRIPTSDIR/write_pipeline_config.pl 

can be used. This script reads in the analysis config hash of the runnable 
(Analysis/Config/Funcgen/<Runnable>.pm) that has already been defined in 
section 2. As options to the script you need to pass 

     -module <Runnable>  to specify the Runnable config to be used
 
     -slice|file         to set the SubmitType, i.e. SubmitSlice or SubmitFile
                         respectively. It indicates the type of input ids. Some 
                         Runnables need slices as input, like Nessie, others need
                         files as input, like SWEmbl

Examples:
     
     write_pipeline_config.pl -help
     write_pipeline_config.pl -module SWEmbl -file
     write_pipeline_config.pl -module Nessie -module TileMap -slice

The output is written to three files located in the $SCRIPTSDIR/conf directory:

    o BatchQueue.conf - contains some analysis-specific settings to the pipeline
      to be copied and pasted into the actual BatchQueue.pm in
      Bio/EnsEMBL/Pipeline/Config (here it needs to be added to the QUEUE_CONFIG 
      list of the Config hash). NOTE: It is important to remove or comment out any 
	  other analyses which maybe present as default and may not have been configured 
	  properly as these could be configured incorrectly and hence cause misdirecting
	  errors.         
    o analysis.conf - contains config to set up the analysis table and will be 
      read when setting up pipeline tables below
    o rules.conf - contains config to set up the rules table and will be read
      when setting up pipeline tables below

A more detailed explanation about the pipeline config files and what the actual 
parameters are for you can find in Ensembl pipeline documentation at 
ensembl-doc/pipeline_docs/. 

Now the pipeline config can be imported by using the shell command

    ImportPipelineConfig <password>

which runs the three pipeline scripts analysis_setup.pl, rule_setup.pl, and 
setup_batchqueue_outputdir.pl.



Finally we needs to create input_ids. In eFG analysis pipeline terms an
input_id consists of an experiment name and a genomic slice or path to an input 
file name concatenated by a colon, i.e.

    GM12878_FAIRE_ENCODE:chromosome:NCBI36:2:234156564:234656627:1

or 

    CD4_H3K4ac:CD4_H3K4ac.bed.gz

or    

    MEF_H3K4me3_Mikkelsen2007:MEF_H3K4me3_le2m_reads.bed.gz

The shell function

    CreateInputIds <password> <SubmitType> [<exp_regex> <exp_suffix>]

allows for doing this, where <SubmitType> can be either

    Slice <'encode'|'toplevel'>

to create Encode slices or toplevel slices as input_ids, or

    File <directory>

to use the gzipped bed files in a given <directory> to create the input_ids.

Behind the scenes it uses a perl script called create_input_ids.pl that either fetches
the slices from the database or reads the files, concatenates them with the 
experiment names (optionally selected by <exp_regex>), and imports them directly 
into the pipeline database input_id_analysis table. As <exp_regex> you can pass any Perl 
regular expression to match a certain subset of experiment names, i.e. '^CD4_' would create 
input_ids for all experiments that begin with 'CD4_' in the name (see examples below). The 
optional paramter <exp_suffix> allows you to specify an add an additional string to the experiment 
name that is subsequently store in the database. This option is to distinguish between sets/experiments 
that have been performed on the same feature and cell type. Note that if you only want to add
a suffix to the experiment name without filtering with <exp_regex> option, <exp_regex> needs to
be an empty sting ('', see example below).

In the case of files the experiment name needs to be encoded in the filename that should have 
the format 

    <cell_type>_<feature_type>[_<...>].bed.gz

Note that the infiles are expected to be gzipped and formatted according to the bed file 
format definitions with the first 6 fields being set: chr; start; end; name; score; 
strand. (see http://genome.ucsc.edu/FAQ/FAQformat)

Also you need to make sure that your eFG database knows about cell and feature types to 
be analysed and imported. You can use $SCRIPTSDIR/run_import_type.pl to import cell and 
feature types (See perl documentation of the script for details). 

Examples:

   CreateInputIds **** File $EFG_DATA/input/SOLEXA/LMI_methylation

creates for all gzipped bed files in $EFG_DATA/input/SOLEXA/LMI_methylation the corresponding input_id

   CreateInputIds **** File /lustre/work1/ensembl/graef/efg/input/SOLEXA/LMI_acetylation '^CD4_H3K.*'

creates for all gzipped bed files in .../SOLEXA/LMI_methylation that match '^CD4_H3K.*' the 
corresponding input_id

   CreateInputIds **** Slice encode 'CD4_DNAse'

creates for all experiments with 'CD4_DNAse' in the name input_ids using the Encode region

   CreateInputIds **** Slice toplevel 'PolII'

creates for all experiments with 'PolII' in the name input_ids using all toplevel slices

   CreateInputIds **** File /home/graef/data/Mikkelsen2007/alignments '' 'Mikkelsen2007'

creates for all gzipped bed files in given directory the corresponding input_id and appends 
the string 'Mikkelsen2007' to the experiment name comprising cell and feature type.

Once the input_ids have been imported into the pipeline database, check the input_id_analysis table
to make sure they look sensible. Now you're ready for running the analysis.

The shell function

    CleanInputIds <password>

allows you to delete ALL input_ids from the input_id_analysis table in the
pipeline database

After setting up and configuring the pipeline system 

    CheckPipelineSanity <password>

performs a series of sanity checks on your database. The underlying perl
script pipeline_sanity.pl is described in more detail in the Ensembl pipeline
infrastructure document (the_ensembl_pipeline_infrastructure.txt). 

Note (taken from the infrastructure document): "Once you have run the 
pipeline_sanity script and the only errors reported are acceptable (note that 
dummy analyses will generally fail the 'module compile' and 'entry in batchqueue' 
checks and this is fine), then you can consider starting the pipeline."

To test if everything has been set up you can use

    test_eFG_Runnable <password> <module> <logic_name> <experiment> <input_id> [<options>]

where
   
    <module> is the name of the runnable
    <logic_name> is the logic)_name of the analysis
    <experiment> is the name of the experiment to be analysed
    <input_id> is the slice of the genomic region to be analysed
    [<options>] are optional additional options 

Example:

 using a file as input_id_type:
   test_eFG_Runnable **** SWEmbl SWEmbl_default File CD4_H3K36me3:CD4_H3K36me3.bed.gz -write

 using a slice (here encode region) as input_id_type:
   test_eFG_Runnable **** Nessie Nessie_NG_g0100_s1M_d0150_p99 Slice GM12878_c-Myc_ENCODE:chromosome:NCBI36:5:141880151:142380150:1

This shell function calls behind the scenes the test_RunnableDB perl script of
the ensembl-pipeline API. Results are going to be written to the eFG database
unless you add further optional test_RunnableDB flags, like -nowrite, -check,
-input_id_type, or -verbose (for details see test_RunnableDB documentation).


Running the pipeline
====================

Once you have run the CheckPipelineSanity and test_eFG_Runnable functions
successfully you can consider starting the pipeline. This can be done with 

   RunAnalysis <password> [ all | <pattern> ]

You can pass the flag "all" if you want to run all input_ids against all
analyses or specify a <pattern> to select a subset of analysis logic_names to
be run.

Example:

  RunAnalysis **** 'Nessie_NG_g0100_.*'


Supervising the pipeline run (Errors and Output)
================================================

There are three shell functions that allow you to supervise and analyse a
pipeline run:

Monitor - reports an overview on the current status of the pipeline
GetFailedJobs - lists all failed jobs and the corresponding log files 
GetRunningJobs - lists all jobs that currently run

Logfiles are written to $ANALYSIS_WORK_DIR/$DBNAME/[0-9]/



Utility methods
===============


 o RemoveLock <password>

   Remove a pipeline lock from the pipeline DB should your pipeline fall over and refuse to restart.


 o CleanInputs <password>

   Cleans all input IDs from the pipeline DB should you need to reload them.


 o DropPipelineDB <password>

   Removes entire pipeline DB should you wish to start from fresh


Summary (Protocol of release 51 running SWEmbl on histone data sets)
====================================================================

set up pipeline environment

   eFG_pipeline 

import feature types

   run_import_type.pl -pass ****

added parameter setting in Bio::Ensembl::Analysis::Config::Funcgen::SWEmbl and
run 

   write_pipeline_config.pl -module SWEmbl -file

create pipeline database

   CreatePipelineDB ****

establish tables in pipeline database

   CreatePipelineDBTables ****

set up rules and analysis

   SetUpPipelineDBTables ****

generate input_ids

   CreateInputIds **** File /lustre/work1/ensembl/graef/efg/input/SOLEXA/LMI_methylation
   CreateInputIds **** File /lustre/work1/ensembl/graef/efg/input/SOLEXA/LMI_acetylation

kick-off analysis

   RunAnalysis **** File all





New transcript:

  01  eFG
  02  eFG_pipeline 
  03  CreateDB zhao_homo_sapiens_funcgen_51_36m ****
  04  CreatePipelineDB ****
  05  write_pipeline_config.pl -module SWEmbl -file -overwrite
  06  ImportPipelineConfig ****
  07  run_import_type.pl -pass ****
  08  CreateInputIds 
  09  CreateInputIds **** File /lustre/work1/ensembl/graef/efg/input/SOLEXA/LMI_methylation
  10  CreateInputIds **** File /lustre/work1/ensembl/graef/efg/input/SOLEXA/LMI_acetylation
  11  screen
  12  Run Analysis **** File
  
