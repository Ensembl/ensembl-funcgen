#!/usr/local/bin/bash

echo "Setting up the Ensembl Functional Genomics pipeline environment..." 

### ENV VARS ###

# Prompt
export PS1="[\
\[\033[31m\]\
$EFG_DBNAME\
\[\033[0m\] \W\
\[\033[0m\]]$ "

#Code/Data Directories
export SRC=${HOME}/ensembl-src                          #Root source code directory. EDIT
export EFG_SRC=$SRC/ensembl-functgenomics       #eFG source directory
export EFG_SQL=${EFG_SRC}/sql                   #eFG SQL
export EFG_MODULES=${EFG_SRC}/modules
export EFG_SCRIPTS=${EFG_SRC}/scripts

export ANALYSIS_WORK_DIR=$EFG_DATA/analysis     #Data working directory. EDIT

export ENSEMBL_SRC=$SRC/ensembl                 #ensembl source directory
export ENSEMBL_MODULES=${ENSEMBL_SRC}/modules
export ENSEMBL_SCRIPTS=${ENSEMBL_SRC}/scripts

export ANALYSIS_SRC=$SRC/ensembl-analysis       #ensembl-analysis source directory
export ANALYSIS_MODULES=${ANALYSIS_SRC}/modules
export ANALYSIS_SCRIPTS=${ANALYSIS_SRC}/scripts

export PIPELINE_SRC=$SRC/ensembl-pipeline       #ensembl-pipline source directory
export PIPELINE_MODULES=${PIPELINE_SRC}/modules
export PIPELINE_SCRIPTS=${PIPELINE_SRC}/scripts

export PATH="\
$SCRIPTSDIR:\
${EFG_SCRIPTS}:${EFG_SCRIPTS}/import:\
${PIPELINE_SCRIPTS}:\
${ANALYSIS_SCRIPTS}:\
${ENSEMBL_SCRIPTS}:\
$PATH" 

#Update PERL5LIB. EDIT add ensembl(core) etc. if required
export PERL5LIB="\
${ENSEMBL_MODULES}:\
${EFG_MODULES}:\
${PIPELINE_MODULES}:\
${ANALYSIS_MODULES}:\
${PIPELINE_SCRIPTS}:\
${HOME}/lib/perl:${HOME}/lib/site_perl:\
${SRC}/bioperl-live"

# add to PERL5LIB for DAS server set up
#${SRC}/Bio-Das-ProServer/lib:\

# eFG database connection parameters
export MYSQL_ARGS="-h${EFG_HOST} -P${EFG_PORT}"

export WRITE_DB_ARGS="-host ${EFG_HOST} -port ${EFG_PORT} -user ${EFG_WRITE_USER}"
export READ_DB_ARGS="-host ${EFG_HOST} -port ${EFG_PORT} -user ${EFG_READ_USER}"

#Default norm and analysis methods
export NORM_METHOD='VSN_GLOG'       # EDIT if required e.g. T.Biweight, Loess
export PEAK_METHOD='Nessie'         # EDIT if required e.g. TileMap, MPeak, Chipotle

### ALIASES ###
#single quotes enable dynamic updating of commands

alias efg='cd $EFG_SRC'
alias efgd='cd $EFG_DATA'
alias efgm='cd $EFG_SRC/modules/Bio/EnsEMBL/Funcgen'
alias mysqlw='mysql $MYSQL_ARGS -u${EFG_WRITE_USER}'
alias mysqlro='mysql $MYSQL_ARGS -u${EFG_READ_USER}'

alias workdir="cd $EFG_DATA"
alias scriptsdir="cd $SCRIPTSDIR"

#check for vars then suggest defaults or take stdin?

echo "Welcome to eFG!"
echo "  << ${EFG_DBNAME} @ ${EFG_HOST}:${EFG_PORT} >>"

### Analysis Pipeline DB ###
export PDBNAME=pipeline_${EFG_DBNAME}
export PDBHOST=${EFG_HOST}
export PDBPORT=${EFG_PORT}

### Functions ###

DropPipelineDB(){

    if [ $# -ne 1 ]; then
        echo "Usage: DropPipelineDB <password>"
        return
    fi

    PASS=$1
    shift
    
	echo "Droping pipeline database $PDBNAME ($MYSQL_ARGS -p$PASS)"
	echo "drop database $PDBNAME" | mysqlw -p$PASS

}

RemoveLock(){

    if [ $# -ne 1 ]; then
        echo "Usage: RemoveLock <password>"
        return
    fi

    PASS=$1
    shift
    
    echo "Removing pipeline lock from $MYSQL_ARGS $PDBNAME"
    echo "delete from meta where meta_key = 'pipeline.lock';" \
        | mysqlw -p$PASS $PDBNAME

}

CreatePipelineDB(){

    if [ $# -ne 1 ]; then
        echo "Usage: CreatePipelineDB <password>"
        return
    fi

    PASS=$1
    shift

	echo "Creating pipeline database $PDBNAME ($MYSQL_ARGS -p$PASS)"
	echo "create database $PDBNAME" | mysqlw -p$PASS
    
    echo "Creating pipeline tables"
    mysqlw -p$PASS $PDBNAME < $SRC/ensembl-pipeline/sql/efg.sql

    echo "Adding schema version $VERSION to database"
    echo "INSERT INTO meta (meta_key, meta_value) VALUES (\"schema_version\", \"$VERSION\");" \
        | mysqlw -p$PASS $PDBNAME

    echo
    echo "Don't forget to write the pipeline config (write_pipeline_config.pl)"
    echo "before you run ImportPipelineConfig!!!"
    echo

}

AddSchemaVersion(){

    if [ $# -ne 1 ]; then
        echo "Usage: AddSchemaVersion <password>"
        return
    fi
    
    PASS=$1
    shift

    echo "Adding schema version $VERSION to database $EFG_DBNAME"
    echo "INSERT INTO meta (meta_key, meta_value) VALUES (\"schema_version\", \"$VERSION\");" \
        | mysqlw -p$PASS $EFG_DBNAME

}

#
# before you can use ImportPipelineConfig we need to make sure we have generated  
# the analysis config file and the rules config file using write_pipline_config.pl.
# ./write_pipeline_config.pl -module Nessie -regexp 'Nessie_NG_' -slice
#

# make sure the directory for the config files exists
if [ ! -d $SCRIPTSDIR/conf ]; then 
    mkdir -p $SCRIPTSDIR/conf
fi
#
# config files generated by write_pipline_config.pl
export ANALYSIS_CONFIG=$SCRIPTSDIR/conf/analysis.conf
export RULES_CONFIG=$SCRIPTSDIR/conf/rules.conf
export BATCHQ_CONFIG=$SCRIPTSDIR/conf/BatchQueue.conf

# make sure the necessary pipeline script are executable
for f in analysis_setup.pl rule_setup.pl setup_batchqueue_outputdir.pl rulemanager.pl; do
    if [ ! -x ${PIPELINE_SCRIPTS}/$f ]; then
        chmod +x ${PIPELINE_SCRIPTS}/$f
    fi
done
    

ImportPipelineConfig (){


    if [ $# -ne 1 ]; then
        echo "Usage: ImportPipelineConfig <password>"
        return
    fi

    PASS=$1
    shift
    
    WRITE_ARGS=" -dbhost $EFG_HOST -dbport $EFG_PORT -dbuser $EFG_WRITE_USER -dbpass $PASS"
    
    echo "Setting up pipeline analysis" 
	analysis_setup.pl $WRITE_ARGS -dbname $PDBNAME -read -file $ANALYSIS_CONFIG
    if [ $? -ne 0 ]; then
        echo "Error: while running analysis_setup.pl.";
        return;
    fi
    
    echo "Setting up pipeline rules" 
	rule_setup.pl $WRITE_ARGS -dbname $PDBNAME -read -file $RULES_CONFIG
    if [ $? -ne 0 ]; then
        echo "Error: while running rule_setup.pl.";
        return;
    fi

	echo "setup the output directories from Bio::EnsEMBL::Pipeline::Config::BatchQueue"
    setup_batchqueue_outputdir.pl 
    if [ $? -ne 0 ]; then
        echo "Error: while running setup_batchqueue_outputdir.pl.";
        return;
    fi

}

CreateInputIds(){

    if [ $# -lt 3 ]; then
        echo "Usage: CreateInputIds <password> <'Slice' ['encode'|'toplevel'] | 'File' dir> [<exp_regex> <exp_suffix>]"
        return
    fi
    
    PASS=$1
    shift
    
    SUBMIT=$1
    shift

    SUBMITVAL=$1
    shift

    if [ "$SUBMIT" == "File" ]; then
       SUBMITVAL="dir $SUBMITVAL"
    fi
    
    OPTIONS=''

    if [ "$1" != "" ]; then OPTIONS="-exp_regex $1"; fi
    shift
    
    if [ "$1" != "" ]; then OPTIONS="$OPTIONS -exp_suffix $1"; fi
    shift
    
    ### determine analysis_id for SubmitType and write input_ids
    create_input_ids.pl $WRITE_DB_ARGS -dbname $EFG_DBNAME -pass $PASS \
        -$SUBMIT -$SUBMITVAL $OPTIONS; 

    if [ $? == 0 ]; then 
        echo "Ready for Analysis"
    else 
        echo "An error occured while inserting input_ids. Use CleanInputIds "
        echo "to drop input_ids and rerun CreateInputIds."
    fi
    
}

CheckPipelineSanity(){

    if [ $# -ne 1 ]; then
        echo "Usage: CheckPipelineSanity <password>"
        return
    fi
    
    PASS=$1
    shift

    echo "Check pipeline sanity"
    pipeline_sanity.pl -dbhost $PDBHOST -dbport $PDBPORT -dbuser $EFG_WRITE_USER -dbpass $PASS -dbname $PDBNAME

}                        

CleanInputIds(){

    if [ $# -ne 1 ]; then
        echo "Usage: CleanInputIds <password>"
        return
    fi
    
    PASS=$1
    shift

	echo "Cleaning input_id_analysis table"
	echo "delete from input_id_analysis" | mysqlw -p$PASS $PDBNAME

}

test_eFG_Runnable () {

    if [ $# -lt 5 ]; then
        echo "Usage: test_eFG_Runnable <password> <module> <logic_name> <input_id_type> <input_id>"
        return
    fi

    PASS=$1; shift
    MODULE=$1; shift
    LOGIC_NAME=$1; shift
    INPUT_ID_TYPE=$1; shift
    INPUT_ID=$1; shift

    echo "MODULE: $MODULE"
    echo "LOGIC_NAME: $LOGIC_NAME"
    echo "INPUT_ID_TYPE: ${INPUT_ID_TYPE}"
    echo "INPUT_ID: ${INPUT_ID}"

    time \
    $PIPELINE_SCRIPTS/test_RunnableDB -dbhost $PDBHOST -dbport $PDBPORT -dbuser $EFG_WRITE_USER -dbpass $PASS -dbname $PDBNAME \
        -runnabledb_path Bio/EnsEMBL/Analysis/RunnableDB/Funcgen \
        -module $MODULE \
        -logic_name $LOGIC_NAME \
        -input_id_type ${INPUT_ID_TYPE} \
        -input_id ${INPUT_ID} \
        $*
    
}

RunAnalysis(){

    if [ $# -lt 2 ]; then
        echo "Usage: RunAnalysis <password> <input_id_type> [<ln_regex>]"
        echo "           input_id_type: either 'File' or 'Slice'" 
        echo "           ln_regex: regular expression to match logic_name of analysis"
        return
    fi
    echo "Doing Analysis"
    
    PASS=$1
    shift
    
    INPUT_ID_TYPE=$1
    shift
    
    LN_REGEX=$1
    shift
    
    WRITE_ARGS=" -dbhost $EFG_HOST -dbport $EFG_PORT -dbuser $EFG_WRITE_USER -dbpass $PASS"

    CMD="rulemanager.pl $WRITE_ARGS  -dbname $PDBNAME -input_id_type ${INPUT_ID_TYPE}"

    if  [ "$LN_REGEX" == "" ]; then

        WHERE=''
        
    else 

        WHERE="WHERE logic_name RLIKE \"$LN_REGEX\""
        
    fi
    
    ANALYSES=$(
        for i in $(echo "select logic_name from analysis $WHERE" \
            | mysqlw -N -p$PASS $PDBNAME); do 
          echo -n "$i "
          done)
    #echo $ANALYSES

    if [ "$LN_REGEX" != "" ]; then
        ANALYSES=$(
            for i in $ANALYSES; do echo -n " -analysis $i"; done
        )
        CMD="$CMD $ANALYSES"

        echo "executing $CMD"
        eval $CMD

    else 

        echo "executing $CMD"
        eval $CMD

    fi


    return;

}

Monitor(){

    monitor -dbhost $EFG_HOST -dbport $EFG_PORT -dbuser $EFG_READ_USER -dbname $PDBNAME $*

}

GetFailedJobs(){
	query="select job.job_id, stderr_file, exec_host from job, job_status where job.job_id = job_status.job_id and is_current = 'y' and job_status.status like \"%FAIL%\";"
	echo $query | mysqlro $PDBNAME
	
}

GetRunningJobs(){
	query="select job.job_id, stderr_file, exec_host from job, job_status where job.job_id = job_status.job_id and is_current = 'y' and job_status.status like \"%RUN%\";"
	echo $query | mysqlro $PDBNAME
	
}
